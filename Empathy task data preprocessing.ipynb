{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benedict Greenwood benedict.greenwood AT ucl.ac.uk\n",
    "\n",
    "Script combines and cleans data from empathy task, in which two subjects (on \"participant\" and one \"companion\") received electrocutaneous shocks at one of three intensities (high/low/safe). Participants also underwent a resting state psychophysiology recording.\n",
    "\n",
    "This script links task datafiles and psychophysiology datafiles from all participants, cleans/recodes the data, supports visual data checks, and calculates psychophysiology parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"ce868b9a-d954-4ba7-b0b2-4f61475c3b1c\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"ce868b9a-d954-4ba7-b0b2-4f61475c3b1c\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.1.1.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"ce868b9a-d954-4ba7-b0b2-4f61475c3b1c\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# Standard library imports\n",
    "# =========================\n",
    "import os  \n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Third-party core libraries\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# File I/O / MATLAB / HDF5\n",
    "# =========================\n",
    "import h5py  # for importing from .mat file\n",
    "from scipy.io import savemat  # for saving as .mat file\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Signal processing / scientific computing\n",
    "# =========================\n",
    "from scipy.signal import detrend, medfilt\n",
    "from scipy.interpolate import interp1d, CubicSpline\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Neurophysiology / ECG / HRV toolkits\n",
    "# =========================\n",
    "import neurokit2 as nk\n",
    "import systole as sys\n",
    "from systole.detection import ecg_peaks  # for detecting peaks\n",
    "from systole.detection import ecg_peaks  # for detecting peaks (duplicate import)\n",
    "import rapidhrv as rhv\n",
    "import heartpy as hp\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Spike2 / CED / Neo handling\n",
    "# =========================\n",
    "import sonpy\n",
    "from sonpy import lib as sp\n",
    "from neo.io.cedio import CedIO\n",
    "from neo.core import Event\n",
    "from quantities import Hz, s\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Visualization libraries\n",
    "# =========================\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]  # Set standard plotsize\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import bokeh as bk\n",
    "bk.io.output_notebook()  # Start Bokeh\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import kaleido\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important - define duration constants (in ms)\n",
    "DURATION_PRE_PHASE = 2000 #duration to include before start of section (for baseline)\n",
    "DURATION_WAITING = 3000 #waiting phase\n",
    "DURATION_SHOCK = 500 #shock duration\n",
    "DURATION_RESPONSE_EXCL_SHOCK = 6000 #response phase excluding shock\n",
    "SPIKE_FS = 1000 #spike sampling rate in Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of session folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nTotal number of 'BG' folders containing participant data: 50\n",
      "BG001\n",
      "BG002\n",
      "BG003\n",
      "BG004\n",
      "BG005\n",
      "BG006\n",
      "BG007\n",
      "BG008\n",
      "BG009\n",
      "BG010\n",
      "BG011\n",
      "BG012\n",
      "BG013\n",
      "BG014\n",
      "BG015\n",
      "BG016\n",
      "BG017\n",
      "BG018\n",
      "BG019\n",
      "BG020\n",
      "BG021\n",
      "BG022\n",
      "BG023\n",
      "BG024\n",
      "BG025\n",
      "BG026\n",
      "BG027\n",
      "BG028\n",
      "BG029\n",
      "BG030\n",
      "BG031\n",
      "BG032\n",
      "BG033\n",
      "BG034\n",
      "BG035\n",
      "BG036\n",
      "BG037\n",
      "BG038\n",
      "BG039\n",
      "BG040\n",
      "BG041\n",
      "BG042\n",
      "BG043\n",
      "BG044\n",
      "BG045\n",
      "BG046\n",
      "BG047\n",
      "BG048\n",
      "BG049\n",
      "BG050\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(r\"F:/ADHD emotion/Empathy study\")\n",
    "directory = BASE_DIR / \"Participant data\"\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.isdir(directory):\n",
    "    raise FileNotFoundError(f\"Directory not found: {directory}\")\n",
    "\n",
    "\n",
    "# Find folders for each session\n",
    "initialsToFind = 'BG'\n",
    "session_folders = [folder for folder in os.listdir(directory) \n",
    "              if os.path.isdir(os.path.join(directory, folder)) and folder.startswith(initialsToFind)]\n",
    "\n",
    "gorillaDataFolder = os.path.join(directory, 'HCT gorilla data')\n",
    "\n",
    "# Print the total count and list of folders\n",
    "print(f\"/nTotal number of 'BG' folders containing participant data: {len(session_folders)}\")\n",
    "for folder in session_folders:\n",
    "    print(folder)\n",
    "\n",
    "# Define print statement colours\n",
    "RED = '\\033[91m'\n",
    "BLUE = '\\033[94m'\n",
    "RESET = '\\033[0m'  # Reset to default color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import matlab task data for all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Extracting matlab task data----------\n",
      "Session:  BG001\n",
      "- Manually scored faulty trials\n",
      "- Extracted matlab data\n",
      "Session:  BG002\n",
      "- Manually scored faulty trials\n",
      "- Extracted matlab data\n",
      "Session:  BG003\n",
      "- Manually scored faulty trials\n",
      "- Extracted matlab data\n",
      "Session:  BG004\n",
      "- Manually scored faulty trials\n",
      "- Extracted matlab data\n",
      "Session:  BG005\n",
      "- Manually scored faulty trials\n",
      "- Extracted matlab data\n",
      "Session:  BG006\n",
      "- Manually scored faulty trials\n",
      "- Extracted matlab data\n",
      "Session:  BG007\n",
      "- Manually scored faulty trials\n",
      "- Extracted matlab data\n",
      "Session:  BG008\n",
      "- Extracted matlab data\n",
      "Session:  BG009\n",
      "- Extracted matlab data\n",
      "Session:  BG010\n",
      "- Extracted matlab data\n",
      "Session:  BG011\n",
      "- Extracted matlab data\n",
      "Session:  BG012\n",
      "- Extracted matlab data\n",
      "Session:  BG013\n",
      "- Extracted matlab data\n",
      "Session:  BG014\n",
      "- Extracted matlab data\n",
      "Session:  BG015\n",
      "- Extracted matlab data\n",
      "Session:  BG016\n",
      "- Extracted matlab data\n",
      "Session:  BG017\n",
      "- Extracted matlab data\n",
      "Session:  BG018\n",
      "- Extracted matlab data\n",
      "Session:  BG019\n",
      "- Extracted matlab data\n",
      "Session:  BG020\n",
      "- Extracted matlab data\n",
      "Session:  BG021\n",
      "- Manually deleted trial 80 due to Spike crashing\n",
      "- Extracted matlab data\n",
      "Session:  BG022\n",
      "- Extracted matlab data\n",
      "Session:  BG023\n",
      "- Extracted matlab data\n",
      "Session:  BG024\n",
      "- Extracted matlab data\n",
      "Session:  BG025\n",
      "- Extracted matlab data\n",
      "Session:  BG026\n",
      "- Extracted matlab data\n",
      "Session:  BG027\n",
      "- Extracted matlab data\n",
      "Session:  BG028\n",
      "- Extracted matlab data\n",
      "Session:  BG029\n",
      "- Extracted matlab data\n",
      "Session:  BG030\n",
      "- Extracted matlab data\n",
      "Session:  BG031\n",
      "- Extracted matlab data\n",
      "Session:  BG032\n",
      "- Extracted matlab data\n",
      "Session:  BG033\n",
      "- Extracted matlab data\n",
      "Session:  BG034\n",
      "- Extracted matlab data\n",
      "Session:  BG035\n",
      "- Extracted matlab data\n",
      "Session:  BG036\n",
      "- Extracted matlab data\n",
      "Session:  BG037\n",
      "- Extracted matlab data\n",
      "Session:  BG038\n",
      "- Extracted matlab data\n",
      "Session:  BG039\n",
      "- Extracted matlab data\n",
      "Session:  BG040\n",
      "- Extracted matlab data\n",
      "Session:  BG041\n",
      "- Extracted matlab data\n",
      "Session:  BG042\n",
      "- Extracted matlab data\n",
      "Session:  BG043\n",
      "- Extracted matlab data\n",
      "Session:  BG044\n",
      "- Extracted matlab data\n",
      "Session:  BG045\n",
      "- Extracted matlab data\n",
      "Session:  BG046\n",
      "- Extracted matlab data\n",
      "Session:  BG047\n",
      "- Extracted matlab data\n",
      "Session:  BG048\n",
      "- Extracted matlab data\n",
      "Session:  BG049\n",
      "- Extracted matlab data\n",
      "Session:  BG050\n",
      "- Extracted matlab data\n"
     ]
    }
   ],
   "source": [
    "# Create empty dataframe to load each participant's data into\n",
    "df_all_matlab = pd.DataFrame()\n",
    "\n",
    "print('--------Extracting MATLAB task data----------')\n",
    "\n",
    "# Dictionary of manually scored faulty trials\n",
    "manual_faulty_trials = {\n",
    "    'BG001': [8, 41, 42, 76],\n",
    "    'BG002': [7, 27],\n",
    "    'BG003': [36, 47],\n",
    "    'BG004': [21, 42],\n",
    "    'BG005': [6, 30, 42],\n",
    "    'BG006': [43, 44],\n",
    "    'BG007': [3, 18, 36, 61, 67]\n",
    "}\n",
    "\n",
    "for sessionID in session_folders:     \n",
    "\n",
    "    # get session folder\n",
    "    sessionFolder = os.path.join(directory, sessionID)\n",
    "    print('Session: ', sessionID)\n",
    "    \n",
    "    # Import matlab (task) data files\n",
    "    matlab_csv_datafile = os.path.join(sessionFolder, sessionID + \"_empathy.csv\")\n",
    "    matlab_search_pattern = os.path.join(sessionFolder, f'*{matlab_csv_datafile}')\n",
    "\n",
    "    try:\n",
    "        df_matlab = pd.read_csv(matlab_csv_datafile)\n",
    "    except:\n",
    "        print('Error importing CSV file containing MATLAB data')\n",
    "\n",
    "    # If there is no column indexing faulty trials then add one\n",
    "    if 'faultyShock' not in df_matlab.columns:\n",
    "        df_matlab['faultyShock'] = 0\n",
    "\n",
    "    # Score faulty trials using the dictionary\n",
    "    faultyTrialNos = manual_faulty_trials.get(sessionID, [])\n",
    "    if faultyTrialNos:\n",
    "        print('-', 'Manually scored faulty trials')\n",
    "\n",
    "    rowsToChange = df_matlab['trialNo'].isin(faultyTrialNos)\n",
    "    df_matlab.loc[rowsToChange, 'faultyShock'] = 1\n",
    "\n",
    "    # IMPORTANT - exclude final trial from BG021 due to Spike crashing\n",
    "    if sessionID == \"BG021\":\n",
    "        trialToExclude = [80]\n",
    "        df_matlab = df_matlab[~df_matlab['trialNo'].isin(trialToExclude)]\n",
    "        print('- Manually deleted trial 80 due to Spike crashing')\n",
    "\n",
    "    # Add this session's data to df_all_matlab\n",
    "    df_all_matlab = pd.concat([df_all_matlab, df_matlab])\n",
    "    print('-', 'Extracted MATLAB data')\n",
    "\n",
    "# Rename the column 'sujectID' to 'subjectID' in df_all_matlab\n",
    "df_all_matlab.rename(columns={'sujectID': 'subjectID'}, inplace=True)\n",
    "\n",
    "# Save df_all_matlab\n",
    "df_all_matlab.to_csv('Data checks/df_all_matlab.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tablet ratings data for all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Extracting tablet ratings----------\n",
      "Session:  BG001\n",
      "- Extracting BG001 p ratings data\n",
      "- Extracting BG001 c ratings data\n",
      "Session:  BG002\n",
      "- Extracting BG002 p ratings data\n",
      "- Extracting BG002 c ratings data\n",
      "Session:  BG003\n",
      "- Extracting BG003 p ratings data\n",
      "- Extracting BG003 c ratings data\n",
      "Session:  BG004\n",
      "- Extracting BG004 p ratings data\n",
      "\u001b[91m- 3 ratings file found\u001b[0m\n",
      "\u001b[94m- Processing CSV file no.  0 \u001b[0m\n",
      "\u001b[94m- Processing CSV file no.  1 \u001b[0m\n",
      "\u001b[94m- Processing CSV file no.  2 \u001b[0m\n",
      "- Extracting BG004 c ratings data\n",
      "Session:  BG005\n",
      "- Extracting BG005 p ratings data\n",
      "- Extracting BG005 c ratings data\n",
      "Session:  BG006\n",
      "- Extracting BG006 p ratings data\n",
      "- Extracting BG006 c ratings data\n",
      "Session:  BG007\n",
      "- Extracting BG007 p ratings data\n",
      "- Extracting BG007 c ratings data\n",
      "Session:  BG008\n",
      "- Extracting BG008 p ratings data\n",
      "- Extracting BG008 c ratings data\n",
      "Session:  BG009\n",
      "- Extracting BG009 p ratings data\n",
      "- Extracting BG009 c ratings data\n",
      "Session:  BG010\n",
      "- Extracting BG010 p ratings data\n",
      "- Extracting BG010 c ratings data\n",
      "Session:  BG011\n",
      "- Extracting BG011 p ratings data\n",
      "- Extracting BG011 c ratings data\n",
      "Session:  BG012\n",
      "- Extracting BG012 p ratings data\n",
      "- Extracting BG012 c ratings data\n",
      "Session:  BG013\n",
      "- Extracting BG013 p ratings data\n",
      "- Extracting BG013 c ratings data\n",
      "Session:  BG014\n",
      "- Extracting BG014 p ratings data\n",
      "- Extracting BG014 c ratings data\n",
      "Session:  BG015\n",
      "- Extracting BG015 p ratings data\n",
      "- Extracting BG015 c ratings data\n",
      "Session:  BG016\n",
      "- Extracting BG016 p ratings data\n",
      "- Extracting BG016 c ratings data\n",
      "Session:  BG017\n",
      "- Extracting BG017 p ratings data\n",
      "- Extracting BG017 c ratings data\n",
      "Session:  BG018\n",
      "- Extracting BG018 p ratings data\n",
      "- Extracting BG018 c ratings data\n",
      "Session:  BG019\n",
      "- Extracting BG019 p ratings data\n",
      "- Extracting BG019 c ratings data\n",
      "Session:  BG020\n",
      "- Extracting BG020 p ratings data\n",
      "- Extracting BG020 c ratings data\n",
      "Session:  BG021\n",
      "- Extracting BG021 p ratings data\n",
      "- Extracting BG021 c ratings data\n",
      "Session:  BG022\n",
      "- Extracting BG022 p ratings data\n",
      "- Extracting BG022 c ratings data\n",
      "Session:  BG023\n",
      "- Extracting BG023 p ratings data\n",
      "- Extracting BG023 c ratings data\n",
      "Session:  BG024\n",
      "- Extracting BG024 p ratings data\n",
      "- Extracting BG024 c ratings data\n",
      "Session:  BG025\n",
      "- Extracting BG025 p ratings data\n",
      "- Extracting BG025 c ratings data\n",
      "Session:  BG026\n",
      "- Extracting BG026 p ratings data\n",
      "- Extracting BG026 c ratings data\n",
      "Session:  BG027\n",
      "- Extracting BG027 p ratings data\n",
      "- Extracting BG027 c ratings data\n",
      "Session:  BG028\n",
      "- Extracting BG028 p ratings data\n",
      "- Extracting BG028 c ratings data\n",
      "Session:  BG029\n",
      "- Extracting BG029 p ratings data\n",
      "- Extracting BG029 c ratings data\n",
      "Session:  BG030\n",
      "- Extracting BG030 p ratings data\n",
      "- Extracting BG030 c ratings data\n",
      "Session:  BG031\n",
      "- Extracting BG031 p ratings data\n",
      "- Extracting BG031 c ratings data\n",
      "Session:  BG032\n",
      "- Extracting BG032 p ratings data\n",
      "- Extracting BG032 c ratings data\n",
      "Session:  BG033\n",
      "- Extracting BG033 p ratings data\n",
      "- Extracting BG033 c ratings data\n",
      "Session:  BG034\n",
      "- Extracting BG034 p ratings data\n",
      "- Extracting BG034 c ratings data\n",
      "Session:  BG035\n",
      "- Extracting BG035 p ratings data\n",
      "- Extracting BG035 c ratings data\n",
      "Session:  BG036\n",
      "- Extracting BG036 p ratings data\n",
      "- Extracting BG036 c ratings data\n",
      "Session:  BG037\n",
      "- Extracting BG037 p ratings data\n",
      "- Extracting BG037 c ratings data\n",
      "Session:  BG038\n",
      "- Extracting BG038 p ratings data\n",
      "- Extracting BG038 c ratings data\n",
      "Session:  BG039\n",
      "- Extracting BG039 p ratings data\n",
      "- Extracting BG039 c ratings data\n",
      "Session:  BG040\n",
      "- Extracting BG040 p ratings data\n",
      "- Extracting BG040 c ratings data\n",
      "Session:  BG041\n",
      "- Extracting BG041 p ratings data\n",
      "- Extracting BG041 c ratings data\n",
      "Session:  BG042\n",
      "- Extracting BG042 p ratings data\n",
      "- Extracting BG042 c ratings data\n",
      "Session:  BG043\n",
      "- Extracting BG043 p ratings data\n",
      "- Extracting BG043 c ratings data\n",
      "Session:  BG044\n",
      "- Extracting BG044 p ratings data\n",
      "- Extracting BG044 c ratings data\n",
      "Session:  BG045\n",
      "- Extracting BG045 p ratings data\n",
      "- Extracting BG045 c ratings data\n",
      "Session:  BG046\n",
      "- Extracting BG046 p ratings data\n",
      "- Extracting BG046 c ratings data\n",
      "Session:  BG047\n",
      "- Extracting BG047 p ratings data\n",
      "- Extracting BG047 c ratings data\n",
      "Session:  BG048\n",
      "- Extracting BG048 p ratings data\n",
      "- Extracting BG048 c ratings data\n",
      "Session:  BG049\n",
      "- Extracting BG049 p ratings data\n",
      "- Extracting BG049 c ratings data\n",
      "Session:  BG050\n",
      "- Extracting BG050 p ratings data\n",
      "- Extracting BG050 c ratings data\n"
     ]
    }
   ],
   "source": [
    "#Create empty dataframe to load each participant's data into\n",
    "df_all_ratings = pd.DataFrame()\n",
    "\n",
    "print('--------Extracting tablet ratings----------')\n",
    "\n",
    "for sessionID in session_folders:     \n",
    "\n",
    "    # get session folder\n",
    "    sessionFolder = os.path.join(directory, sessionID)\n",
    "    print('Session: ', sessionID)\n",
    "    \n",
    "    # Import tablet data files for PARTICIPANT\n",
    "    print('-', 'Extracting', sessionID, 'p', 'ratings data')\n",
    "    p_search_pattern = sessionID + 'p'\n",
    "    year_search_pattern = '2024'\n",
    "    p_search_regex = re.compile(f'.*{p_search_pattern}.*{year_search_pattern}.*\\.csv', re.IGNORECASE)  # create regex search pattern that ignores case\n",
    "    p_all_csv_files = glob.glob(os.path.join(sessionFolder, '*.csv'))  # Use glob to find all CSV files\n",
    "    p_ratings_files = [f for f in p_all_csv_files if p_search_regex.match(os.path.basename(f))]\n",
    "\n",
    "    if len(p_ratings_files) == 1:\n",
    "        #if there is one ratings file then extract data\n",
    "        p_ratings_file = p_ratings_files[0]\n",
    "        df_p_ratings = pd.read_csv(p_ratings_file, skiprows=1) #import as dataframe\n",
    "        df_all_ratings = pd.concat([df_all_ratings, df_p_ratings]) #add this session's data to df_all_ratings\n",
    "\n",
    "    elif len(p_ratings_files) == 0:\n",
    "        print(RED + '-', 'No ratings file found' + RESET)\n",
    "\n",
    "    elif len(p_ratings_files) > 1:\n",
    "        print(RED + '-', len(p_ratings_files), 'ratings file found' + RESET)\n",
    "\n",
    "        #*IMPORTANT - process subjects with multiple datafiles (i.e. when participant accidentally quit so we had to re-open tablet ratings html)\n",
    "        if p_search_pattern == 'BG004p':\n",
    "            \n",
    "            df_p_ratings = pd.DataFrame() #create empty df\n",
    "\n",
    "            for i_file in range(len(p_ratings_files)):\n",
    "                print(BLUE + '-', 'Processing CSV file no. ', i_file, RESET)\n",
    "\n",
    "                p_ratings_file = p_ratings_files[i_file]\n",
    "                p_ratings = pd.read_csv(p_ratings_file, skiprows=1) #import as dataframe\n",
    "\n",
    "                if i_file == 1:\n",
    "                    df_p_ratings = pd.concat([df_p_ratings, p_ratings]) #add this session's data to df_all_rating\n",
    "                    max_trial_no_prev_file = p_ratings['trialNo'].iloc[-1] #calculate maximum trial number this file\n",
    "                    \n",
    "                elif i_file == 2: #rescore trial numbers\n",
    "                    p_ratings['trialNo'] = p_ratings['trialNo'] + max_trial_no_prev_file #take max trial number from previous file and add it to trial numbers in curent file \n",
    "                    df_p_ratings = pd.concat([df_p_ratings, p_ratings]) #add this session's data to df_all_rating\n",
    "        \n",
    "        df_all_ratings = pd.concat([df_all_ratings, df_p_ratings]) #add this session's data to df_all_ratings\n",
    "\n",
    "    # Import matlab (task) data files for COMPANION\n",
    "    print('-', 'Extracting', sessionID, 'c', 'ratings data')\n",
    "    c_search_pattern = sessionID + 'c'\n",
    "    c_search_regex = re.compile(f'.*{c_search_pattern}.*{year_search_pattern}.*\\.csv', re.IGNORECASE)  # create regex search pattern that ignores case\n",
    "    c_all_csv_files = glob.glob(os.path.join(sessionFolder, '*.csv'))  # Use glob to find all CSV files\n",
    "    c_ratings_files = [f for f in c_all_csv_files if c_search_regex.match(os.path.basename(f))]\n",
    "    \n",
    "\n",
    "    if len(c_ratings_files) == 1:\n",
    "        #if there is one ratings file then extract\n",
    "        c_ratings_file = c_ratings_files[0]\n",
    "        df_c_ratings = pd.read_csv(c_ratings_file, skiprows=1) #import as dataframe\n",
    "        df_all_ratings = pd.concat([df_all_ratings, df_c_ratings]) #add this session's data to df_all_ratings\n",
    "\n",
    "    elif len(c_ratings_files) == 0:\n",
    "        print(RED + '-', 'No ratings file found' + RESET)\n",
    "\n",
    "    elif len(c_ratings_files) > 1:\n",
    "        print(RED + '-', len(c_ratings_files), 'ratings file found' + RESET)\n",
    "\n",
    "#save df_all_ratings\n",
    "df_all_ratings.to_csv('Data checks/df_all_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link ratings data to task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to insert rows\n",
    "\n",
    "def insert_rows(ratings, trial_no_insert_after, count_new_rows, subjID):\n",
    "    insert_index = ratings[ratings['trialNo'] == trial_no_insert_after].index[0] + 1 #get index to insert at\n",
    "    new_rows = pd.DataFrame({ #define new rows\n",
    "        'userID': [subjID] * count_new_rows,\n",
    "        'phase': ['main_task'] * count_new_rows\n",
    "    }, index=range(count_new_rows))\n",
    "\n",
    "    ratings = pd.concat([ #add new rows\n",
    "        ratings.iloc[:insert_index],\n",
    "        new_rows,\n",
    "        ratings.iloc[insert_index:]\n",
    "    ]).reset_index(drop=True)\n",
    "\n",
    "    return ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Linking tablet ratings to matlab task data----------\n",
      "Session:  BG001\n",
      "- Processing BG001p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG001c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG002\n",
      "- Processing BG002p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG002c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG003\n",
      "- Processing BG003p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG003c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG004\n",
      "- Processing BG004p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG004c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG005\n",
      "- Processing BG005p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG005c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG006\n",
      "- Processing BG006p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG006c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG007\n",
      "- Processing BG007p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG007c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG008\n",
      "- Processing BG008p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG008c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG009\n",
      "- Processing BG009p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG009c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG010\n",
      "- Processing BG010p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG010c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG011\n",
      "- Processing BG011p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG011c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG012\n",
      "- Processing BG012p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG012c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG013\n",
      "- Processing BG013p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG013c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG014\n",
      "- Processing BG014p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG014c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG015\n",
      "- Processing BG015p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG015c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG016\n",
      "- Processing BG016p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG016c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG017\n",
      "- Processing BG017p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG017c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG018\n",
      "- Processing BG018p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG018c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG019\n",
      "- Processing BG019p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG019c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG020\n",
      "- Processing BG020p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG020c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG021\n",
      "- Processing BG021p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG021c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG022\n",
      "- Processing BG022p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG022c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG023\n",
      "- Processing BG023p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG023c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG024\n",
      "- Processing BG024p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG024c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG025\n",
      "- Processing BG025p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG025c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG026\n",
      "- Processing BG026p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG026c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG027\n",
      "- Processing BG027p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG027c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG028\n",
      "- Processing BG028p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG028c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG029\n",
      "- Processing BG029p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG029c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG030\n",
      "- Processing BG030p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG030c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG031\n",
      "- Processing BG031p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG031c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG032\n",
      "- Processing BG032p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG032c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG033\n",
      "- Processing BG033p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG033c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG034\n",
      "- Processing BG034p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG034c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG035\n",
      "- Processing BG035p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG035c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG036\n",
      "- Processing BG036p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG036c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG037\n",
      "- Processing BG037p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG037c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG038\n",
      "- Processing BG038p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG038c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG039\n",
      "- Processing BG039p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG039c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG040\n",
      "- Processing BG040p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG040c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG041\n",
      "- Processing BG041p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG041c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG042\n",
      "- Processing BG042p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG042c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG043\n",
      "- Processing BG043p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG043c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG044\n",
      "- Processing BG044p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG044c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG045\n",
      "- Processing BG045p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG045c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG046\n",
      "- Processing BG046p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG046c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG047\n",
      "- Processing BG047p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG047c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG048\n",
      "- Processing BG048p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG048c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG049\n",
      "- Processing BG049p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG049c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "Session:  BG050\n",
      "- Processing BG050p\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n",
      "- Processing BG050c\n",
      "\u001b[94m- Successfully linked ratings to matlab task data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe with matlab task data to add ratings data to\n",
    "df_all_combined = df_all_matlab.copy()\n",
    "\n",
    "# define variables to add as nans\n",
    "varsToAdd = ['sensationLocation', 'sensationCoordinates', 'sensationCoordinatesPerc', 'sensationIntensity', 'ratingArousal', 'ratingValence', 'emotionIntensity']\n",
    "\n",
    "print('--------Linking tablet ratings to matlab task data----------')\n",
    "\n",
    "# Dictionary mapping participant IDs to trial numbers to delete\n",
    "trial_nos_dict = {\n",
    "    # PARTICIPANT IDs\n",
    "    'BG001p': [1,2,3,4,5,6,7,8,9,10,11,14,57],\n",
    "    'BG003p': [21],\n",
    "    'BG007p': [5],\n",
    "    'BG008p': [10],\n",
    "    'BG009p': [17, 18],\n",
    "    'BG011p': [7],\n",
    "    'BG014p': [39, 40],\n",
    "    'BG016p': [34],\n",
    "    'BG018p': [3, 4, 11],\n",
    "    'BG022p': [7],\n",
    "    'BG026p': [3,8,9,11,12,13,18],\n",
    "    'BG028p': [7],\n",
    "    'BG030p': [3, 9, 42],\n",
    "    'BG041p': [34],\n",
    "    'BG044p': [3, 9, 10],\n",
    "    'BG046p': [6],\n",
    "    'BG049p': [8, 14],\n",
    "\n",
    "    # COMPANION IDs\n",
    "    'BG001c': [1,2,3,4,5,6,7,8,9,10,11],\n",
    "    'BG004c': [5, 22, 24],\n",
    "    'BG007c': [5],\n",
    "    'BG008c': [8],\n",
    "    'BG009c': [17, 18],\n",
    "    'BG017c': [40],\n",
    "    'BG026c': [3,8,9,11,12,13,18],\n",
    "    'BG028c': [7],\n",
    "    'BG029c': [8, 35],\n",
    "    'BG039c': [3],\n",
    "    'BG044c': [3,8,9,44],\n",
    "    'BG046c': [2],\n",
    "    'BG049c': [13]\n",
    "}\n",
    "\n",
    "for sessionID in session_folders:\n",
    "\n",
    "    print('Session: ', sessionID)\n",
    "\n",
    "    # start p_rows_inserted as False\n",
    "    p_rows_inserted = False\n",
    "    c_rows_inserted = False\n",
    "\n",
    "    # -------------Link data from PARTICIPANT\n",
    "    p_id = sessionID + 'p'\n",
    "    print('-', 'Processing', p_id)\n",
    "    \n",
    "    # Find relevant trials (rows) in df_all_matlab\n",
    "    p_rows_combined = df_all_combined['subjectID'].str.contains((p_id), na=False, case=False)\n",
    "    rating_rows_combined = df_all_combined['ratingSubmitted'] == 'yes'\n",
    "    p_rating_rows_combined = (p_rows_combined) & (rating_rows_combined)\n",
    "\n",
    "    # Find relevant trials (rows) in df_all_ratings\n",
    "    p_rows_allratings = df_all_ratings['userID'].str.contains((p_id), na=False, case=False)\n",
    "    p_ratings = df_all_ratings[p_rows_allratings]\n",
    "\n",
    "    # set trial_nos_to_delete using dictionary\n",
    "    trial_nos_ratings_to_delete = trial_nos_dict.get(p_id, [])\n",
    "\n",
    "    # Special participant-specific corrections\n",
    "    if p_id == 'BG001p':\n",
    "        p_ratings.loc[:, 'phase'] = 'main_task'\n",
    "    elif p_id == 'BG004p':\n",
    "        insert_after_trial = 23\n",
    "        n_new_rows = 4\n",
    "        p_ratings = insert_rows(p_ratings, insert_after_trial, n_new_rows, p_id)\n",
    "        p_rows_inserted = True\n",
    "    elif p_id == 'BG006p':\n",
    "        p_ratings.loc[p_ratings['trialNo'] == 30, 'shockRecipient'] = 'Self'\n",
    "    elif p_id == 'BG009p':\n",
    "        p_ratings.loc[p_ratings['trialNo'] == 39, 'shockRecipient'] = 'Self'\n",
    "    elif p_id == 'BG011p':\n",
    "        p_ratings.loc[p_ratings['trialNo'] == 21, 'shockRecipient'] = 'Other'\n",
    "    elif p_id == 'BG017p':\n",
    "        p_ratings.loc[p_ratings['trialNo'] == 19, 'shockRecipient'] = 'Self'\n",
    "    elif p_id == 'BG021p':\n",
    "        p_ratings.loc[p_ratings['trialNo'] == 33, 'shockRecipient'] = 'Other'\n",
    "    elif p_id == 'BG022p':\n",
    "        p_ratings = p_ratings.iloc[1:43]\n",
    "    elif p_id == 'BG024p':\n",
    "        p_ratings = p_ratings.iloc[1:42]\n",
    "    elif p_id == 'BG027p':\n",
    "        p_ratings.loc[p_ratings['trialNo'] == 29, 'shockRecipient'] = 'Self'\n",
    "    elif p_id == 'BG028p':\n",
    "        p_ratings = p_ratings.iloc[1:44]\n",
    "    elif p_id == 'BG029p':\n",
    "        p_rows_inserted = True\n",
    "    elif p_id == 'BG030p':\n",
    "        insert_after_trial = 38\n",
    "        n_new_rows = 1\n",
    "        p_ratings = insert_rows(p_ratings, insert_after_trial, n_new_rows, p_id)\n",
    "        p_rows_inserted = True\n",
    "    elif p_id == 'BG035p':\n",
    "        p_ratings = p_ratings.iloc[1:42]\n",
    "        p_rows_inserted = True\n",
    "    elif p_id == 'BG036p':\n",
    "        p_rows_inserted = True\n",
    "    elif p_id == 'BG037p':\n",
    "        p_ratings.loc[p_ratings['trialNo'] == 35, 'shockRecipient'] = 'Self'\n",
    "    elif p_id == 'BG044p':\n",
    "        p_ratings.loc[p_ratings['trialNo'] == 25, 'shockRecipient'] = 'Other'\n",
    "    elif p_id == 'BG045p':\n",
    "        p_ratings.loc[p_ratings['trialNo'] == 5, 'shockRecipient'] = 'Self'\n",
    "    elif p_id == 'BG046p':\n",
    "        insert_after_trial = 1\n",
    "        n_new_rows = 1\n",
    "        p_ratings = insert_rows(p_ratings, insert_after_trial, n_new_rows, p_id)\n",
    "        p_rows_inserted = True\n",
    "    elif p_id == 'BG047p':\n",
    "        p_ratings.loc[p_ratings['trialNo'] == 29, 'shockRecipient'] = 'Self'\n",
    "    elif p_id == 'BG049p':\n",
    "        insert_after_trial = 18\n",
    "        n_new_rows = 1\n",
    "        p_ratings = insert_rows(p_ratings, insert_after_trial, n_new_rows, p_id)\n",
    "        p_rows_inserted = True\n",
    "    elif p_id == 'BG050p':\n",
    "        p_ratings = p_ratings.iloc[1:42]\n",
    "\n",
    "    # delete rows\n",
    "    p_rows = p_ratings['userID'].str.contains((p_id), na=False, case=False)\n",
    "    p_rows_to_delete = (p_ratings['trialNo'].isin(trial_nos_ratings_to_delete)) & (p_rows)\n",
    "    p_ratings = p_ratings[~p_rows_to_delete]\n",
    "\n",
    "    # Find relevant trials (rows) in df_all_ratings now ratings have been fixed\n",
    "    p_rows_main_task = (p_ratings['phase'] == 'main_task')\n",
    "    p_rows = p_ratings['userID'].str.contains((p_id), na=False, case=False)\n",
    "    p_rows_ratings_maintask = (p_rows) & (p_rows_main_task)\n",
    "    p_ratings_to_keep = p_ratings.loc[p_rows_ratings_maintask, varsToAdd].reset_index(drop=True)\n",
    "\n",
    "    p_conds_tablet_debugging = p_ratings.loc[p_rows_ratings_maintask, 'shockRecipient'].str.lower().reset_index(drop=True)\n",
    "    p_conds_matlab_debugging = df_all_combined.loc[p_rating_rows_combined, 'shockRecipient'].reset_index(drop=True)\n",
    "\n",
    "    if sum(p_rating_rows_combined) != len(p_ratings_to_keep):\n",
    "        print(RED + '-', 'ERROR: there are', sum(p_rating_rows_combined), 'rows in matlab data requiring ratings, but', len(p_ratings_to_keep), 'rows of ratings in tablet data' + RESET)\n",
    "    elif (p_rows_inserted == False) & (not p_conds_tablet_debugging.equals(p_conds_matlab_debugging)):\n",
    "        print(RED + '-' , 'ERROR: Mismatches between tablet shock recipients and matlab' + RESET)\n",
    "    else:\n",
    "        df_all_combined.loc[p_rating_rows_combined, varsToAdd] = p_ratings_to_keep[varsToAdd].values\n",
    "        print(BLUE + '-', 'Successfully linked ratings to matlab task data' + RESET)\n",
    "\n",
    "    # -------------Link data from COMPANION\n",
    "    c_id = sessionID + 'c'\n",
    "    print('-', 'Processing', c_id)\n",
    "\n",
    "    c_rows_combined = df_all_combined['subjectID'].str.contains((c_id), na=False, case=False)\n",
    "    rating_rows_combined = df_all_combined['ratingSubmitted'] == 'yes'\n",
    "    c_rating_rows_combined = (c_rows_combined) & (rating_rows_combined)\n",
    "\n",
    "    c_rows_allratings = df_all_ratings['userID'].str.contains((c_id), na=False, case=False)\n",
    "    c_ratings = df_all_ratings[c_rows_allratings]\n",
    "\n",
    "    trial_nos_ratings_to_delete = trial_nos_dict.get(c_id, [])\n",
    "\n",
    "    # Special companion-specific corrections\n",
    "    if c_id == 'BG001c':\n",
    "        c_ratings.loc[:, 'phase'] = 'main_task'\n",
    "        c_ratings.loc[c_ratings['trialNo'] == 47, 'shockRecipient'] = 'Other'\n",
    "    elif c_id == 'BG004c':\n",
    "        insert_after_trial = 16\n",
    "        n_new_rows = 1\n",
    "        c_ratings = insert_rows(c_ratings, insert_after_trial, n_new_rows, c_id)\n",
    "        c_rows_inserted = True\n",
    "    elif c_id == 'BG010c':\n",
    "        c_ratings.loc[c_ratings['trialNo'] == 23, 'shockRecipient'] = 'Other'\n",
    "    elif c_id == 'BG011c':\n",
    "        c_rows_inserted = True\n",
    "    elif c_id == 'BG016c':\n",
    "        c_ratings.loc[c_ratings['trialNo'] == 19, 'shockRecipient'] = 'Self'\n",
    "    elif c_id == 'BG018c':\n",
    "        c_rows_inserted = True\n",
    "    elif c_id == 'BG019c':\n",
    "        c_rows_inserted = True\n",
    "    elif c_id == 'BG022c':\n",
    "        c_rows_inserted = True\n",
    "    elif c_id == 'BG027c':\n",
    "        c_ratings.loc[c_ratings['trialNo'] == 24, 'shockRecipient'] = 'Self'\n",
    "    elif c_id == 'BG029c':\n",
    "        c_ratings.loc[c_ratings['trialNo'] == 38, 'shockRecipient'] = 'Self'\n",
    "    elif c_id == 'BG030c':\n",
    "        c_rows_inserted = True\n",
    "    elif c_id == 'BG037c':\n",
    "        c_ratings.loc[c_ratings['trialNo'] == 4, 'shockRecipient'] = 'Self'\n",
    "    elif c_id == 'BG041c':\n",
    "        c_ratings.loc[c_ratings['trialNo'] == 29, 'shockRecipient'] = 'Other'\n",
    "    elif c_id == 'BG046c':\n",
    "        insert_after_trial = 5\n",
    "        n_new_rows = 1\n",
    "        c_ratings = insert_rows(c_ratings, insert_after_trial, n_new_rows, c_id)\n",
    "        c_rows_inserted = True\n",
    "\n",
    "    # delete rows\n",
    "    c_rows = c_ratings['userID'].str.contains((c_id), na=False, case=False)\n",
    "    c_rows_to_delete = (c_ratings['trialNo'].isin(trial_nos_ratings_to_delete)) & (c_rows)\n",
    "    c_ratings = c_ratings[~c_rows_to_delete]\n",
    "\n",
    "    c_rows_main_task = (c_ratings['phase'] == 'main_task')\n",
    "    c_rows = c_ratings['userID'].str.contains((c_id), na=False, case=False)\n",
    "    c_rows_ratings_maintask = (c_rows) & (c_rows_main_task)\n",
    "    c_ratings_to_keep = c_ratings.loc[c_rows_ratings_maintask, varsToAdd].reset_index(drop=True)\n",
    "\n",
    "    c_conds_tablet_debugging = c_ratings.loc[c_rows_ratings_maintask, 'shockRecipient'].str.lower().reset_index(drop=True)\n",
    "    c_conds_matlab_debugging = df_all_combined.loc[c_rating_rows_combined, 'shockRecipient'].reset_index(drop=True)\n",
    "\n",
    "    if sum(c_rating_rows_combined) != len(c_ratings_to_keep):\n",
    "        print(RED + '-', 'ERROR: there are', sum(c_rating_rows_combined), 'rows in matlab data requiring ratings, but', len(c_ratings_to_keep), 'rows of ratings in tablet data' + RESET)\n",
    "    elif (c_rows_inserted == False) & (not c_conds_tablet_debugging.equals(c_conds_matlab_debugging)):\n",
    "        print(RED + '-' , 'ERROR: Mismatches between tablet shock recipients and matlab' + RESET)\n",
    "    else:\n",
    "        df_all_combined.loc[c_rating_rows_combined, varsToAdd] = c_ratings_to_keep[varsToAdd].values\n",
    "        print(BLUE + '-', 'Successfully linked ratings to matlab task data' + RESET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amend ratings based on testing session notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Editing data in line with testing notes----------\n",
      "Excel file imported successfully.\n",
      "Subject:  BG001p\n",
      " - Scored faulty trials\n",
      "Subject:  BG001c\n",
      " - Scored faulty trials\n",
      "Subject:  BG002p\n",
      " - Scored faulty trials\n",
      "Subject:  BG002c\n",
      " - Scored faulty trials\n",
      "Subject:  BG003p\n",
      " - Scored faulty trials\n",
      "Subject:  BG003c\n",
      " - Scored faulty trials\n",
      "Subject:  BG004p\n",
      " - Scored faulty trials\n",
      "Subject:  BG004c\n",
      " - Scored faulty trials\n",
      "Subject:  BG005p\n",
      " - Updated lowIntensityOther\n",
      "Subject:  BG005c\n",
      " - Updated lowIntensitySelf\n",
      "Subject:  BG006p\n",
      " - Scored faulty trials\n",
      "Subject:  BG006c\n",
      " - Scored faulty trials\n",
      "Subject:  BG007p\n",
      " - Scored faulty trials\n",
      "Subject:  BG007c\n",
      " - Scored faulty trials\n",
      "Subject:  BG009p\n",
      " - Updated lowIntensitySelf\n",
      " - Updated highIntensitySelf\n",
      " - Updated highIntensityOther\n",
      "Subject:  BG009c\n",
      " - Updated highIntensitySelf\n",
      " - Updated lowIntensityOther\n",
      " - Updated highIntensityOther\n",
      "Subject:  BG022p\n",
      " - Scored faulty trials\n",
      "Subject:  BG022c\n",
      " - Scored faulty trials\n",
      "Subject:  BG026p\n",
      " - Scored faulty trials\n",
      "Subject:  BG026c\n",
      " - Scored faulty trials\n",
      "Subject:  BG027p\n",
      " - Updated lowIntensitySelf\n",
      " - Updated lowIntensityOther\n",
      "Subject:  BG027c\n",
      " - Updated lowIntensitySelf\n",
      " - Updated lowIntensityOther\n",
      "Subject:  BG029p\n",
      " - Scored faulty trials\n",
      " - Updated lowIntensitySelf\n",
      "Subject:  BG029c\n",
      " - Scored faulty trials\n",
      " - Updated lowIntensityOther\n",
      "Subject:  BG031p\n",
      " - Updated highIntensityOther\n",
      "Subject:  BG031c\n",
      " - Updated highIntensitySelf\n",
      "Subject:  BG033p\n",
      " - Updated lowIntensityOther\n",
      "Subject:  BG033c\n",
      " - Updated lowIntensitySelf\n",
      "Subject:  BG035p\n",
      " - Updated lowIntensitySelf\n",
      "Subject:  BG035c\n",
      " - Updated lowIntensityOther\n",
      "Subject:  BG042p\n",
      " - Scored faulty trials\n",
      " - Updated lowIntensitySelf\n",
      "Subject:  BG042c\n",
      " - Scored faulty trials\n",
      " - Updated lowIntensityOther\n",
      "Subject:  BG044p\n",
      " - Updated lowIntensitySelf\n",
      "Subject:  BG044c\n",
      " - Updated lowIntensityOther\n",
      "Subject:  BG045p\n",
      " - Updated lowIntensitySelf\n",
      "Subject:  BG045c\n",
      " - Updated lowIntensityOther\n",
      "Subject:  BG048p\n",
      " - Updated lowIntensitySelf\n",
      " - Updated highIntensitySelf\n",
      " - Updated lowIntensityOther\n",
      "Subject:  BG048c\n",
      " - Updated lowIntensitySelf\n",
      " - Updated lowIntensityOther\n",
      " - Updated highIntensityOther\n",
      "Subjects BG039p + BG039c\n",
      " - Scored faulty trial\n"
     ]
    }
   ],
   "source": [
    "print('--------Editing data in line with testing notes----------')\n",
    "\n",
    "# Path to the Excel file\n",
    "trials_to_edit_file = = BASE_DIR / \"Participant data\" / \"Data checks\" / \"notes_trials_to_edit.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Import the Excel file into a DataFrame\n",
    "    df_trials_to_edit = pd.read_excel(trials_to_edit_file)\n",
    "    print(\"Excel file imported successfully.\")\n",
    "except Exception as e:\n",
    "    print('Error importing xlsx file:', e)\n",
    "    exit()\n",
    "\n",
    "# Iterate through rows in the DataFrame\n",
    "for index, row in df_trials_to_edit.iterrows():\n",
    "    # Extract the subject ID from the current row\n",
    "    subjID = row['SubjectID']  # Use column name directly from the DataFrame\n",
    "    print('Subject: ', subjID)\n",
    "    \n",
    "    # Find participant trials (rows) in df_all_combined\n",
    "    subj_rows_combined = df_all_combined['subjectID'].str.contains(subjID, na=False, case=False)  # Ensure correct column name\n",
    "\n",
    "    # Rescore faulty trials\n",
    "    if pd.notna(row['trialsFaultyShock']):  # Check if 'trialsFaultyShock' is not NA\n",
    "        faultyTrials = row['trialsFaultyShock']\n",
    "\n",
    "        # Identify rows to amend\n",
    "        rows_to_amend = (df_all_combined['trialNo'] == faultyTrials) & (subj_rows_combined)\n",
    "\n",
    "        # Update the 'faultyShock' column\n",
    "        df_all_combined.loc[rows_to_amend, 'faultyShock'] = 1\n",
    "\n",
    "        print(' - Scored faulty trials')\n",
    "\n",
    "    # Update low shock strength for self\n",
    "    if pd.notna(row['newLowSelf']): \n",
    "        df_all_combined.loc[subj_rows_combined, 'lowIntensitySelf'] = row['newLowSelf']\n",
    "        print(' - Updated lowIntensitySelf')\n",
    "\n",
    "    # Update high shock strength for self\n",
    "    if pd.notna(row['newHighSelf']): \n",
    "        df_all_combined.loc[subj_rows_combined, 'highIntensitySelf'] = row['newHighSelf']\n",
    "        print(' - Updated highIntensitySelf')\n",
    "\n",
    "    # Update low shock strength for other\n",
    "    if pd.notna(row['newLowOther']): \n",
    "        df_all_combined.loc[subj_rows_combined, 'lowIntensityOther'] = row['newLowOther']\n",
    "        print(' - Updated lowIntensityOther')\n",
    "\n",
    "    # Update high shock strength for other\n",
    "    if pd.notna(row['newHighOther']): \n",
    "        df_all_combined.loc[subj_rows_combined, 'highIntensityOther'] = row['newHighOther']\n",
    "        print(' - Updated highIntensityOther')\n",
    "\n",
    "#BG039 - change trial 10 to NOT an error trial.\n",
    "# Find subject trials (rows) in df_all_combined\n",
    "subj_rows_combined = df_all_combined['subjectID'].str.contains('BG0039p|BG0039c', na=False, case=False)\n",
    "rows_to_amend = (df_all_combined['trialNo'] == 10) & (subj_rows_combined)\n",
    "df_all_combined.loc[rows_to_amend, 'faultyShock'] = 0\n",
    "print('Subjects BG039p + BG039c')\n",
    "print(' - Scored faulty trial')\n",
    "\n",
    "#save df_all_ratings for debugging\n",
    "#saveFileCombined = BASE_DIR / \"Participant data\" / \"Preprocessed task data\" / \"Empathy task\" / \"df_all_combined.csv\"\n",
    "#df_all_combined.to_csv(saveFileCombined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fcuntion for determinig breakpoints in Spike file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_n_breakpoints(number, n):\n",
    "    \"\"\"\n",
    "    Split an integer length into n approximately equal breakpoints.\n",
    "\n",
    "    Example:\n",
    "        number=10, n=3 -> [0, 4, 7, 10]\n",
    "    \"\"\"\n",
    "    \n",
    "    base = number // n\n",
    "    remainder = number % n\n",
    "    breakpoints = [0]  # Start with 0\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        point = i * base + min(i, remainder)\n",
    "        breakpoints.append(point)\n",
    "    \n",
    "    breakpoints.append(number)  # End with the original number\n",
    "    \n",
    "    return breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting ECG peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ECG_peaks(sessionID, p_or_c, ecg_data, ecg_peaks, ecg_signal, df_textmarks, resting_or_main):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot ECG signal with detected peaks and flagged outlier inter-beat intervals (IBIs).\n",
    "\n",
    "    If `resting_or_main` is \"resting\", the ECG is cropped using resting-state textmarks (\"rss\", \"rse\").\n",
    "    If \"main\", selected task textmarks (\"apr\", \"rat\") are added as annotated vertical lines.\n",
    "\n",
    "    Args:\n",
    "        sessionID (str): Session identifier.\n",
    "        p_or_c (str): \"p\" for participant or \"c\" for companion.\n",
    "        ecg_data: Object containing ECG time values in `ecg_data.times`.\n",
    "        ecg_peaks (array-like of bool): Boolean mask indicating detected peaks.\n",
    "        ecg_signal (array-like): ECG amplitude values.\n",
    "        df_textmarks (pd.DataFrame): DataFrame with 'text' and 'sampleNo' columns.\n",
    "        resting_or_main (str): Either \"resting\" or \"main\".\n",
    "\n",
    "    Returns:\n",
    "        None: Displays an interactive Plotly figure.\n",
    "\n",
    "    Notes:\n",
    "        Requires `SPIKE_FS` (and optionally `RED`) to be defined globally.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    print(sessionID)\n",
    "\n",
    "    #extract ecg times\n",
    "    ecg_times = ecg_data.times\n",
    "\n",
    "    if resting_or_main == 'resting':\n",
    "        #Find textmarks for start and end of resting state recording\n",
    "        if sessionID == 'BG002':\n",
    "            #Process exceptions\n",
    "            tm_rs_start = 418900\n",
    "            tm_rs_end = tm_rs_start + 120000\n",
    "\n",
    "        #For all othr sessions\n",
    "        else:\n",
    "            tm_rs_start = int(df_textmarks.loc[df_textmarks['text'] == 'rss', 'sampleNo'].iloc[-1]) # take final rss textmark\n",
    "            tm_rs_end = int(df_textmarks.loc[df_textmarks['text'] == 'rse', 'sampleNo'].iloc[-1]) # take final rss textmark\n",
    "        \n",
    "        #Check textmarks found\n",
    "        if pd.isna(tm_rs_start):\n",
    "            print(RED + 'WARNING: no rss textmark found')\n",
    "        elif pd.isna(tm_rs_end):\n",
    "            print(RED + 'WARNING: no rse textmark found')\n",
    "            \n",
    "        ecg_times = ecg_data.times[tm_rs_start : tm_rs_end]\n",
    "        ecg_signal = ecg_signal[tm_rs_start : tm_rs_end]\n",
    "        ecg_peaks = ecg_peaks[tm_rs_start : tm_rs_end]\n",
    "\n",
    "    #Create peak massk for plot\n",
    "    peak_mask = ecg_peaks == True # Create a mask for true values in p_ecg_peaks\n",
    "\n",
    "    #Identify outlier IBIs based on being much shorter or longer than previous ibi\n",
    "    peak_indices = np.where(peak_mask)[0]\n",
    "    ibis = np.diff(peak_indices)\n",
    "    outlier_ibi_mask = np.zeros_like(ibis, dtype=bool)\n",
    "    for i in range(1, len(ibis)):\n",
    "        if ibis[i] < 0.6 * ibis[i-1] or ibis[i] > 1.4 * ibis[i-1]:\n",
    "            outlier_ibi_mask[i] = True\n",
    "\n",
    "    full_outlier_mask = np.zeros_like(peak_mask, dtype=bool) # Create a full-length mask for outliers\n",
    "    full_outlier_mask[peak_indices[1:][outlier_ibi_mask]] = True\n",
    "\n",
    "    # Add the time series data\n",
    "    fig.add_trace(go.Scatter(x=ecg_times, y=ecg_signal, mode='lines', name='ECG Signal')) #plot ecg trace\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=ecg_times[peak_mask], #plot peaks\n",
    "        y=np.ones(sum(peak_mask)),  # All y-values are set to 1\n",
    "        mode='markers',\n",
    "        marker=dict(color='red', size=8, symbol='cross'),  # Customize marker style\n",
    "        name='ECG Peaks'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=ecg_times[full_outlier_mask], #plot outlier ibis\n",
    "        y=np.ones(sum(full_outlier_mask)),  # All y-values are set to 1\n",
    "        mode='markers',\n",
    "        marker=dict(color='yellow', size=8, symbol='cross'),  # Customize marker style\n",
    "        name='Outliers'\n",
    "    ))\n",
    "\n",
    "    ## Add textmarks\n",
    "    if resting_or_main == 'main':\n",
    "        textmarks_of_interest = ['apr', 'rat'] #apr = anticipation before name displayed; rat = subjects prompted to submit rating\n",
    "        df_textmarks_subset = df_textmarks[df_textmarks['text'].isin(textmarks_of_interest)]\n",
    "        for _, row in df_textmarks_subset.iterrows():\n",
    "            tm_sample_no = row['sampleNo']\n",
    "            tm_text = row['text']\n",
    "            \n",
    "            # Add vertical line\n",
    "            fig.add_vline(x=tm_sample_no/(SPIKE_FS), line_width=2, line_dash=\"dash\", line_color=\"grey\") #divide sample number by SPIKE_FS to convert to s\n",
    "            \n",
    "            # Add text annotation\n",
    "            fig.add_annotation(\n",
    "                x=tm_sample_no/(SPIKE_FS),\n",
    "                y=1,  # Adjust this value to position the text vertically\n",
    "                text=tm_text,\n",
    "                showarrow=False,\n",
    "                textangle=-90,\n",
    "                yanchor=\"bottom\",\n",
    "                font=dict(color=\"black\", size=20)\n",
    "            )\n",
    "        \n",
    "    #Add title specifying p or c\n",
    "    if p_or_c == 'p':\n",
    "        part_type = 'participant'\n",
    "    else:\n",
    "        part_type = 'companion'\n",
    "\n",
    "    fig.update_layout(\n",
    "        title = f\"{sessionID}: {part_type} data\"\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for calculating HR responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_mean_hr(start_idx, end_idx, inst_hr_timeseries, subj_noisy_samples, SPIKE_FS):\n",
    "    \"\"\"\n",
    "    Compute mean instantaneous heart rate (HR) over a sample window.\n",
    "\n",
    "    Returns NaN if any noisy samples occur within the window.\n",
    "\n",
    "    Args:\n",
    "        start_idx (int): Start sample index (inclusive).\n",
    "        end_idx (int): End sample index (exclusive).\n",
    "        inst_hr_timeseries (array-like): Instantaneous HR values (bpm) per sample.\n",
    "        subj_noisy_samples (array-like of bool): Mask indicating noisy samples.\n",
    "        SPIKE_FS (int or float): Sampling frequency (Hz). Included for compatibility.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean HR (bpm) or np.nan if window contains noise.\n",
    "    \"\"\"\n",
    "    # Exclude window if any noisy samples\n",
    "    if np.any(subj_noisy_samples[start_idx:end_idx]):\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        mean_hr = inst_hr_timeseries[start_idx:end_idx].mean()\n",
    "        return mean_hr\n",
    "\n",
    "\n",
    "def downsample_linear_interpolation(data, orig_fs, target_fs):\n",
    "    \"\"\"\n",
    "    Downsample a signal to a target sampling rate using linear interpolation.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): Input signal.\n",
    "        orig_fs (float): Original sampling rate (Hz).\n",
    "        target_fs (float): Target sampling rate (Hz).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Downsampled signal.\n",
    "    \"\"\"\n",
    "    # Calculate the duration of the signal\n",
    "    duration = len(data) / orig_fs\n",
    "\n",
    "    # Create the original time points\n",
    "    original_time = np.linspace(0, duration, len(data), endpoint=True)\n",
    "\n",
    "    # Create the new time points for the target frequency\n",
    "    num_target_samples = int(round(duration * target_fs) + 1) #add 1 to ensure last sample is included\n",
    "    target_time = np.linspace(0, duration, num_target_samples, endpoint=True)\n",
    "\n",
    "    # Perform linear interpolation\n",
    "    downsampled_data = np.interp(target_time, original_time, data)\n",
    "    \n",
    "    return downsampled_data\n",
    "\n",
    "\n",
    "def downsample_with_last(arr, original_fs, target_fs):\n",
    "    \"\"\"\n",
    "    Downsample a signal by integer decimation while ensuring the final sample is included.\n",
    "\n",
    "    Args:\n",
    "        arr (array-like): Input signal.\n",
    "        original_fs (float): Original sampling rate (Hz).\n",
    "        target_fs (float): Target sampling rate (Hz).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Downsampled signal including the last sample.\n",
    "    \"\"\"\n",
    "    factor = int(original_fs / target_fs)\n",
    "\n",
    "    idx = np.arange(0, len(arr), factor)\n",
    "    if idx[-1] != len(arr) - 1:\n",
    "        idx = np.append(idx, len(arr) - 1)\n",
    "    return arr[idx]\n",
    "\n",
    "\n",
    "def calculate_hr_timeseries(start_sample, end_sample, inst_hr_timeseries, noisy_samples, orig_fs, target_fs):\n",
    "    \"\"\"\n",
    "    Extract and downsample instantaneous HR for a given window, excluding noisy segments.\n",
    "\n",
    "    Returns an array containing NaN if noise is present in the window.\n",
    "\n",
    "    Args:\n",
    "        start_sample (int): Start sample index (inclusive).\n",
    "        end_sample (int): End sample index (inclusive).\n",
    "        inst_hr_timeseries (array-like): Instantaneous HR values (bpm) per sample.\n",
    "        noisy_samples (array-like of bool): Mask indicating noisy samples.\n",
    "        orig_fs (float): Original sampling rate (Hz).\n",
    "        target_fs (float): Target sampling rate (Hz).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Downsampled HR series (rounded to 2 decimals) or [np.nan].\n",
    "    \"\"\"\n",
    "    # Check for noisy samples in phase\n",
    "    if np.any(noisy_samples[start_sample:end_sample+1]):\n",
    "        return np.array([np.nan])\n",
    "    else:\n",
    "        # Downsample to target frequency and include last sample\n",
    "        downsampled_hr = np.round(downsample_with_last(inst_hr_timeseries[start_sample:end_sample], orig_fs, target_fs), 2)\n",
    "\n",
    "    return downsampled_hr\n",
    "\n",
    "\n",
    "def calculate_instantaneous_hr(subj_ecg_peaks, sampling_rate):\n",
    "    \"\"\"\n",
    "    Compute instantaneous heart rate (bpm) from ECG R-peak locations.\n",
    "\n",
    "    HR is assigned to all samples between consecutive peaks. Output is the same\n",
    "    length as the input signal, with NaNs where HR cannot be computed.\n",
    "\n",
    "    Args:\n",
    "        subj_ecg_peaks (array-like of bool): Boolean mask indicating R-peak samples.\n",
    "        sampling_rate (float): ECG sampling rate (Hz).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Instantaneous HR time series (bpm).\n",
    "    \"\"\"\n",
    "    subj_ecg_peaks = np.asarray(subj_ecg_peaks)\n",
    "    peak_indices = np.where(subj_ecg_peaks)[0]\n",
    "    hr_timeseries = np.full_like(subj_ecg_peaks, np.nan, dtype=float)\n",
    "\n",
    "    if len(peak_indices) < 2:\n",
    "        return hr_timeseries  # Not enough peaks to compute HR\n",
    "\n",
    "    # Calculate RR intervals and HR for each interval\n",
    "    rr_intervals = np.diff(peak_indices) / sampling_rate\n",
    "    hr_values = 60.0 / rr_intervals\n",
    "\n",
    "    # Assign HR value to all samples between each pair of R-peaks\n",
    "    for i in range(len(rr_intervals)):\n",
    "        start = peak_indices[i]\n",
    "        end = peak_indices[i+1]\n",
    "        hr_timeseries[start:end] = hr_values[i]\n",
    "\n",
    "    return hr_timeseries\n",
    "\n",
    "\n",
    "def interpolate_ectopic_hr(inst_hr_timeseries, p_ecg_peaks, idx_p_ectopics_to_interpolate):\n",
    "    \"\"\"\n",
    "    Interpolate HR values around ectopic peaks using cubic spline interpolation.\n",
    "\n",
    "    For each ectopic peak, HR values between the nearest surrounding true R-peaks\n",
    "    are replaced with spline-interpolated values.\n",
    "\n",
    "    Args:\n",
    "        inst_hr_timeseries (array-like): Instantaneous HR values (bpm) per sample.\n",
    "        p_ecg_peaks (array-like of bool): Boolean mask indicating detected peaks.\n",
    "        idx_p_ectopics_to_interpolate (array-like of int): Indices of ectopic peaks.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: HR time series with ectopic intervals interpolated.\n",
    "    \"\"\"\n",
    "    inst_hr_timeseries_interp = inst_hr_timeseries.copy()\n",
    "    r_peak_indices = np.where(p_ecg_peaks)[0]\n",
    "\n",
    "    # Exclude ectopic peaks from true R-peaks\n",
    "    true_r_peaks = np.setdiff1d(r_peak_indices, idx_p_ectopics_to_interpolate)\n",
    "\n",
    "    # Identify all indices to interpolate (between true peaks surrounding each ectopic)\n",
    "    for ectopic_idx in idx_p_ectopics_to_interpolate:\n",
    "        prev_peaks = true_r_peaks[true_r_peaks < ectopic_idx]\n",
    "        next_peaks = true_r_peaks[true_r_peaks > ectopic_idx]\n",
    "        if len(prev_peaks) == 0 or len(next_peaks) == 0:\n",
    "            print(RED + f\"Cannot interpolate for ectopic peak at index {ectopic_idx}: no bounding true peaks.\" + RESET)\n",
    "            continue  # Cannot interpolate if no bounding true peaks\n",
    "        \n",
    "        # Get the last true peak before the ectopic and the first true peak after\n",
    "        prev_peak = prev_peaks[-1]\n",
    "        next_peak = next_peaks[0]\n",
    "\n",
    "        #extend sample numbers by 2 to make sure we do not include ectopic HR\n",
    "        prev_peak = prev_peak - 2\n",
    "        next_peak = next_peak + 2\n",
    "\n",
    "        # Indices to interpolate (exclude endpoints)\n",
    "        interp_indices = np.arange(prev_peak, next_peak)\n",
    "        \n",
    "        # Known points (start and end of the segment)\n",
    "        x_known = np.array([prev_peak, next_peak])\n",
    "        y_known_hr = inst_hr_timeseries_interp[x_known]\n",
    "        \n",
    "        # Create cubic spline interpolator\n",
    "        cs_hr = CubicSpline(x_known, y_known_hr)\n",
    "\n",
    "        # Interpolate and replace hr values in inst_hr_timeseries_interp\n",
    "        inst_hr_timeseries_interp[interp_indices] = cs_hr(interp_indices)\n",
    "\n",
    "    return inst_hr_timeseries_interp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECG data checks and HR response calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session:  BG001\n",
      "- Importing Spike data for BG001\n",
      "BG001p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [2039311 2188113 2416994 2417234 2582198 3310513 3726899 4009342 4424940\n",
      " 4464605 4527491 4539044 4619962 4776050 4842699 4922331 4922558 5155788\n",
      " 5365235 5540069 5669440 5751039 5751247]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [2417320 4922507]\u001b[0m\n",
      "BG001c peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [3454816 4128070 4128326] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [3454815 4128069 4128325]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [3454815 4128069 4128325]\u001b[0m\n",
      "BG001p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Processing noisy sections\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG001c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG002\n",
      "- Importing Spike data for BG002\n",
      "BG002p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG002c peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1527531 1537921] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1527530 1537920]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1527530 1537920]\u001b[0m\n",
      "BG002p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (40) matches no. rows (40) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG002c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (40) matches no. rows (40) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG003\n",
      "- Importing Spike data for BG003\n",
      "BG003p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG003c peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [4736059 4736297] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [4736058 4736296]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [4736058 4736296]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [4736429]\u001b[0m\n",
      "BG003p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG003c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG004\n",
      "- Importing Spike data for BG004\n",
      "BG004p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [4356454 4356879]\u001b[0m\n",
      "BG004c peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [2790959 5340423 5340627 5340907 5743813 5744061 5844803 5845008 6089024\n",
      " 6089227] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [2790958 5340422 5340626 5340906 5743812 5744060 5844802 5845007 6089023\n",
      " 6089226]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [2790958 5340422 5340626 5340906 5743812 5744060 5844802 5845007 6089023\n",
      " 6089226]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [5340845]\u001b[0m\n",
      "BG004p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG004c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG005\n",
      "- Importing Spike data for BG005\n",
      "BG005p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [2765613 2766536 2897147 3353460 3354044 3638972 4292620 4292341 4292850\n",
      " 4293145 4455491 4455969 4817559 4817765 4817970 5326465 5624302 5624511\n",
      " 5624720 5624964 5720679 5789618 5949191 5949409 6164693]\u001b[0m\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [2765046] \u001b[0m\n",
      "\u001b[94m- Added missing peaks  [2765365 2769500 4292701 4293268 4817664 5624633 5949249]\u001b[0m\n",
      "BG005c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG005p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Processing noisy sections\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG005c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG006\n",
      "- Importing Spike data for BG006\n",
      "BG006p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG006c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG006p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG006c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG007\n",
      "- Importing Spike data for BG007\n",
      "BG007p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [3737934 4772055 4772281 4772500 4772702 4772909 4773130 4773716 4773924\n",
      " 4774151 6057471]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [4772560]\u001b[0m\n",
      "BG007c peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [2545581] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [2545580]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [2545580]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [2545691]\u001b[0m\n",
      "BG007p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG007c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Processing noisy sections\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG008\n",
      "- Importing Spike data for BG008\n",
      "BG008p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [4167716 5813228] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [4167715 5813227]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [4167715 5813227]\u001b[0m\n",
      "BG008c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [2916046 3175358 3675620 3676229 4281602 4287438 4287664 4288135 4508031\n",
      " 4508281 4906341 4906583 4982502 5033684 5033908 5807056]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [2916228]\u001b[0m\n",
      "BG008p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG008c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG009\n",
      "- Importing Spike data for BG009\n",
      "BG009p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [3994830 3995062 4664295 4691701 4693547 4691916 4693880] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [3994829 3995061 4664294 4691700 4693546 4691915 4693879]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [3994829 3995061 4664294 4691700 4693546 4691915 4693879]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [3995206 4692034 4693934]\u001b[0m\n",
      "BG009c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG009p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG009c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG010\n",
      "- Importing Spike data for BG010\n",
      "BG010p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG010c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 3533287:3536285 \u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks at [2507465 2555997 2601836 2602037 2602278 3234840 3235288 3235500 3362720\n",
      " 3709259 3710259 3710704 3844156 3844368 3845924 3942654 3942864 3943570\n",
      " 4122493 4122718 4234865 4248670 4259858 4261922 4265682 4269728 4269931\n",
      " 4293226 5003702 5058044 5123725 5123965 5125066]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [2556098 2602416 3533732 3534438 3535188 3535951 3709345 3710764 3844207\n",
      " 5123763]\u001b[0m\n",
      "BG010p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG010c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Processing noisy sections\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG011\n",
      "- Importing Spike data for BG011\n",
      "BG011p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG011c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG011p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG011c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG012\n",
      "- Importing Spike data for BG012\n",
      "BG012p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG012c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [3717432]\u001b[0m\n",
      "BG012p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG012c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG013\n",
      "- Importing Spike data for BG013\n",
      "BG013p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [2691478 3534161 3963041 3963265 3963555 5558712] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [2691477 3534160 3963040 3963264 3963554 5558711]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [2691477 3534160 3963040 3963264 3963554 5558711]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [3963380]\u001b[0m\n",
      "BG013c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [2687903 4159437 4164833 4332127]\u001b[0m\n",
      "BG013p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG013c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG014\n",
      "- Importing Spike data for BG014\n",
      "BG014p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [4124924 4125147] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [4124923 4125146]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [4124923 4125146]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [4125094]\u001b[0m\n",
      "BG014c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG014p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG014c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG015\n",
      "- Importing Spike data for BG015\n",
      "BG015p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 4240600:4241980 \u001b[0m\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [4296342] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [4296341]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [4296341]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [4240461 4241065 4241703]\u001b[0m\n",
      "BG015c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [3685531 4206371]\u001b[0m\n",
      "BG015p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG015c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG016\n",
      "- Importing Spike data for BG016\n",
      "BG016p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG016c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [2263715]\u001b[0m\n",
      "BG016p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG016c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG017\n",
      "- Importing Spike data for BG017\n",
      "BG017p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG017c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG017p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG017c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG018\n",
      "- Importing Spike data for BG018\n",
      "BG018p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [5280952 5281169] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [5280951 5281168]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [5280951 5281168]\u001b[0m\n",
      "BG018c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [2515072]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [2515314]\u001b[0m\n",
      "BG018p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG018c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Processing noisy sections\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG019\n",
      "- Importing Spike data for BG019\n",
      "BG019p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG019c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG019p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG019c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG020\n",
      "- Importing Spike data for BG020\n",
      "BG020p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1753116 2292588 3583855 3663312 3897938 4104101 2392904] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1753115 2292587 3583854 3663311 3897937 4104100 2392903]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1753115 2292587 3583854 3663311 3897937 4104100 2392903]\u001b[0m\n",
      "BG020c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [1806593 2359327 4177700 4178136]\u001b[0m\n",
      "BG020p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG020c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG021\n",
      "- Importing Spike data for BG021\n",
      "BG021p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1798249 1798450] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1798248 1798449]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1798248 1798449]\u001b[0m\n",
      "\u001b[94m- Interpolated HR for  1  ectopic beats at [2150315]\u001b[0m\n",
      "BG021c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG021p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (79) matches no. rows (79) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94mSet HR responses for trial 79 resposne phase to nan\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG021c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (79) matches no. rows (79) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94mSet HR responses for trial 79 resposne phase to nan\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG022\n",
      "- Importing Spike data for BG022\n",
      "BG022p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 1722900:1725862, 1934520:1940000 \u001b[0m\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1705098 1773817 2107752 2108322 2507501 2536648 2536892 2859140 2885731\n",
      " 3037745 3170847 3171100 3510457 3510716 3511448 4139946 4878683 4962043\n",
      " 5126210 5126878 5155726 5155951 5156379 5555467 5556257 5556709 5556935\n",
      " 5558006 5593581] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1705097 1773816 2107751 2108321 2507500 2536647 2536891 2859139 2885730\n",
      " 3037744 3170846 3171099 3510456 3510715 3511447 4139945 4878682 4962042\n",
      " 5126209 5126877 5155725 5155950 5156378 5555466 5556256 5556708 5556934\n",
      " 5558005 5593580]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1705097 1773816 2107751 2108321 2507500 2536647 2536891 2859139 2885730\n",
      " 3037744 3170846 3171099 3510456 3510715 3511447 4139945 4878682 4962042\n",
      " 5126209 5126877 5155725 5155950 5156378 5555466 5556256 5556708 5556934\n",
      " 5558005 5593580]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [1723028 1723675 1724310 1724956 1725573 1934667 1935298 1935936 1936579\n",
      " 1937288 1937845 1938494 1938999 2536826 2859284 3171035 3510586]\u001b[0m\n",
      "BG022c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [1844902 2536025 4592546 4592768]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [4593070]\u001b[0m\n",
      "BG022p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Processing noisy sections\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG022c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "[1934667 1935298 1935936 1936579 1937288 1937845 1938494 1938999]\n",
      "Session:  BG023\n",
      "- Importing Spike data for BG023\n",
      "BG023p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG023c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG023p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG023c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Processing noisy sections\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG024\n",
      "- Importing Spike data for BG024\n",
      "BG024p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 4853330:4854948, 5064760:5071319 \u001b[0m\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [2082993 2209100 4085330 4085552 4124200 4259056 4267389 4297238 4633499\n",
      " 4633745 4878638 4883112 4883346] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [2082992 2209099 4085329 4085551 4124199 4259055 4267388 4297237 4633498\n",
      " 4633744 4878637 4883111 4883345]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [2082992 2209099 4085329 4085551 4124199 4259055 4267388 4297237 4633498\n",
      " 4633744 4878637 4883111 4883345]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [4085622 4633558 4853726 4854417 4855095 4883291 5065185 5065888 5066587\n",
      " 5067306 5068063 5068789 5069539 5070266 5070958]\u001b[0m\n",
      "BG024c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [3467058 3576102 3576396 3870214 3870460 5041142]\u001b[0m\n",
      "BG024p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG024c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG025\n",
      "- Importing Spike data for BG025\n",
      "BG025p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1823195 5865935 6035166] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1823194 5865934 6035165]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1823194 5865934 6035165]\u001b[0m\n",
      "BG025c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [1733881 1734083 3357193]\u001b[0m\n",
      "BG025p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG025c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG026\n",
      "- Importing Spike data for BG026\n",
      "BG026p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG026c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [2105329 2541189 4552480 4602535 5100214 5566606]\u001b[0m\n",
      "\u001b[94m- Interpolated HR for  6  ectopic beats at [2105329 2541189 4552480 4602535 5100214 5566606]\u001b[0m\n",
      "BG026p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG026c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG027\n",
      "- Importing Spike data for BG027\n",
      "BG027p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG027c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [5275863]\u001b[0m\n",
      "BG027p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG027c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG028\n",
      "- Importing Spike data for BG028\n",
      "BG028p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 1673540:1675223 \u001b[0m\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1249526 1249979 1267722 1791060 1791293] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1249525 1249978 1267721 1791059 1791292]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1249525 1249978 1267721 1791059 1791292]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [1673607 1675449 1676549 1677557 1678495 1679493 1680445 1681380]\u001b[0m\n",
      "\u001b[94m- Interpolated HR for  1  ectopic beats at [1674540]\u001b[0m\n",
      "BG028c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG028p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG028c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG029\n",
      "- Importing Spike data for BG029\n",
      "BG029p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1996394 2076651 2359158 2362693 2739637 2740690 3255897] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1996393 2076650 2359157 2362692 2739636 2740689 3255896]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1996393 2076650 2359157 2362692 2739636 2740689 3255896]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [2359517 2362772]\u001b[0m\n",
      "\u001b[94m- Interpolated HR for  2  ectopic beats at [2740908 3256044]\u001b[0m\n",
      "BG029c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [1222553 1335048 1642728 3120685 3182702 3296429 3720438 3782674 4335617\n",
      " 4417647]\u001b[0m\n",
      "\u001b[94m- Interpolated HR for  10  ectopic beats at [1222553 1335048 1642728 3120685 3182702 3296429 3720438 3782674 4335617\n",
      " 4417647]\u001b[0m\n",
      "BG029p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG029c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG030\n",
      "- Importing Spike data for BG030\n",
      "BG030p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1207268 1731288 1742801 1957452 2134849 2246865 2348363 2685676 2923152\n",
      " 3175921 3504792 3736743 4174694 4383747 4408303 4520136] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1207267 1731287 1742800 1957451 2134848 2246864 2348362 2685675 2923151\n",
      " 3175920 3504791 3736742 4174693 4383746 4408302 4520135]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1207267 1731287 1742800 1957451 2134848 2246864 2348362 2685675 2923151\n",
      " 3175920 3504791 3736742 4174693 4383746 4408302 4520135]\u001b[0m\n",
      "\u001b[94m- Interpolated HR for  15  ectopic beats at [1207268 1731288 1742801 1957452 2134849 2246865 2348363 2685676 3175921\n",
      " 3504792 3736743 4174694 4383747 4408303 4520136]\u001b[0m\n",
      "BG030c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG030p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG030c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG031\n",
      "- Importing Spike data for BG031\n",
      "BG031p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG031c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 4227709:4228667 \u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks at [2574494 2574697 2934175 2934377]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [4228121]\u001b[0m\n",
      "BG031p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG031c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG032\n",
      "- Importing Spike data for BG032\n",
      "BG032p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG032c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [3036849 4583536 4583773]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [4583719]\u001b[0m\n",
      "BG032p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG032c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG033\n",
      "- Importing Spike data for BG033\n",
      "BG033p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1608885 1609116 1787684 2276949 2277170 2277470 2660925 2661309 2850649\n",
      " 3202721 3515688 3717737 4731722 4731979 4732206 4734835 4974142] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1608884 1609115 1787683 2276948 2277169 2277469 2660924 2661308 2850648\n",
      " 3202720 3515687 3717736 4731721 4731978 4732205 4734834 4974141]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1608884 1609115 1787683 2276948 2277169 2277469 2660924 2661308 2850648\n",
      " 3202720 3515687 3717736 4731721 4731978 4732205 4734834 4974141]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [1609048 2277513 4732152]\u001b[0m\n",
      "BG033c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 4585103:4585876 \u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks at [1610462 2733058 2985014 3558558 3558993 3830445 4000101 4000325 4185910\n",
      " 4186198 4372790 4372995 4373206 4442024 4442257 4442461 4512968]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [4000155 4186258 4372834]\u001b[0m\n",
      "\u001b[94m- Interpolated HR for  1  ectopic beats at [4585456]\u001b[0m\n",
      "BG033p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG033c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG034\n",
      "- Importing Spike data for BG034\n",
      "BG034p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG034c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [4445153]\u001b[0m\n",
      "BG034p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG034c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG035\n",
      "- Importing Spike data for BG035\n",
      "BG035p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG035c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG035p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG035c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG036\n",
      "- Importing Spike data for BG036\n",
      "BG036p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [927595]\u001b[0m\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1133829 1150144 1215192 1547987 1754398 1798952 1970309 2186857 2192621\n",
      " 2211216 2218729 2460889 2483943 2527138] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1133828 1150143 1215191 1547986 1754397 1798951 1970308 2186856 2192620\n",
      " 2211215 2218728 2460888 2483942 2527137]\u001b[0m\n",
      "\u001b[91mHowever no peaks found at [927594]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1133828 1150143 1215191 1547986 1754397 1798951 1970308 2186856 2192620\n",
      " 2211215 2218728 2460888 2483942 2527137]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [2219005]\u001b[0m\n",
      "\u001b[94m- Interpolated HR for  23  ectopic beats at [ 686611  787949  927595 1133829 1150144 1215192 1547987 1754398 1798952\n",
      " 1831759 1970309 2013102 2186857 2192621 2211216 2460889 2483943 2527138\n",
      " 2861419 3046024 3073421 3430020 3508494]\u001b[0m\n",
      "BG036c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [1243814]\u001b[0m\n",
      "BG036p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG036c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG037\n",
      "- Importing Spike data for BG037\n",
      "BG037p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG037c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [768653 768871]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [768816]\u001b[0m\n",
      "BG037p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG037c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG038\n",
      "- Importing Spike data for BG038\n",
      "BG038p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG038c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG038p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG038c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG039\n",
      "- Importing Spike data for BG039\n",
      "BG039p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG039c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG039p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG039c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG040\n",
      "- Importing Spike data for BG040\n",
      "BG040p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [3287940 4126882 4207010] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [3287939 4126881 4207009]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [3287939 4126881 4207009]\u001b[0m\n",
      "BG040c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [1349130 1349493 1751197 1751399 1751829 1905353 2304373 3320892]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [1349439 1751262 1751858]\u001b[0m\n",
      "BG040p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG040c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG041\n",
      "- Importing Spike data for BG041\n",
      "BG041p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG041c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG041p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG041c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG042\n",
      "- Importing Spike data for BG042\n",
      "BG042p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [3230926] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [3230925]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [3230925]\u001b[0m\n",
      "BG042c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 1486962:1487753 \u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks at [1361176 1483991 1658751 1826799 1827203 2087524 2090570 2092108 2092347\n",
      " 2835548 2835771 2836730 3111608 3666517 3711138 4961222 4969061 5046135\n",
      " 5046336]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [1487385 1487809 1827144 2092285 2835679 5046236]\u001b[0m\n",
      "BG042p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG042c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG043\n",
      "- Importing Spike data for BG043\n",
      "BG043p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG043c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [1091340]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [1091478]\u001b[0m\n",
      "BG043p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG043c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG044\n",
      "- Importing Spike data for BG044\n",
      "BG044p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [178378]\u001b[0m\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [2147087 3870875 3871089] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [2147086 3870874 3871088]\u001b[0m\n",
      "\u001b[91mHowever no peaks found at [178377]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [2147086 3870874 3871088]\u001b[0m\n",
      "BG044c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG044p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG044c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG045\n",
      "- Importing Spike data for BG045\n",
      "BG045p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [2000935] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [2000934]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [2000934]\u001b[0m\n",
      "BG045c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG045p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG045c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG046\n",
      "- Importing Spike data for BG046\n",
      "BG046p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG046c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [3101333]\u001b[0m\n",
      "BG046p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG046c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG047\n",
      "- Importing Spike data for BG047\n",
      "BG047p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1170233 3740653] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1170232 3740652]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1170232 3740652]\u001b[0m\n",
      "BG047c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG047p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG047c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "\u001b[94m- Excluded BG047c ECG data\u001b[0m\n",
      "Session:  BG048\n",
      "- Importing Spike data for BG048\n",
      "BG048p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG048c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [1455986 1651258 1651472 3361195 3361434 3503902 4557180]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [1651233 3361263]\u001b[0m\n",
      "BG048p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG048c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG049\n",
      "- Importing Spike data for BG049\n",
      "BG049p peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG049c peak deletion/addition\n",
      "- No peaks deleted\n",
      "BG049p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG049c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "Session:  BG050\n",
      "- Importing Spike data for BG050\n",
      "BG050p peak deletion/addition\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [1097315 2146343 2146561 2147527 2196970 2216763] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [1097314 2146342 2146560 2147526 2196969 2216762]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [1097314 2146342 2146560 2147526 2196969 2216762]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [2146503]\u001b[0m\n",
      "BG050c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [4842514 4842722]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [4842675]\u001b[0m\n",
      "BG050p calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n",
      "BG050c calculating HR responses\n",
      "\u001b[94m- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\u001b[0m\n",
      "\u001b[94m- Successfully calculated HR resposnes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run_ecg_analyses = True #define whether to run this section of skip it\n",
    "hr_visual_verification = False #whether to plot HR trace\n",
    "\n",
    "if run_ecg_analyses:\n",
    "    # Suppress the specific DeprecationWarning\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "    # Visual verification variables\n",
    "    num_plotted = 0\n",
    "    max_plotted = 100\n",
    "    ecg_visual_verif_done = [] #Add session ID here once visual verification done\n",
    "\n",
    "    #------- Peak detection algorithm exceptions\n",
    "    peak_detection_algs = {sessionID: 'sleepecg' for sessionID in session_folders}\n",
    "    #peak_detection_algs['BG026'] = 'hamilton' #option to swap peak detection algorithm to 'hamilton' for selected participants if 'sleepecg' algorithm\n",
    "\n",
    "    #------- Import excel file containing notes from ecg inspection\n",
    "    analysisFolder = BASE_DIR / \"Analysis\"\n",
    "    ecg_inspection_file = os.path.join(analysisFolder, 'Data checks', 'empathy_ecg_inspection.xlsx')\n",
    "    df_ecg_inspection = pd.read_excel(ecg_inspection_file)\n",
    "\n",
    "    #Predefine hr timeseries columns as objects to allow insertion of entire timeseries into each cell\n",
    "    df_all_combined['hr_series_waiting'] = None\n",
    "    df_all_combined = df_all_combined.astype({'hr_series_waiting': 'object'})\n",
    "\n",
    "    df_all_combined['hr_series_anticipation'] = None\n",
    "    df_all_combined = df_all_combined.astype({'hr_series_anticipation': 'object'})\n",
    "\n",
    "    df_all_combined['hr_series_response'] = None\n",
    "    df_all_combined = df_all_combined.astype({'hr_series_response': 'object'})\n",
    "\n",
    "    for sessionID in session_folders:\n",
    "\n",
    "        # get session folder\n",
    "        sessionFolder = os.path.join(directory, sessionID)\n",
    "        print('Session: ', sessionID) \n",
    "        \n",
    "        # Import Spike2 data file\n",
    "        spike_file_extension = \"empathy.smrx\"\n",
    "        spike_search_pattern = os.path.join(sessionFolder, f'*{spike_file_extension}')\n",
    "        spike_files = glob.glob(spike_search_pattern)\n",
    "        if len(spike_files) > 1:\n",
    "            print(\"-\", \"{} SMRX files found.\".format(len(spike_files)))\n",
    "        elif len(spike_files) < 1:\n",
    "            print(RED + 'WARNING: no empathy spike data files found')\n",
    "\n",
    "        # Loop over files\n",
    "        for i_spike_file in range(len(spike_files)):\n",
    "            \n",
    "            spike_filename = spike_files[i_spike_file]\n",
    "\n",
    "            print(\"-\", \"Importing Spike data for {}\".format(sessionID))\n",
    "\n",
    "            # Read Spike2 file\n",
    "            reader = CedIO(filename=spike_filename)\n",
    "            \n",
    "            block = reader.read_block()\n",
    "            spike2data = block.segments[0] # single segment\n",
    "\n",
    "            # Create a dictionary of analog channels and their indexes\n",
    "            analog_channel_dictionary = {}\n",
    "\n",
    "            for data_channel in range(len(spike2data.analogsignals)):\n",
    "                channel_idx = data_channel\n",
    "                channel_name = spike2data.analogsignals[data_channel].array_annotations['channel_names'][0]\n",
    "                analog_channel_dictionary[channel_name] = channel_idx\n",
    "\n",
    "            # ---------- Get the ECG data for the subjects\n",
    "            p_ecg_data = spike2data.analogsignals[analog_channel_dictionary['p_ECG']] #*IMPORTANT\n",
    "            c_ecg_data = spike2data.analogsignals[analog_channel_dictionary['c_ECG']] #*IMPORTANT\n",
    "        \n",
    "        # ---------- Extract textmarks from smrx file using SonPy\n",
    "        smrfile = sonpy.lib.SonFile(spike_filename, True)\n",
    "        textmarkChanNo = 30\n",
    "        chanIdxInFile = textmarkChanNo - 1\n",
    "        max_n_tick = smrfile.ChannelMaxTime(chanIdxInFile)\n",
    "\n",
    "        n_chunks = 4\n",
    "        breakpoints = divide_into_n_breakpoints(max_n_tick, n_chunks)\n",
    "        df_textmarks_allchunks = pd.DataFrame()\n",
    "\n",
    "        for i_chunk in range(n_chunks):\n",
    "            first_sample = breakpoints[i_chunk]\n",
    "            last_sample = breakpoints[i_chunk + 1]\n",
    "\n",
    "            # Adjust the last chunk to ensure it includes the final tick\n",
    "            if i_chunk == n_chunks - 1:\n",
    "                last_sample += 1  # Add 1 to include the very last tick\n",
    "            \n",
    "            nSamplesToImport = last_sample - first_sample\n",
    "            textmarks = smrfile.ReadTextMarks(chan=chanIdxInFile, nMax=nSamplesToImport, tFrom=first_sample, tUpto=last_sample)\n",
    "\n",
    "            textmarks_sampleNo = np.full(len(textmarks), np.nan)\n",
    "            textmarks_text = np.array(['---'] * len(textmarks), dtype=str)\n",
    "\n",
    "            for i_mark in range(len(textmarks)):\n",
    "                mark = textmarks[i_mark]\n",
    "                textmarks_sampleNo[i_mark] = round(mark.Tick / 100)\n",
    "                textmarks_text[i_mark] = ''.join([str(mark[0]), str(mark[1]), str(mark[2])])\n",
    "            \n",
    "            df_textmarks = pd.DataFrame({'sampleNo': textmarks_sampleNo, 'text': textmarks_text})\n",
    "            df_textmarks_allchunks = pd.concat([df_textmarks_allchunks, df_textmarks], ignore_index=True)\n",
    "        \n",
    "        \n",
    "        # Get details of ecg data\n",
    "        p_ecg_data_np = np.squeeze(p_ecg_data)\n",
    "        c_ecg_data_np = np.squeeze(c_ecg_data)\n",
    "        p_ecg_signal = p_ecg_data.magnitude.squeeze()\n",
    "        c_ecg_signal = c_ecg_data.magnitude.squeeze()\n",
    "        fps_p_ecg = int(p_ecg_data.sampling_rate.magnitude)\n",
    "        fps_c_ecg = int(c_ecg_data.sampling_rate.magnitude)\n",
    "\n",
    "        #*IMPORTANT - Process exceptions\n",
    "        if sessionID == \"BG004\":\n",
    "            p_ecg_data_np.magnitude[3933300:3975500] = 0\n",
    "            p_ecg_signal[3933300:3975500] = 0\n",
    "\n",
    "        if sessionID == \"BG026\":\n",
    "            p_ecg_data_np.magnitude[0:343500] = 0\n",
    "            p_ecg_signal[0:343500] = 0\n",
    "\n",
    "        # Detect R peaks *IMPORTANT* lines\n",
    "        _, p_ecg_peaks = ecg_peaks(signal=p_ecg_data_np, method='sleepecg', sfreq=fps_p_ecg) # participant data - use sleepecg algorithm from Systole\n",
    "        _, c_ecg_peaks = ecg_peaks(signal=c_ecg_data_np, method='sleepecg', sfreq=fps_c_ecg) # companion data - use sleepecg algorithm from Systole\n",
    "\n",
    "\n",
    "        if hr_visual_verification and sessionID not in ecg_visual_verif_done and num_plotted < max_plotted:\n",
    "            plot_ECG_peaks(sessionID, 'p', p_ecg_data, p_ecg_peaks, p_ecg_signal, df_textmarks_allchunks, resting_or_main = 'main')\n",
    "            plot_ECG_peaks(sessionID, 'c', c_ecg_data, c_ecg_peaks, c_ecg_signal, df_textmarks_allchunks, resting_or_main = 'main')\n",
    "        \n",
    "        #------Participant - Delete entire sections of mislabelled peaks-------\n",
    "        print(sessionID + 'p' + ' peak deletion/addition')\n",
    "\n",
    "        # Find rows in ecg inspection excel file for this recording from this participant\n",
    "        row_p_recording = (df_ecg_inspection['pID'] == sessionID + 'p')\n",
    "\n",
    "        # Delete entire sections of peaks (for replacing slightly but acceptably noisy sections with specific peaks listed in add_peaks)\n",
    "        p_idx_sections_to_delete_peaks = df_ecg_inspection.loc[row_p_recording, 'sections_all_peaks_zero'] # Get sample numbers of entire sections of peaks to delete\n",
    "\n",
    "        if isinstance(p_idx_sections_to_delete_peaks, str): # Ensure it's a list or iterable of strings, in case it's a single value\n",
    "            p_idx_sections_to_delete_peaks = [p_idx_sections_to_delete_peaks]\n",
    "\n",
    "        p_indices_to_delete = [] # Initialise an empty list to collect all indices to delete\n",
    "\n",
    "        for cell_value in p_idx_sections_to_delete_peaks: # Process each cell value\n",
    "            if cell_value.lower() == 'none':  # Skip cells with 'none'\n",
    "                continue\n",
    "            for section in cell_value.split(','):  # Split multiple ranges by commas\n",
    "                start, end = map(int, section.split(':'))  # Split each range and convert to integers\n",
    "                p_indices_to_delete.extend(range(start, end + 1))  # Include the endpoint\n",
    "\n",
    "        if p_indices_to_delete: # Convert to a numpy array for assignment if there are indices to delete\n",
    "            p_indices_to_delete = np.array(p_indices_to_delete, dtype=int)\n",
    "            # Delete all peaks in the specified sections\n",
    "            p_ecg_peaks[p_indices_to_delete] = False  # Set the values to False\n",
    "            print(\n",
    "                BLUE + '-',\n",
    "                'Successfully deleted peaks from entire ranges:',\n",
    "                ', '.join(p_idx_sections_to_delete_peaks),\n",
    "                RESET\n",
    "            )\n",
    "\n",
    "\n",
    "        #------Participant - Delete specific mislabelled peaks-------\n",
    "        # Get sample numbers of peaks to delete\n",
    "        idx_p_peaks_to_delete = df_ecg_inspection.loc[row_p_recording, 'delete_peaks']\n",
    "        idx_p_peaks_to_delete = idx_p_peaks_to_delete.to_numpy() #convert from series to numpy\n",
    "        idx_p_peaks_to_delete = np.fromstring(','.join(idx_p_peaks_to_delete), sep=',', dtype=float)#convert to float\n",
    "        idx_p_peaks_to_delete = idx_p_peaks_to_delete.astype(int)\n",
    "\n",
    "        if len(idx_p_peaks_to_delete) == 0:\n",
    "            print('-', 'No peaks deleted')\n",
    "\n",
    "        else:\n",
    "            #check that there is a peak at this sample number (i.e. that the value in p_ecg_peaks at that sample is 1)\n",
    "            p_peak_vals = p_ecg_peaks[idx_p_peaks_to_delete]\n",
    "            p_mislabelled_peaks = idx_p_peaks_to_delete[p_peak_vals == 0] #get sample numbers where there is no peak to delete\n",
    "            p_truelabelled_peaks = idx_p_peaks_to_delete[p_peak_vals == 1] #get sample numbers where there is indeed a peak to delete\n",
    "\n",
    "            #Delete peaks\n",
    "            if len(p_truelabelled_peaks) > 0:\n",
    "                p_ecg_peaks[p_truelabelled_peaks] = False #change from one to zero in p_ecg_peaks_all_chunks to delete peak\n",
    "                print(BLUE + '-', 'Successfully deleted peaks at',  str(p_truelabelled_peaks) + RESET)\n",
    "\n",
    "            # Print warning if there is no peak at the sample no. then print warning\n",
    "            if len(p_mislabelled_peaks) > 0:\n",
    "                print(RED + 'ERROR: No peaks to delete at sample nos. ', p_mislabelled_peaks, RESET) #print message\n",
    "\n",
    "                #For some participants the sample numbers are shifted by 1 so check if there are peaks on the previous samples\n",
    "                p_peak_vals_min1 = p_ecg_peaks[idx_p_peaks_to_delete - 1]\n",
    "                p_mislabelled_peaks_min1 = idx_p_peaks_to_delete[p_peak_vals_min1 == 0]\n",
    "                #or next samples\n",
    "                p_peak_vals_plus1 = p_ecg_peaks[idx_p_peaks_to_delete + 1]\n",
    "                p_mislabelled_peaks_plus1 = idx_p_peaks_to_delete[p_peak_vals_plus1 == 0]\n",
    "\n",
    "                if any(p_peak_vals_min1) == 1:\n",
    "                    #If all peaks are present 1 sample previously then process\n",
    "                    print(BLUE + 'However peaks found on previous samples ' + str(idx_p_peaks_to_delete[p_peak_vals_min1 == 1]-1) + RESET) \n",
    "\n",
    "                    if len(p_mislabelled_peaks_min1 > 0 ):\n",
    "                        print(RED + 'However no peaks found at ' + str(p_mislabelled_peaks_min1-1) + RESET)\n",
    "\n",
    "                    #delete peaks from previous samples\n",
    "                    #p_ecg_peaks[idx_p_peaks_to_delete[p_peak_vals_min1 == 1]-1] = 0 #change from one to zero in p_ecg_peaks_all_chunks to delete peak\n",
    "                    print(BLUE + '-', 'Successfully deleted peaks from previous samples at',  str(idx_p_peaks_to_delete[p_peak_vals_min1 == 1]-1) + RESET)\n",
    "                    \n",
    "\n",
    "                if any(p_peak_vals_plus1) == 1:\n",
    "                    #If any peaks are present 1 sample subsequently then process\n",
    "                    print(BLUE + 'However peaks found on next samples ' + str(idx_p_peaks_to_delete[p_peak_vals_plus1 == 1]) + RESET) \n",
    "\n",
    "                    if len(p_mislabelled_peaks_plus1 > 0 ):\n",
    "                        print(RED + 'However no peaks found at ' + str(p_mislabelled_peaks_plus1) + RESET)\n",
    "\n",
    "                    #delete peaks from subsequent samples\n",
    "                    #p_ecg_peaks[idx_p_peaks_to_delete[p_peak_vals_plus1 == 1]] = 0 #change from one to zero in p_ecg_peaks_all_chunks to delete peak\n",
    "                    print(BLUE + '-', 'Successfully deleted peaks from subsequent samples at',  str(idx_p_peaks_to_delete[p_peak_vals_plus1 == 1]) + RESET)\n",
    "\n",
    "            #Delete peaks on sample, previous sample, and next sample to be sure\n",
    "            p_ecg_peaks[idx_p_peaks_to_delete] = False\n",
    "            p_ecg_peaks[idx_p_peaks_to_delete - 1] = False\n",
    "            p_ecg_peaks[idx_p_peaks_to_delete + 1] = False\n",
    "\n",
    "        #-----------Participant - Add missed peaks---------------------\n",
    "        # Get sample numbers of peaks to add\n",
    "        idx_p_peaks_to_add = df_ecg_inspection.loc[row_p_recording, 'add_peaks']\n",
    "        idx_p_peaks_to_add = idx_p_peaks_to_add.to_numpy() #convert from series to numpy\n",
    "        idx_p_peaks_to_add = np.fromstring(','.join(idx_p_peaks_to_add), sep=',', dtype=float)#convert to float\n",
    "        idx_p_peaks_to_add = idx_p_peaks_to_add.astype(int)\n",
    "\n",
    "        #add peak to p_ecg_peaks\n",
    "        if len(idx_p_peaks_to_add) > 0:\n",
    "            p_ecg_peaks[idx_p_peaks_to_add] = 1 #change to 1 in p_ecg_peaks\n",
    "            print(BLUE + '-', 'Added missing peaks ', str(idx_p_peaks_to_add) + RESET)\n",
    "\n",
    "        #----------Participant - calculate instantaneous HR---------------------\n",
    "        #Calculate instantaneous HR from subj_ecg_peaks\n",
    "        p_inst_hr = calculate_instantaneous_hr(p_ecg_peaks, SPIKE_FS)\n",
    "\n",
    "        #-----------Participant - interpolate HR for R-R intervals containing ectopic beats---------------------\n",
    "        #Interpolate location of ectopic beats, but only for ectopics that increase the IBI,other ectopics are just deleted\n",
    "        # Get sample numbers of approximate positions to add beat\n",
    "        idx_p_ectopics_to_interpolate = df_ecg_inspection.loc[row_p_recording, 'peak_to_interpolate']\n",
    "        idx_p_ectopics_to_interpolate = idx_p_ectopics_to_interpolate.to_numpy()\n",
    "        idx_p_ectopics_to_interpolate = np.fromstring(','.join(idx_p_ectopics_to_interpolate), sep=',', dtype=float)\n",
    "        idx_p_ectopics_to_interpolate = idx_p_ectopics_to_interpolate.astype(int)\n",
    "\n",
    "        #Delete ectopic beats\n",
    "        if len(idx_p_ectopics_to_interpolate) > 0:\n",
    "            p_ecg_peaks[idx_p_ectopics_to_interpolate] = False\n",
    "            p_ecg_peaks[idx_p_ectopics_to_interpolate -1] = False\n",
    "            p_ecg_peaks[idx_p_ectopics_to_interpolate +1] = False\n",
    "\n",
    "        # Interpolate HR for R-R intervals containing ectopic beats\n",
    "        if len(idx_p_ectopics_to_interpolate) > 0:\n",
    "            p_inst_hr = interpolate_ectopic_hr(p_inst_hr, p_ecg_peaks, idx_p_ectopics_to_interpolate)\n",
    "            print(BLUE + '-', 'Interpolated HR for ', str(len(idx_p_ectopics_to_interpolate)), ' ectopic beats at', str(idx_p_ectopics_to_interpolate) + RESET)\n",
    "\n",
    "        #------companion - Delete entire section of mislabelled peaks-------\n",
    "        print(sessionID + 'c' + ' peak deletion/addition')\n",
    "\n",
    "        # Find rows in ecg inspection excel file for this recording from this companion\n",
    "        row_c_recording = (df_ecg_inspection['pID'] == sessionID + 'c')\n",
    "\n",
    "        # Delete entire sections of peaks (for replacing slightly but acceptably noisy sections with specific peaks listed in add_peaks)\n",
    "        c_idx_sections_to_delete_peaks = df_ecg_inspection.loc[row_c_recording, 'sections_all_peaks_zero'] # Get sample numbers of entire sections of peaks to delete\n",
    "\n",
    "        if isinstance(c_idx_sections_to_delete_peaks, str): # Ensure it's a list or iterable of strings, in case it's a single value\n",
    "            c_idx_sections_to_delete_peaks = [c_idx_sections_to_delete_peaks]\n",
    "\n",
    "        c_indices_to_delete = [] # Initialise an empty list to collect all indices to delete\n",
    "\n",
    "        for cell_value in c_idx_sections_to_delete_peaks: # Process each cell value\n",
    "            if cell_value.lower() == 'none':  # Skip cells with 'none'\n",
    "                continue\n",
    "            for section in cell_value.split(','):  # Split multiple ranges by commas\n",
    "                start, end = map(int, section.split(':'))  # Split each range and convert to integers\n",
    "                c_indices_to_delete.extend(range(start, end + 1))  # Include the endpoint\n",
    "\n",
    "        if c_indices_to_delete: # Convert to a numpy array for assignment if there are indices to delete\n",
    "            c_indices_to_delete = np.array(c_indices_to_delete, dtype=int)\n",
    "            # Delete all peaks in the specified sections\n",
    "            c_ecg_peaks[c_indices_to_delete] = False  # Set the values to zero\n",
    "            print(\n",
    "                BLUE + '-',\n",
    "                'Successfully deleted peaks from entire ranges:',\n",
    "                ', '.join(c_idx_sections_to_delete_peaks),\n",
    "                RESET\n",
    "            )\n",
    "        \n",
    "        #------Companion - Delete specific mislabelled peaks-------\n",
    "        # Get sample numbers of peaks to delete\n",
    "        idx_c_peaks_to_delete = df_ecg_inspection.loc[row_c_recording, 'delete_peaks']\n",
    "        idx_c_peaks_to_delete = idx_c_peaks_to_delete.to_numpy() #convert from series to numpy\n",
    "        idx_c_peaks_to_delete = np.fromstring(','.join(idx_c_peaks_to_delete), sep=',', dtype=float)#convert to float\n",
    "        idx_c_peaks_to_delete = idx_c_peaks_to_delete.astype(int)\n",
    "\n",
    "        if len(idx_c_peaks_to_delete) == 0:\n",
    "            print('-', 'No peaks deleted')\n",
    "\n",
    "        else:\n",
    "            #check that there is a peak at this sample number (i.e. that the value in c_ecg_peaks at that sample is 1)\n",
    "            c_peak_vals = c_ecg_peaks[idx_c_peaks_to_delete]\n",
    "            c_mislabelled_peaks = idx_c_peaks_to_delete[c_peak_vals == 0] #get sample numbers where there is no peak to delete\n",
    "            c_truelabelled_peaks = idx_c_peaks_to_delete[c_peak_vals == 1] #get sample numbers where there is indeed a peak to delete\n",
    "\n",
    "            #Delete peaks\n",
    "            if len(c_truelabelled_peaks) > 0:\n",
    "                #c_ecg_peaks[c_truelabelled_peaks] = 0 #change from one to zero in c_ecg_peaks_all_chunks to delete peak\n",
    "                print(BLUE + '-', 'Successfully deleted peaks at',  str(c_truelabelled_peaks) + RESET)\n",
    "\n",
    "            # Print warning if there is no peak at the sample no. then print warning\n",
    "            if len(c_mislabelled_peaks) > 0:\n",
    "                print(RED + 'ERROR: No peaks to delete at sample nos. ', c_mislabelled_peaks, RESET) #print message\n",
    "\n",
    "                #For some participants the sample numbers are shifted by 1 so check if there are peaks on the previous samples\n",
    "                c_peak_vals_min1 = c_ecg_peaks[idx_c_peaks_to_delete - 1]\n",
    "                c_mislabelled_peaks_min1 = idx_c_peaks_to_delete[c_peak_vals_min1 == 0]\n",
    "                #or next samples\n",
    "                c_peak_vals_plus1 = c_ecg_peaks[idx_c_peaks_to_delete + 1]\n",
    "                c_mislabelled_peaks_plus1 = idx_c_peaks_to_delete[c_peak_vals_plus1 == 0]\n",
    "\n",
    "                if any(c_peak_vals_min1) == 1:\n",
    "                    #If any peaks are present 1 sample previously then process\n",
    "                    print(BLUE + 'However peaks found on previous samples ' + str(idx_c_peaks_to_delete[c_peak_vals_min1 == 1]-1) + RESET) \n",
    "\n",
    "\n",
    "                if any(c_peak_vals_plus1) == 1:\n",
    "                    #If any peaks are present 1 sample subsequently then process\n",
    "                    print(BLUE + 'However peaks found on next samples ' + str(idx_c_peaks_to_delete[c_peak_vals_plus1 == 1]) + RESET) \n",
    "\n",
    "                    if len(c_mislabelled_peaks_plus1 > 0 ):\n",
    "                        print(RED + 'However no peaks found at ' + str(c_mislabelled_peaks_plus1) + RESET)\n",
    "\n",
    "            #Delete peaks on sample, previous sample, and next sample to be sure\n",
    "            c_ecg_peaks[idx_c_peaks_to_delete] = False\n",
    "            c_ecg_peaks[idx_c_peaks_to_delete - 1] = False\n",
    "            c_ecg_peaks[idx_c_peaks_to_delete + 1] = False\n",
    "\n",
    "        #-----------companion - Add missed peaks---------------------\n",
    "        # Get sample numbers of peaks to add\n",
    "        idx_c_peaks_to_add = df_ecg_inspection.loc[row_c_recording, 'add_peaks']\n",
    "        idx_c_peaks_to_add = idx_c_peaks_to_add.to_numpy() #convert from series to numpy\n",
    "        idx_c_peaks_to_add = np.fromstring(','.join(idx_c_peaks_to_add), sep=',', dtype=float)#convert to float\n",
    "        idx_c_peaks_to_add = idx_c_peaks_to_add.astype(int)\n",
    "\n",
    "        #add peak to c_ecg_peaks_all_chunks\n",
    "        if len(idx_c_peaks_to_add) > 0:\n",
    "            c_ecg_peaks[idx_c_peaks_to_add] = True #change to 1 in c_ecg_peaks_all_chunks\n",
    "            print(BLUE + '-', 'Added missing peaks ', str(idx_c_peaks_to_add) + RESET)\n",
    "        \n",
    "        #----------Companion - calculate instantaneous HR---------------------\n",
    "        #Calculate instantaneous HR from subj_ecg_peaks\n",
    "        c_inst_hr = calculate_instantaneous_hr(c_ecg_peaks, SPIKE_FS)\n",
    "\n",
    "        #-----------Participant - Interpolate ectopic beats---------------------\n",
    "        #Interpolate location of ectopic beats, but only for ectopics that increase the IBI,other ectopics are just deleted\n",
    "        # Get sample numbers of approximate positions to add beat\n",
    "        idx_c_ectopics_to_interpolate = df_ecg_inspection.loc[row_c_recording, 'peak_to_interpolate']\n",
    "        idx_c_ectopics_to_interpolate = idx_c_ectopics_to_interpolate.to_numpy()\n",
    "        idx_c_ectopics_to_interpolate = np.fromstring(','.join(idx_c_ectopics_to_interpolate), sep=',', dtype=float)\n",
    "        idx_c_ectopics_to_interpolate = idx_c_ectopics_to_interpolate.astype(int)\n",
    "\n",
    "        # Interpolate HR for R-R intervals containing ectopic beats\n",
    "        if len(idx_c_ectopics_to_interpolate) > 0:\n",
    "            c_inst_hr = interpolate_ectopic_hr(c_inst_hr, c_ecg_peaks, idx_c_ectopics_to_interpolate)\n",
    "            print(BLUE + '-', 'Interpolated HR for ', str(len(idx_c_ectopics_to_interpolate)), ' ectopic beats at', str(idx_c_ectopics_to_interpolate) + RESET)\n",
    "        \n",
    "        #IMPORTANT - process exceptions\n",
    "        if sessionID == 'BG002':\n",
    "            #start from row idx 19\n",
    "            df_textmarks_allchunks = df_textmarks_allchunks.iloc[19:]\n",
    "\n",
    "        elif sessionID == 'BG021':\n",
    "            #add rat textmark at last sample of spike file (when it crashed)\n",
    "            new_row_rating = pd.DataFrame({\"sampleNo\": [5509836], \"text\": ['rat']})  # Create a new DataFrame with the new row\n",
    "            df_textmarks_allchunks = pd.concat([df_textmarks_allchunks, new_row_rating], ignore_index=True)  # Append the row  \n",
    "        \n",
    "        #generate IDs of both subjects in session\n",
    "        subjsInSession = [\"p\", \"c\"]\n",
    "\n",
    "        for subj in subjsInSession:\n",
    "            \n",
    "            subjID = sessionID + subj\n",
    "\n",
    "            print(subjID + ' calculating HR responses')\n",
    "\n",
    "            #check that number of anticipation and shock textmarks matches number of trials\n",
    "            nTrialsInTextmarks = len(df_textmarks_allchunks[df_textmarks_allchunks['text'].isin(['apr'])]) #get number of trials in textmarks\n",
    "            subj_rows_combined = df_all_combined['subjectID'].str.contains(subjID, na=False, case=False)  #get subject rows in df_all_combined\n",
    "\n",
    "            if nTrialsInTextmarks != sum(subj_rows_combined):\n",
    "                print(RED + 'ERROR: there are ' + str(nTrialsInTextmarks) + \n",
    "                    ' apr textmarks but ' + str(sum(subj_rows_combined)) + \n",
    "                    ' apr textmarks but ' + str(sum(subj_rows_combined)) + \n",
    "                    ' rows of data for this subject in df_all_combined' + RESET)\n",
    "            else:\n",
    "                print(BLUE + '- No. of apr textmarks (' + str(nTrialsInTextmarks) + \n",
    "                    ') matches no. rows (' + str(sum(subj_rows_combined)) + \n",
    "                    ') data for this subject in df_all_combined' + RESET)\n",
    "        \n",
    "            \n",
    "            #create array specifying noisy sections\n",
    "            row_subj_recording = (df_ecg_inspection['pID'] == subjID)# Find rows in ecg inspection excel file for this subject\n",
    "            idx_noisy_section = df_ecg_inspection.loc[row_subj_recording, 'bad_segments'] # Get sample numbers of entire sections of peaks to delete\n",
    "\n",
    "            if isinstance(idx_noisy_section, str): # Ensure it's a list or iterable of strings, in case it's a single value\n",
    "                idx_noisy_section = [idx_noisy_section]\n",
    "\n",
    "            idx_noisy_trials = [] # Initialise an empty list to collect all indices to delete\n",
    "\n",
    "            for cell_value in idx_noisy_section: # Process each cell value\n",
    "                if cell_value.lower() == 'none':  # Skip cells with 'none'\n",
    "                    continue\n",
    "                for section in cell_value.split(','):  # Split multiple ranges by commas\n",
    "                    print(BLUE + '- Processing noisy sections' + RESET)\n",
    "                    start, end = map(int, section.split(':'))  # Split each range and convert to integers\n",
    "                    idx_noisy_trials.extend(range(start, end + 1))  # Include the endpoint\n",
    "\n",
    "            if subj == \"p\": #IMPORTANT - defines array for calculating HR responses\n",
    "                subj_ecg_peaks = p_ecg_peaks\n",
    "                subj_inst_hr = p_inst_hr\n",
    "            elif subj == \"c\":\n",
    "                subj_ecg_peaks = c_ecg_peaks\n",
    "                subj_inst_hr = c_inst_hr\n",
    "            else:\n",
    "                print(RED + 'ERROR - issue defining subj_ecg_peaks - subj not recognised')\n",
    "\n",
    "            #Process exceptions\n",
    "            if subjID == 'BG006p':\n",
    "                #BG006 p and c psychophys channels are wrong way round for empathy task. Swap over!\n",
    "                subj_ecg_peaks = c_ecg_peaks\n",
    "                subj_inst_hr = c_inst_hr\n",
    "            elif subjID == 'BG006c':\n",
    "                 subj_ecg_peaks = p_ecg_peaks\n",
    "                 subj_inst_hr = p_inst_hr\n",
    "\n",
    "            subj_noisy_samples = np.full(len(subj_ecg_peaks), False, dtype=bool) #define logical array\n",
    "            subj_noisy_samples[idx_noisy_trials] = True #set noisy trials to true\n",
    "\n",
    "            # Iterate through each trial and calculate mean HR and mean HR change in each section\n",
    "            #> Filter rows for the relevant textmarks in df_textmarks_all_chunks\n",
    "            relevant_textmarks = [\"apr\", #anticipation pre-name\n",
    "                                  \"apo\", #anticipation post-name\n",
    "                                  \"sph\", \"spl\", \"sps\", \"sch\", \"scl\", \"scs\", #shock delivery\n",
    "                                  \"res\", #start of response period\n",
    "                                  \"rat\"] #submit rating/experimenter change shock settings\n",
    "            filtered_textmarks = df_textmarks_allchunks[df_textmarks_allchunks[\"text\"].isin(relevant_textmarks)]\n",
    "\n",
    "            # Find all indices of the \"apr\" textmark (one for each trial)\n",
    "            apr_indices = filtered_textmarks[filtered_textmarks[\"text\"] == \"apr\"].index\n",
    "\n",
    "            # Iterate through each trial using \"apr\" textmarks as the starting point\n",
    "            for trial_idx, apr_idx in enumerate(apr_indices):\n",
    "                \n",
    "                #Calculate trial number (start at 1 not 0)\n",
    "                trial_no = trial_idx + 1\n",
    "                \n",
    "                #calculate subject row as logical array\n",
    "                subject_row = (df_all_combined['subjectID'].str.contains(subjID, na=False, case=False)) & (df_all_combined['trialNo'] == trial_no)\n",
    "\n",
    "                # Get the indices of the textmarks for this trial\n",
    "                sample_no_apr = int(filtered_textmarks.loc[apr_idx, \"sampleNo\"])\n",
    "\n",
    "                # Get the next \"apo\" textmark (signalling start of anticipation post name period) after \"apr\"\n",
    "                sample_no_apo = filtered_textmarks.loc[\n",
    "                    (filtered_textmarks.index > apr_idx) & (filtered_textmarks[\"text\"] == \"apo\"), \"sampleNo\"\n",
    "                ].iloc[0]\n",
    "\n",
    "                # Get the first shock (sph, spl, sps, sch, scl, scs) textmark after \"apo\"\n",
    "                sample_no_shock = filtered_textmarks.loc[\n",
    "                    (filtered_textmarks.index > apr_idx)\n",
    "                    & (filtered_textmarks[\"text\"].isin([\"sph\", \"spl\", \"sps\", \"sch\", \"scl\", \"scs\"])),\n",
    "                    \"sampleNo\",\n",
    "                ].iloc[0]\n",
    "\n",
    "                #Get the first response period (res) textmark after \"apo\"\n",
    "                sample_no_res = filtered_textmarks.loc[\n",
    "                    (filtered_textmarks.index > apr_idx) & (filtered_textmarks[\"text\"] == \"res\"), \"sampleNo\"\n",
    "                ].iloc[0]\n",
    "\n",
    "                # Get the next \"rat\" textmark (signalling rating) after anticipation post name\n",
    "                sample_no_rat = filtered_textmarks.loc[\n",
    "                    (filtered_textmarks.index > apr_idx)\n",
    "                    & (filtered_textmarks[\"text\"] == \"rat\"),\n",
    "                    \"sampleNo\",\n",
    "                ].iloc[0]\n",
    "\n",
    "                #Convert indices to integers\n",
    "                sample_no_apr = int(sample_no_apr)\n",
    "                sample_no_apo = int(sample_no_apo)\n",
    "                sample_no_shock = int(sample_no_shock)\n",
    "                sample_no_res = int(sample_no_res)\n",
    "                sample_no_rat = int(sample_no_rat)\n",
    "\n",
    "                \"\"\" # Important - check durations of each phase are correct\n",
    "                #Waiting phase\n",
    "                if (sample_no_apo - sample_no_apr > 3100) | (sample_no_apo - sample_no_apr < 2900) :\n",
    "                     print(RED + '- Trial ' +  str(trial_no) + ', Waiting Phase is ' + str(sample_no_apo - sample_no_apr) + ' ms' + RESET)\n",
    "                     #print(BLUE + '- Using 3000 ms as Waiting Phase duration' + RESET)\n",
    "\n",
    "                #Response phase\n",
    "                if (sample_no_rat - sample_no_res > 6100) | (sample_no_rat - sample_no_res < 5900):\n",
    "                    #print(RED + '- Trial ' +  str(trial_no) + ', Response Phase > 6100 ms' + RESET)\n",
    "                    print(RED + '- Trial ' +  str(trial_no) + ', Response Phase is ' + str(sample_no_rat - sample_no_res) + ' ms' + RESET)\n",
    "\n",
    "                #Process exceptions\n",
    "                #> For BG035 trial number 2 'rat' textmark is late so trim to 6000 ms\n",
    "                if (sessionID == 'BG035') & (trial_no == 19):\n",
    "                    sample_no_rat = sample_no_res + 6000\n",
    "                    print(BLUE + '- Trial ' +  str(trial_no) + ', Corrected Response Phase textmarks to 6000 ms' + RESET) \"\"\"\n",
    "\n",
    "                #Anticipation phase\n",
    "                if (sample_no_shock - sample_no_apo > 10100) | (sample_no_shock - sample_no_apo < 5900):\n",
    "                    #print(RED + '- Trial ' +  str(trial_no) + ', Anticipation Phase is ' + str(sample_no_shock - sample_no_apo) + ' ms' + RESET)\n",
    "                    \n",
    "                    #If antiicpation phase is incorrect length, work out shock time as 500ms before start of response phase\n",
    "                    sample_no_shock = sample_no_res - DURATION_SHOCK\n",
    "                    #print(BLUE + '- Trial ' +  str(trial_no) + ', New Anticipation Phase duration: ' + str(sample_no_shock - sample_no_apo) + ' ms' + RESET)\n",
    "                    \n",
    "                    #if this does not fix it then print error\n",
    "                    if (sample_no_shock - sample_no_apo > 10100) | (sample_no_shock - sample_no_apo < 5900):\n",
    "                        print(RED + '- ERROR: Trial ' +  str(trial_no) + ', Anticipation Phase is ' + str(sample_no_shock - sample_no_apo) + ' ms even after using \"res\" textmark.' + RESET)\n",
    "                \n",
    "\n",
    "                #------ Calculate HR measures for each phase\n",
    "\n",
    "                # Calculate mean HR for each phase\n",
    "                mean_hr_waiting = calculate_mean_hr(sample_no_apr, sample_no_apr + DURATION_WAITING, subj_inst_hr, subj_noisy_samples, SPIKE_FS)\n",
    "                mean_hr_anticipation = calculate_mean_hr(sample_no_apo, sample_no_shock, subj_inst_hr, subj_noisy_samples, SPIKE_FS)\n",
    "                mean_hr_response = calculate_mean_hr(sample_no_shock + DURATION_SHOCK, sample_no_shock + DURATION_SHOCK + DURATION_RESPONSE_EXCL_SHOCK, subj_inst_hr, subj_noisy_samples, SPIKE_FS) #do not include shock\n",
    "\n",
    "                # Calculate instantaneous HR timeseries during each phase\n",
    "                hr_timeseries_waiting = calculate_hr_timeseries(sample_no_apr - DURATION_PRE_PHASE, sample_no_apr + DURATION_WAITING, subj_inst_hr, subj_noisy_samples, orig_fs = SPIKE_FS, target_fs = 5)\n",
    "                hr_timeseries_anticipation = calculate_hr_timeseries(sample_no_apo - DURATION_PRE_PHASE, sample_no_shock, subj_inst_hr, subj_noisy_samples, orig_fs = SPIKE_FS, target_fs = 5)\n",
    "                hr_timeseries_response = calculate_hr_timeseries(sample_no_shock - DURATION_PRE_PHASE, sample_no_shock + DURATION_SHOCK + DURATION_RESPONSE_EXCL_SHOCK, subj_inst_hr, subj_noisy_samples, orig_fs = SPIKE_FS, target_fs = 5) #include shock in timeseries\n",
    "\n",
    "                #if subject_row is valid then add HR and HR change values to dataframe\n",
    "                if sum(subject_row) == 1:\n",
    "                    # Add mean HR values to the dataframe\n",
    "                    df_all_combined.loc[subject_row, \"mean_hr_waiting\"] = round(mean_hr_waiting, 4)\n",
    "                    df_all_combined.loc[subject_row, \"mean_hr_anticipation\"] = round(mean_hr_anticipation, 4)\n",
    "                    df_all_combined.loc[subject_row, \"mean_hr_response\"] = round(mean_hr_response, 4)\n",
    "\n",
    "                    # Add HR timeseries to the dataframe\n",
    "                    #>Assign as a string representation of the list\n",
    "                    df_all_combined.loc[subject_row, 'hr_series_waiting'] = str(hr_timeseries_waiting.tolist())\n",
    "                    df_all_combined.loc[subject_row, 'hr_series_anticipation'] = str(hr_timeseries_anticipation.tolist())\n",
    "                    df_all_combined.loc[subject_row, 'hr_series_response'] = str(hr_timeseries_response.tolist())\n",
    "\n",
    "                    #Process exceptions\n",
    "\n",
    "                    #> BG021 trial 79 crashed during response phase so set responses to nan\n",
    "                    if (sessionID == 'BG021') & (trial_no == 79):\n",
    "                        df_all_combined.loc[subject_row, \"mean_hr_response\"] = np.nan\n",
    "                        df_all_combined.loc[subject_row, 'hr_series_response'] = str( np.array([np.nan]) )\n",
    "                        print(BLUE + 'Set HR responses for trial 79 resposne phase to nan' + RESET)\n",
    "\n",
    "                    #> BG040c has very long IBI immediately before trial 40. HOWEVER this is a genuine long IBI so have left unchanged. \n",
    "\n",
    "                elif sum(subject_row) > 1 :\n",
    "                    print(RED + f\"WARNING: multiple subject_rows {subject_row} is in df_all_combined.\" + RESET)\n",
    "                elif sum(subject_row) == 0 :\n",
    "                    print(RED + f\"WARNING: subject_row {subject_row} is not in df_all_combined.\" + RESET)\n",
    "\n",
    "            #Print message\n",
    "            print(BLUE + '- Successfully calculated HR resposnes' + RESET)\n",
    "\n",
    "        #Process exceptions (BG047 - exclude all HR data due to ventricular arrhythmia)\n",
    "        if subjID == 'BG047c': \n",
    "            print(BLUE + '- Excluded BG047c ECG data' + RESET)\n",
    "            # Mean HR\n",
    "            df_all_combined.loc[subj_rows_combined, \"mean_hr_waiting\"] = pd.NA\n",
    "            df_all_combined.loc[subj_rows_combined, \"mean_hr_anticipation\"] = pd.NA\n",
    "            df_all_combined.loc[subj_rows_combined, \"mean_hr_response\"] = pd.NA\n",
    "\n",
    "            #HR timeseries\n",
    "            df_all_combined.loc[subj_rows_combined, \"hr_series_waiting\"] = str( np.array([np.nan]) )\n",
    "            df_all_combined.loc[subj_rows_combined, \"hr_series_anticipation\"] = str( np.array([np.nan]) )\n",
    "            df_all_combined.loc[subj_rows_combined, 'hr_series_response'] = str( np.array([np.nan]) )\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as csv\n",
    "- Option to save for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_destination = os.path.join(BASE_DIR / \"Participant data\" / \"Preprocessed task data\" / \"Empathy task\",\n",
    "#                                \"df_all_combined_withHR.csv\")\n",
    "\n",
    "#df_all_combined.to_csv(save_destination, index=False)  # Optional: index=False to avoid writing row indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data from participant notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session:  BG001\n",
      "BG001 extracting word doc\n",
      "BG001p extracting blood pressure\n",
      "- Blood pressure = 120/72\n",
      "- EDA range = 25\n",
      "BG001c extracting blood pressure\n",
      "- Blood pressure = 123/74\n",
      "- EDA range = 25\n",
      "Session:  BG002\n",
      "BG002 extracting word doc\n",
      "BG002p extracting blood pressure\n",
      "- Blood pressure = 109/71\n",
      "- EDA range = 25\n",
      "BG002c extracting blood pressure\n",
      "- Blood pressure = 115/63\n",
      "- EDA range = 25\n",
      "Session:  BG003\n",
      "BG003 extracting word doc\n",
      "BG003p extracting blood pressure\n",
      "- Blood pressure = 109/70\n",
      "- EDA range = 25\n",
      "BG003c extracting blood pressure\n",
      "- Blood pressure = 124/73\n",
      "- EDA range = 10\n",
      "Session:  BG004\n",
      "BG004 extracting word doc\n",
      "BG004p extracting blood pressure\n",
      "- Blood pressure = 106/72\n",
      "- EDA range = 25\n",
      "BG004c extracting blood pressure\n",
      "- Blood pressure = 122/74\n",
      "- EDA range = 25\n",
      "Session:  BG005\n",
      "BG005 extracting word doc\n",
      "BG005p extracting blood pressure\n",
      "- Blood pressure = 115/70\n",
      "- EDA range = 25\n",
      "BG005c extracting blood pressure\n",
      "- Blood pressure = 128/80\n",
      "- EDA range = 25\n",
      "Session:  BG006\n",
      "BG006 extracting word doc\n",
      "BG006p extracting blood pressure\n",
      "- Blood pressure = 115/69\n",
      "- EDA range = 25\n",
      "BG006c extracting blood pressure\n",
      "- Blood pressure = 128/76\n",
      "- EDA range = 25\n",
      "Session:  BG007\n",
      "BG007 extracting word doc\n",
      "BG007p extracting blood pressure\n",
      "- Blood pressure = 106/70\n",
      "- EDA range = 25\n",
      "BG007c extracting blood pressure\n",
      "- Blood pressure = 108/74\n",
      "- EDA range = 25\n",
      "Session:  BG008\n",
      "BG008 extracting word doc\n",
      "BG008p extracting blood pressure\n",
      "- Blood pressure = 116/78\n",
      "- EDA range = 25\n",
      "BG008c extracting blood pressure\n",
      "- Blood pressure = 110/75\n",
      "- EDA range = 25\n",
      "Session:  BG009\n",
      "BG009 extracting word doc\n",
      "BG009p extracting blood pressure\n",
      "- Blood pressure = 133/81\n",
      "- EDA range = 25\n",
      "BG009c extracting blood pressure\n",
      "- Blood pressure = 116/69\n",
      "- EDA range = 25\n",
      "Session:  BG010\n",
      "BG010 extracting word doc\n",
      "BG010p extracting blood pressure\n",
      "- Blood pressure = 101/65\n",
      "- EDA range = 25\n",
      "BG010c extracting blood pressure\n",
      "- Blood pressure = 100/67\n",
      "- EDA range = 25\n",
      "Session:  BG011\n",
      "BG011 extracting word doc\n",
      "BG011p extracting blood pressure\n",
      "- Blood pressure = 122/72\n",
      "- EDA range = 25\n",
      "BG011c extracting blood pressure\n",
      "- Blood pressure = 152/98\n",
      "- EDA range = 25\n",
      "Session:  BG012\n",
      "BG012 extracting word doc\n",
      "BG012p extracting blood pressure\n",
      "- Blood pressure = 109/78\n",
      "- EDA range = 25\n",
      "BG012c extracting blood pressure\n",
      "- Blood pressure = 109/69\n",
      "- EDA range = 25\n",
      "Session:  BG013\n",
      "BG013 extracting word doc\n",
      "BG013p extracting blood pressure\n",
      "- Blood pressure = 117/76\n",
      "- EDA range = 25\n",
      "BG013c extracting blood pressure\n",
      "- Blood pressure = 142/95\n",
      "- EDA range = 25\n",
      "Session:  BG014\n",
      "BG014 extracting word doc\n",
      "BG014p extracting blood pressure\n",
      "- Blood pressure = 116/78\n",
      "- EDA range = 25\n",
      "BG014c extracting blood pressure\n",
      "- Blood pressure = 134/84\n",
      "- EDA range = 25\n",
      "Session:  BG015\n",
      "BG015 extracting word doc\n",
      "BG015p extracting blood pressure\n",
      "- Blood pressure = 116/63\n",
      "- EDA range = 25\n",
      "BG015c extracting blood pressure\n",
      "- Blood pressure = 106/65\n",
      "- EDA range = 25\n",
      "Session:  BG016\n",
      "BG016 extracting word doc\n",
      "BG016p extracting blood pressure\n",
      "- Blood pressure = 103/53\n",
      "- EDA range = 25\n",
      "BG016c extracting blood pressure\n",
      "- Blood pressure = 122/80\n",
      "- EDA range = 25\n",
      "Session:  BG017\n",
      "BG017 extracting word doc\n",
      "BG017p extracting blood pressure\n",
      "- Blood pressure = 141/74\n",
      "- EDA range = 25\n",
      "BG017c extracting blood pressure\n",
      "- Blood pressure = 112/74\n",
      "- EDA range = 25\n",
      "Session:  BG018\n",
      "BG018 extracting word doc\n",
      "BG018p extracting blood pressure\n",
      "- Blood pressure = 111/74\n",
      "- EDA range = 25\n",
      "BG018c extracting blood pressure\n",
      "- Blood pressure = 88/62\n",
      "- EDA range = 25\n",
      "Session:  BG019\n",
      "BG019 extracting word doc\n",
      "BG019p extracting blood pressure\n",
      "- Blood pressure = 108/61\n",
      "- EDA range = 25\n",
      "BG019c extracting blood pressure\n",
      "- Blood pressure = 123/69\n",
      "- EDA range = 25\n",
      "Session:  BG020\n",
      "BG020 extracting word doc\n",
      "BG020p extracting blood pressure\n",
      "- Blood pressure = 119/71\n",
      "- EDA range = 25\n",
      "BG020c extracting blood pressure\n",
      "- Blood pressure = 120/79\n",
      "- EDA range = 25\n",
      "Session:  BG021\n",
      "BG021 extracting word doc\n",
      "BG021p extracting blood pressure\n",
      "- Blood pressure = 146/86\n",
      "- EDA range = 25\n",
      "BG021c extracting blood pressure\n",
      "- Blood pressure = 134/78\n",
      "- EDA range = 25\n",
      "Session:  BG022\n",
      "BG022 extracting word doc\n",
      "BG022p extracting blood pressure\n",
      "- Blood pressure = 111/82\n",
      "- EDA range = 25\n",
      "BG022c extracting blood pressure\n",
      "- Blood pressure = 117/66\n",
      "- EDA range = 25\n",
      "Session:  BG023\n",
      "BG023 extracting word doc\n",
      "BG023p extracting blood pressure\n",
      "- Blood pressure = 113/73\n",
      "- EDA range = 25\n",
      "BG023c extracting blood pressure\n",
      "- Blood pressure = 104/71\n",
      "- EDA range = 25\n",
      "Session:  BG024\n",
      "BG024 extracting word doc\n",
      "BG024p extracting blood pressure\n",
      "- Blood pressure = 103/65\n",
      "- EDA range = 25\n",
      "BG024c extracting blood pressure\n",
      "- Blood pressure = 120/74\n",
      "- EDA range = 25\n",
      "Session:  BG025\n",
      "BG025 extracting word doc\n",
      "BG025p extracting blood pressure\n",
      "- Blood pressure = 109/71\n",
      "- EDA range = 25\n",
      "BG025c extracting blood pressure\n",
      "- Blood pressure = 128/83\n",
      "- EDA range = 25\n",
      "Session:  BG026\n",
      "BG026 extracting word doc\n",
      "BG026p extracting blood pressure\n",
      "- Blood pressure = 111/76\n",
      "- EDA range = 25\n",
      "BG026c extracting blood pressure\n",
      "- Blood pressure = 124/77\n",
      "- EDA range = 25\n",
      "Session:  BG027\n",
      "BG027 extracting word doc\n",
      "BG027p extracting blood pressure\n",
      "- Blood pressure = 115/81\n",
      "- EDA range = 25\n",
      "BG027c extracting blood pressure\n",
      "- Blood pressure = 143/89\n",
      "- EDA range = 25\n",
      "Session:  BG028\n",
      "BG028 extracting word doc\n",
      "BG028p extracting blood pressure\n",
      "- Blood pressure = 104/71\n",
      "- EDA range = 10\n",
      "BG028c extracting blood pressure\n",
      "- Blood pressure = 108/72\n",
      "- EDA range = 10\n",
      "Session:  BG029\n",
      "BG029 extracting word doc\n",
      "BG029p extracting blood pressure\n",
      "- Blood pressure = 92/60\n",
      "- EDA range = 25\n",
      "BG029c extracting blood pressure\n",
      "- Blood pressure = 113/80\n",
      "- EDA range = 25\n",
      "Session:  BG030\n",
      "BG030 extracting word doc\n",
      "BG030p extracting blood pressure\n",
      "- Blood pressure = 113/79\n",
      "- EDA range = 25\n",
      "BG030c extracting blood pressure\n",
      "- Blood pressure = 114/85\n",
      "- EDA range = 25\n",
      "Session:  BG031\n",
      "BG031 extracting word doc\n",
      "BG031p extracting blood pressure\n",
      "- Blood pressure = 120/80\n",
      "- EDA range = 25\n",
      "BG031c extracting blood pressure\n",
      "- Blood pressure = 125/80\n",
      "- EDA range = 25\n",
      "Session:  BG032\n",
      "BG032 extracting word doc\n",
      "BG032p extracting blood pressure\n",
      "- Blood pressure = 120/79\n",
      "- EDA range = 25\n",
      "BG032c extracting blood pressure\n",
      "- Blood pressure = 121/84\n",
      "- EDA range = 10\n",
      "Session:  BG033\n",
      "BG033 extracting word doc\n",
      "BG033p extracting blood pressure\n",
      "- Blood pressure = 121/74\n",
      "- EDA range = 25\n",
      "BG033c extracting blood pressure\n",
      "- Blood pressure = 119/70\n",
      "- EDA range = 25\n",
      "Session:  BG034\n",
      "BG034 extracting word doc\n",
      "BG034p extracting blood pressure\n",
      "- Blood pressure = 119/74\n",
      "- EDA range = 25\n",
      "BG034c extracting blood pressure\n",
      "- Blood pressure = 130/90\n",
      "- EDA range = 25\n",
      "Session:  BG035\n",
      "BG035 extracting word doc\n",
      "BG035p extracting blood pressure\n",
      "- Blood pressure = 117/71\n",
      "- EDA range = 25\n",
      "BG035c extracting blood pressure\n",
      "- Blood pressure = 121/73\n",
      "- EDA range = 25\n",
      "Session:  BG036\n",
      "BG036 extracting word doc\n",
      "BG036p extracting blood pressure\n",
      "- Blood pressure = 101/68\n",
      "- EDA range = 25\n",
      "BG036c extracting blood pressure\n",
      "- Blood pressure = 107/71\n",
      "- EDA range = 25\n",
      "Session:  BG037\n",
      "BG037 extracting word doc\n",
      "BG037p extracting blood pressure\n",
      "- Blood pressure = 90/66\n",
      "- EDA range = 25\n",
      "BG037c extracting blood pressure\n",
      "- Blood pressure = 110/72\n",
      "- EDA range = 25\n",
      "Session:  BG038\n",
      "BG038 extracting word doc\n",
      "BG038p extracting blood pressure\n",
      "- Blood pressure = 115/76\n",
      "- EDA range = 25\n",
      "BG038c extracting blood pressure\n",
      "- Blood pressure = 122/75\n",
      "- EDA range = 25\n",
      "Session:  BG039\n",
      "BG039 extracting word doc\n",
      "BG039p extracting blood pressure\n",
      "- Blood pressure = 111/68\n",
      "- EDA range = 25\n",
      "BG039c extracting blood pressure\n",
      "- Blood pressure = 124/66\n",
      "- EDA range = 25\n",
      "Session:  BG040\n",
      "BG040 extracting word doc\n",
      "BG040p extracting blood pressure\n",
      "- Blood pressure = 127/78\n",
      "- EDA range = 25\n",
      "BG040c extracting blood pressure\n",
      "- Blood pressure = 134/83\n",
      "- EDA range = 25\n",
      "Session:  BG041\n",
      "BG041 extracting word doc\n",
      "BG041p extracting blood pressure\n",
      "- Blood pressure = 123/72\n",
      "- EDA range = 25\n",
      "BG041c extracting blood pressure\n",
      "- Blood pressure = 136/81\n",
      "- EDA range = 25\n",
      "Session:  BG042\n",
      "BG042 extracting word doc\n",
      "BG042p extracting blood pressure\n",
      "- Blood pressure = 100/69\n",
      "- EDA range = 25\n",
      "BG042c extracting blood pressure\n",
      "- Blood pressure = 129/87\n",
      "- EDA range = 25\n",
      "Session:  BG043\n",
      "BG043 extracting word doc\n",
      "BG043p extracting blood pressure\n",
      "- Blood pressure = 128/83\n",
      "- EDA range = 25\n",
      "BG043c extracting blood pressure\n",
      "- Blood pressure = 121/76\n",
      "- EDA range = 25\n",
      "Session:  BG044\n",
      "BG044 extracting word doc\n",
      "BG044p extracting blood pressure\n",
      "- Blood pressure = 143/84\n",
      "- EDA range = 25\n",
      "BG044c extracting blood pressure\n",
      "- Blood pressure = 146/87\n",
      "- EDA range = 25\n",
      "Session:  BG045\n",
      "BG045 extracting word doc\n",
      "BG045p extracting blood pressure\n",
      "- Blood pressure = 109/71\n",
      "- EDA range = 25\n",
      "BG045c extracting blood pressure\n",
      "- Blood pressure = 136/83\n",
      "- EDA range = 25\n",
      "Session:  BG046\n",
      "BG046 extracting word doc\n",
      "BG046p extracting blood pressure\n",
      "- Blood pressure = 119/71\n",
      "- EDA range = 25\n",
      "BG046c extracting blood pressure\n",
      "- Blood pressure = 125/74\n",
      "- EDA range = 25\n",
      "Session:  BG047\n",
      "BG047 extracting word doc\n",
      "BG047p extracting blood pressure\n",
      "- Blood pressure = 106/67\n",
      "- EDA range = 25\n",
      "BG047c extracting blood pressure\n",
      "- Blood pressure = 135/65\n",
      "- EDA range = 25\n",
      "Session:  BG048\n",
      "BG048 extracting word doc\n",
      "BG048p extracting blood pressure\n",
      "- Blood pressure = 111/71\n",
      "- EDA range = 25\n",
      "BG048c extracting blood pressure\n",
      "- Blood pressure = 122/82\n",
      "- EDA range = 25\n",
      "Session:  BG049\n",
      "BG049 extracting word doc\n",
      "BG049p extracting blood pressure\n",
      "- Blood pressure = 112/66\n",
      "- EDA range = 25\n",
      "BG049c extracting blood pressure\n",
      "- Blood pressure = 118/76\n",
      "- EDA range = 25\n",
      "Session:  BG050\n",
      "BG050 extracting word doc\n",
      "BG050p extracting blood pressure\n",
      "- Blood pressure = 110/65\n",
      "- EDA range = 25\n",
      "BG050c extracting blood pressure\n",
      "- Blood pressure = 137/88\n",
      "- EDA range = 25\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_text_from_docx(doc_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Extract all readable text from a Word (.docx) document.\n",
    "\n",
    "    Text is collected from both paragraphs and tables. Empty paragraphs and\n",
    "    empty table cells are ignored. Table rows are returned as a single string\n",
    "    with cell contents separated by \" | \".\n",
    "\n",
    "    Args:\n",
    "        doc_path (str or Path): File path to the .docx document.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted document text as a single newline-separated string.\n",
    "\n",
    "    Notes:\n",
    "        Requires `Document` from the `python-docx` package.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the Word document\n",
    "    document = Document(doc_path)\n",
    "    \n",
    "    # Collect all text content\n",
    "    text_content = []\n",
    "\n",
    "    # Extract text from paragraphs\n",
    "    for paragraph in document.paragraphs:\n",
    "        if paragraph.text.strip():  # Exclude empty paragraphs\n",
    "            text_content.append(paragraph.text)\n",
    "\n",
    "    # Extract text from tables\n",
    "    for table in document.tables:\n",
    "        for row in table.rows:\n",
    "            row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]  # Get non-empty cell text\n",
    "            if row_text:  # Add row content if not empty\n",
    "                text_content.append(\" | \".join(row_text))  # Combine cell text with ' | ' for readability\n",
    "\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "#predefine\n",
    "df_all_notes = pd.DataFrame()\n",
    "\n",
    "for sessionID in session_folders:\n",
    "\n",
    "    # get session folder\n",
    "    sessionFolder = os.path.join(directory, sessionID)\n",
    "    print('Session: ', sessionID) \n",
    "\n",
    "    print(sessionID + ' extracting word doc')\n",
    "\n",
    "    # Path to the Word document\n",
    "    doc_path = sessionFolder + '/' +  sessionID + ' notes.docx'\n",
    "\n",
    "    # Extract and print the text\n",
    "    full_text = extract_text_from_docx(doc_path)\n",
    "\n",
    "    #generate IDs of both subjects in session\n",
    "    subjsInSession = [\"p\", \"c\"]\n",
    "\n",
    "    for subj in subjsInSession:\n",
    "        \n",
    "        subjID = sessionID + subj\n",
    "\n",
    "        print(subjID + ' extracting blood pressure')\n",
    "        \n",
    "        # Convert to lowercase for case-insensitive search\n",
    "        full_text = full_text.lower()\n",
    "        \n",
    "        # Find all occurrences of \"companion\"\n",
    "        companion_indices = [i for i in range(len(full_text)) if full_text.startswith(\"companion\", i)]\n",
    "\n",
    "        # If there is only one occurence then use it to split the text\n",
    "        if len(companion_indices) > 1:\n",
    "            print(RED + \"Warning: Multiple occurrences of 'companion' found.\" + RESET)\n",
    "        elif len(companion_indices) == 1:\n",
    "            if subj == \"p\":\n",
    "                #take text up to word 'companion'\n",
    "                subj_text = full_text[:companion_indices[0]].strip()\n",
    "            elif subj == \"c\":\n",
    "                #take text from word 'companion' to end\n",
    "                subj_text = full_text[companion_indices[0]:].strip()\n",
    "        else:\n",
    "            print(RED + \"Warning: No occurrences of 'companion' found.\" + RESET)\n",
    "\n",
    "\n",
    "        #extract arm blood pressure\n",
    "        bp_string = \"blood pressure:\"\n",
    "        blood_pressure_indices = [i for i in range(len(subj_text)) if subj_text.lower().startswith(bp_string, i)]\n",
    "        bp_text = subj_text[blood_pressure_indices[-1] + + len(bp_string): blood_pressure_indices[-1] + len(bp_string) +7].strip()\n",
    "        print('- Blood pressure = '  + bp_text)\n",
    "        bp_sys, bp_dia = map(int, bp_text.split('/')) #Split blood pressure text\n",
    "\n",
    "        #extract EDA range\n",
    "        eda_string = \"eda range:\"\n",
    "        eda_range_pressure_indices = [i for i in range(len(subj_text)) if subj_text.lower().startswith(eda_string, i)]\n",
    "        eda_range = int(subj_text[eda_range_pressure_indices[-1] + len(eda_string): eda_range_pressure_indices[-1] + len(eda_string) +4].strip())\n",
    "        print('- EDA range = '  + str(eda_range))\n",
    "\n",
    "        # Extract text between 'hct' and 'hct psychophys'\n",
    "        hct_start_string = \"hct\"\n",
    "        hct_end_string = \"hct psychophys\"\n",
    "\n",
    "        #Extract notes about hct performance\n",
    "        hct_start_index = subj_text.lower().find(hct_start_string)\n",
    "        hct_end_index = subj_text.lower().find(hct_end_string)\n",
    "\n",
    "        if hct_start_index != -1 and hct_end_index != -1 and hct_start_index < hct_end_index:\n",
    "            hct_text = subj_text[hct_start_index + len(hct_start_string):hct_end_index].strip()\n",
    "        else:\n",
    "            print(RED + 'ERROR - problems finding hct_start_index or hct_end_index')\n",
    "            hct_text = None  # Set to None if 'hct' or 'hct psychophys' is not found in the correct order\n",
    "\n",
    "        # Add this subject's notes data to df_all_combined\n",
    "        df_notes = pd.DataFrame()\n",
    "        df_notes.loc[0,'subjID'] = subjID\n",
    "        df_notes.loc[0,'all_notes'] = subj_text\n",
    "        df_all_notes = pd.concat([df_all_notes, df_notes])\n",
    "        p_rows_combined = df_all_combined['subjectID'].str.contains((subjID), na=False, case=False) #get logical array of rows containing participant ID\n",
    "        df_all_combined.loc[p_rows_combined, 'all_notes'] = subj_text\n",
    "        df_all_combined.loc[p_rows_combined, 'arm_sys'] = bp_sys\n",
    "        df_all_combined.loc[p_rows_combined, 'arm_dia'] = bp_dia\n",
    "        df_all_combined.loc[p_rows_combined, 'eda_range'] = eda_range\n",
    "\n",
    "#save df_all_notes\n",
    "df_all_notes.to_csv('Data checks/df_all_notes.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting EDA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eda(sessionID, p_or_c, eda_data, df_textmarks, transfer_constant, save_fig):\n",
    "  \n",
    "    \"\"\"\n",
    "    Plot EDA (electrodermal activity) signal for a session with key task textmarks.\n",
    "\n",
    "    The function applies a transfer constant to convert raw values, detrends the signal,\n",
    "    extracts the phasic component using NeuroKit2, downsamples for visualization, and\n",
    "    plots raw, detrended, and phasic EDA traces using Plotly. Task textmarks are added\n",
    "    as annotated vertical lines.\n",
    "\n",
    "    Args:\n",
    "        sessionID (str): Session identifier.\n",
    "        p_or_c (str): \"p\" for participant or \"c\" for companion.\n",
    "        eda_data: Neo analog signal object containing EDA data and `.times`.\n",
    "        df_textmarks (pd.DataFrame): DataFrame of textmarks with 'text' and 'sampleNo' columns.\n",
    "        transfer_constant (float): Scaling factor applied to the raw EDA signal.\n",
    "        save_fig (bool): If True, saves the figure as a PNG.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays an interactive Plotly figure and optionally saves it.\n",
    "\n",
    "    Notes:\n",
    "        - Assumes `SPIKE_FS` is defined globally for converting sample numbers to seconds.\n",
    "        - Requires Plotly (`plotly.graph_objs as go`) and NeuroKit2 (`neurokit2 as nk`).\n",
    "        - Downsamples by a fixed factor for plotting only.\n",
    "        - Saving requires `BASE_DIR` to be defined globally.\n",
    "    \"\"\"\n",
    "\n",
    "    #get details of eda data\n",
    "    eda_signal = eda_data.magnitude.squeeze()\n",
    "\n",
    "    #apply transfer constant\n",
    "    eda_signal = eda_signal * transfer_constant\n",
    "\n",
    "    # Detrending removes linear drift (fits a linear regression line to the data and subtracts it from the signal)\n",
    "    detrended_signal = detrend(eda_signal)\n",
    "\n",
    "    #calculate SCRs\n",
    "    eda_measures = nk.eda_phasic(np.array(eda_signal), sampling_rate=1000)\n",
    "    eda_phasic = np.array(eda_measures['EDA_Phasic'])\n",
    "\n",
    "    # Convert Neo times to numerical array\n",
    "    times = eda_data.times.magnitude.squeeze()\n",
    "\n",
    "    # Downsample for plot\n",
    "    downsample_factor = 100 # Keep every Nth sample\n",
    "    eda_signal = eda_signal[::downsample_factor]\n",
    "    detrended_signal = detrended_signal[::downsample_factor]\n",
    "    eda_phasic = eda_phasic[::downsample_factor]\n",
    "    times = times[::downsample_factor]\n",
    "\n",
    "    # Add the time series data\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=times, y=eda_signal, mode='lines', name='Raw')) #plot eda trace\n",
    "    fig.add_trace(go.Scatter(x=times, y=detrended_signal, mode='lines', name='Detrended')) #plot detrended data\n",
    "    fig.add_trace(go.Scatter(x=times, y=eda_phasic, mode='lines', name='Phasic')) #plot detrended data\n",
    "\n",
    "    ## Add textmarks\n",
    "    textmarks_of_interest = ['apr', 'rat', 'sph', 'spl', 'sch', 'scl', 'sps', 'scs'] #apr = anticipation before name displayed; rat = subjects prompted to submit rating\n",
    "    df_textmarks_subset = df_textmarks[df_textmarks['text'].isin(textmarks_of_interest)]\n",
    "    for _, row in df_textmarks_subset.iterrows():\n",
    "        tm_sample_no = row['sampleNo']\n",
    "        tm_text = row['text']\n",
    "        \n",
    "        # Add vertical line\n",
    "        fig.add_vline(x=tm_sample_no/(SPIKE_FS), line_width=2, line_dash=\"dash\", line_color=\"grey\") #divide sample number by SPIKE_FS to convert to s\n",
    "        \n",
    "        # Add text annotation\n",
    "        fig.add_annotation(\n",
    "            x=tm_sample_no/(SPIKE_FS),\n",
    "            y=0,  # Adjust this value to position the text vertically\n",
    "            text=tm_text,\n",
    "            showarrow=False,\n",
    "            textangle=-90,\n",
    "            yanchor=\"bottom\",\n",
    "            font=dict(color=\"black\", size=20)\n",
    "        )\n",
    "\n",
    "    if p_or_c == 'p':\n",
    "        part_type = 'participant'\n",
    "    else:\n",
    "        part_type = 'companion'\n",
    "\n",
    "    fig.update_layout(\n",
    "        title = f\"{sessionID}: {part_type} EDA data\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    #save\n",
    "    if save_fig:\n",
    "        saveDestination = = BASE_DIR / \"Analysis\" / \"Data checks\" / \"\"\n",
    "        fig.update_layout( #update dimensions before saving\n",
    "        autosize=False,\n",
    "        width=1000,\n",
    "        height=250)\n",
    "\n",
    "        fig.write_image(f\"{saveDestination}{sessionID}_{part_type}_eda.png\")\n",
    "\n",
    "        print(\"saved figure as .png on OneDrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check EDA data (figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eda_data = False #define whether to run this section or skip it\n",
    "\n",
    "if plot_eda_data:\n",
    "    import warnings # Suppress the specific DeprecationWarning\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "    # Visual verification variables\n",
    "    eda_visual_verification = True\n",
    "    save_eda_figs = False\n",
    "    num_plotted = 0\n",
    "    max_plotted = 100\n",
    "\n",
    "    #------- Import excel file containing notes from ecg inspection\n",
    "    analysisFolder = BASE_DIR / \"Analysis\"\n",
    "    eda_inspection_file = os.path.join(analysisFolder, 'Data checks', 'empathy_eda_inspection.xlsx')\n",
    "    df_eda_inspection = pd.read_excel(eda_inspection_file)\n",
    "\n",
    "    for sessionID in session_folders:\n",
    "\n",
    "        # get session folder\n",
    "        sessionFolder = os.path.join(directory, sessionID)\n",
    "        print('Session: ', sessionID) \n",
    "        \n",
    "        # Import Spike2 data file\n",
    "        spike_file_extension = \"empathy.smrx\"\n",
    "        spike_search_pattern = os.path.join(sessionFolder, f'*{spike_file_extension}')\n",
    "        spike_files = glob.glob(spike_search_pattern)\n",
    "        if len(spike_files) > 1:\n",
    "            print(\"-\", \"{} SMRX files found.\".format(len(spike_files)))\n",
    "        elif len(spike_files) < 1:\n",
    "            print(RED + 'WARNING: no empathy spike data files found')\n",
    "\n",
    "        # Process exceptions\n",
    "        if sessionID == 'BG020':\n",
    "            #For some reason, BG020 spike file is not being imported correctly so have duplicated, which works fine\n",
    "            spike_files = BASE_DIR / \"Participant data\" / \"BG020\" / \"BG020_empathy_duplicate.smrx\"\n",
    "\n",
    "        # Loop over files\n",
    "        for i_spike_file in range(len(spike_files)):\n",
    "            \n",
    "            spike_filename = spike_files[i_spike_file]\n",
    "\n",
    "            print(\"-\", \"Importing Spike data for {}\".format(sessionID))\n",
    "\n",
    "            # Read Spike2 file\n",
    "            reader = CedIO(filename=spike_filename)\n",
    "            \n",
    "            block = reader.read_block()\n",
    "            spike2data = block.segments[0] # single segment\n",
    "\n",
    "            # Create a dictionary of analog channels and their indexes\n",
    "            analog_channel_dictionary = {}\n",
    "\n",
    "            for data_channel in range(len(spike2data.analogsignals)):\n",
    "                channel_idx = data_channel\n",
    "                channel_name = spike2data.analogsignals[data_channel].array_annotations['channel_names'][0]\n",
    "                #print(channel_name)\n",
    "                analog_channel_dictionary[channel_name] = channel_idx\n",
    "\n",
    "            # ---------- Get the EDA data for the subjects\n",
    "            p_eda_data = spike2data.analogsignals[analog_channel_dictionary['p_EDA']] #*IMPORTANT\n",
    "            c_eda_data = spike2data.analogsignals[analog_channel_dictionary['c_EDA']] #*IMPORTANT\n",
    "\n",
    "        \n",
    "        # ---------- Extract textmarks from smrx file using SonPy\n",
    "        smrfile = sonpy.lib.SonFile(spike_filename, True)\n",
    "\n",
    "        textmarkChanNo = 30\n",
    "        chanIdxInFile = textmarkChanNo - 1\n",
    "\n",
    "        max_n_tick = smrfile.ChannelMaxTime(chanIdxInFile)\n",
    "\n",
    "        n_chunks = 4\n",
    "        breakpoints = divide_into_n_breakpoints(max_n_tick, n_chunks)\n",
    "        df_textmarks_allchunks = pd.DataFrame()\n",
    "\n",
    "        for i_chunk in range(n_chunks):\n",
    "            first_sample = breakpoints[i_chunk]\n",
    "            last_sample = breakpoints[i_chunk + 1]\n",
    "\n",
    "            # Adjust the last chunk to ensure it includes the final tick\n",
    "            if i_chunk == n_chunks - 1:\n",
    "                last_sample += 1  # Add 1 to include the very last tick\n",
    "            \n",
    "            nSamplesToImport = last_sample - first_sample\n",
    "            textmarks = smrfile.ReadTextMarks(chan=chanIdxInFile, nMax=nSamplesToImport, tFrom=first_sample, tUpto=last_sample)\n",
    "\n",
    "            textmarks_sampleNo = np.full(len(textmarks), np.nan)\n",
    "            textmarks_text = np.array(['---'] * len(textmarks), dtype=str)\n",
    "\n",
    "            for i_mark in range(len(textmarks)):\n",
    "                mark = textmarks[i_mark]\n",
    "                textmarks_sampleNo[i_mark] = round(mark.Tick / 100)\n",
    "                textmarks_text[i_mark] = ''.join([str(mark[0]), str(mark[1]), str(mark[2])])\n",
    "            \n",
    "            df_textmarks = pd.DataFrame({'sampleNo': textmarks_sampleNo, 'text': textmarks_text})\n",
    "            df_textmarks_allchunks = pd.concat([df_textmarks_allchunks, df_textmarks], ignore_index=True)\n",
    "\n",
    "        #IMPORTANT - process textmark exceptions\n",
    "        if sessionID == 'BG002':\n",
    "            #start from row idx 19\n",
    "            df_textmarks_allchunks = df_textmarks_allchunks.iloc[19:]\n",
    "\n",
    "        elif sessionID == 'BG021':\n",
    "            #add rat textmark at last sample of spike file (when it crashed)\n",
    "            new_row_rating = pd.DataFrame({\"sampleNo\": [5509836], \"text\": ['rat']})  # Create a new DataFrame with the new row\n",
    "            df_textmarks_allchunks = pd.concat([df_textmarks_allchunks, new_row_rating], ignore_index=True)  # Append the row  \n",
    "\n",
    "        #IMPORTANT - calculate scaling factors based off input range    \n",
    "        dataRange = 10 #CED 2502 data is acquired between -5 and +5\n",
    "\n",
    "        #calculate participant scaling factor\n",
    "        p_id = sessionID + 'p'\n",
    "        row1_p_combined = df_all_combined.index[df_all_combined[\"subjectID\"].str.lower() == p_id.lower()][0]\n",
    "        p_eda_range = df_all_combined.loc[row1_p_combined, 'eda_range'] #get input range\n",
    "        if isinstance(p_eda_range, pd.Series):\n",
    "            # If 'p_eda_range' is still a series with multiple rows, retrieve the first element\n",
    "            p_eda_range = p_eda_range.iloc[0]  # Get the first value\n",
    "        \n",
    "        p_transferConstant = p_eda_range/dataRange #calculate participant transfer constant\n",
    "        print('- p transfer constant = ' + str(p_transferConstant))\n",
    "\n",
    "        #calculate companion scaling factor\n",
    "        c_id = sessionID + 'c'\n",
    "        row1_c_combined = df_all_combined.index[df_all_combined[\"subjectID\"].str.lower() == c_id.lower()][0]\n",
    "        c_eda_range = df_all_combined.loc[row1_c_combined, 'eda_range'] #get input range\n",
    "        if isinstance(c_eda_range, pd.Series):\n",
    "            # If 'c_eda_range' is still a series with multiple rows, retrieve the first element\n",
    "            c_eda_range = c_eda_range.iloc[0]  # Get the first value\n",
    "        c_transferConstant = c_eda_range/dataRange #calculate companion transfer constant\n",
    "        print('- c transfer constant = ' + str(c_transferConstant))\n",
    "\n",
    "        #IMPORTANT - plot eda data\n",
    "        plot_eda(sessionID, 'p', p_eda_data, df_textmarks_allchunks, p_transferConstant, save_eda_figs)\n",
    "        plot_eda(sessionID, 'c', c_eda_data, df_textmarks_allchunks, c_transferConstant, save_eda_figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate EDA measures\n",
    "\n",
    "https://neuropsychology.github.io/NeuroKit/functions/eda.html#eda-process\n",
    "nk.eda_process cleans data using neurokit defaults (Low-pass filter with a 3 Hz cutoff frequency and a 4th order Butterworth filter), \n",
    "then identifies SCRs using a high pass filter with a cutoff frequency of 0.05 Hz,\n",
    "calculates tonic component by subtracting SCRs from signal\n",
    "N.B. the way I am doing it below is better than eda_eventrelated because eda_eventrelated only counts the first SCR in the epoch.\n",
    "Using neurokit deafults, SCRs are excluded if their amplitude is less than 10% of the largest SCR amplitude in the timeseries (amplitude_min=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session:  BG001\n",
      "- Importing Spike data for BG001\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG001p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG001c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG002\n",
      "- Importing Spike data for BG002\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG002p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (40) matches no. rows (40) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG002c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (40) matches no. rows (40) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG003\n",
      "- Importing Spike data for BG003\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG003p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG003c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG004\n",
      "- Importing Spike data for BG004\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG004p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG004c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG005\n",
      "- Importing Spike data for BG005\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG005p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG005c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG006\n",
      "- Importing Spike data for BG006\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG006p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG006c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG007\n",
      "- Importing Spike data for BG007\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG007p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG007c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "\u001b[94m- BG007c set SCL/SCR responses for all trials nan due to unusable EDA data\u001b[0m\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG008\n",
      "- Importing Spike data for BG008\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG008p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG008c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG009\n",
      "- Importing Spike data for BG009\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG009p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG009c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG010\n",
      "- Importing Spike data for BG010\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG010p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG010c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG011\n",
      "- Importing Spike data for BG011\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG011p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG011c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG012\n",
      "- Importing Spike data for BG012\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG012p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG012c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG013\n",
      "- Importing Spike data for BG013\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG013p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG013c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG014\n",
      "- Importing Spike data for BG014\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG014p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG014c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG015\n",
      "- Importing Spike data for BG015\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG015p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG015c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG016\n",
      "- Importing Spike data for BG016\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG016p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG016c Processing EDA data\n",
      "\u001b[94m- Processing noisy sections\u001b[0m\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG017\n",
      "- Importing Spike data for BG017\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG017p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG017c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG018\n",
      "- Importing Spike data for BG018\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG018p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG018c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG019\n",
      "- Importing Spike data for BG019\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG019p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG019c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG020\n",
      "- Importing Spike data for BG020\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG020p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG020c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG021\n",
      "- Importing Spike data for BG021\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG021p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (79) matches no. rows (79) data for this subject in df_all_combined\n",
      "\u001b[94mSet SCL/SCR responses for trial 79 resposne phase to nan\u001b[0m\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG021c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (79) matches no. rows (79) data for this subject in df_all_combined\n",
      "\u001b[94mSet SCL/SCR responses for trial 79 resposne phase to nan\u001b[0m\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG022\n",
      "- Importing Spike data for BG022\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG022p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG022c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG023\n",
      "- Importing Spike data for BG023\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG023p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG023c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG024\n",
      "- Importing Spike data for BG024\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG024p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG024c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG025\n",
      "- Importing Spike data for BG025\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG025p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG025c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG026\n",
      "- Importing Spike data for BG026\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG026p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG026c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG027\n",
      "- Importing Spike data for BG027\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG027p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG027c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG028\n",
      "- Importing Spike data for BG028\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG028p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG028c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG029\n",
      "- Importing Spike data for BG029\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG029p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG029c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG030\n",
      "- Importing Spike data for BG030\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG030p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG030c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG031\n",
      "- Importing Spike data for BG031\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG031p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG031c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG032\n",
      "- Importing Spike data for BG032\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG032p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG032c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG033\n",
      "- Importing Spike data for BG033\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG033p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG033c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG034\n",
      "- Importing Spike data for BG034\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG034p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG034c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG035\n",
      "- Importing Spike data for BG035\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG035p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG035c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG036\n",
      "- Importing Spike data for BG036\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG036p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG036c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG037\n",
      "- Importing Spike data for BG037\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG037p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG037c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG038\n",
      "- Importing Spike data for BG038\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG038p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG038c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG039\n",
      "- Importing Spike data for BG039\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG039p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG039c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG040\n",
      "- Importing Spike data for BG040\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG040p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG040c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG041\n",
      "- Importing Spike data for BG041\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG041p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG041c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG042\n",
      "- Importing Spike data for BG042\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG042p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG042c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG043\n",
      "- Importing Spike data for BG043\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG043p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG043c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG044\n",
      "- Importing Spike data for BG044\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG044p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG044c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG045\n",
      "- Importing Spike data for BG045\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG045p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG045c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG046\n",
      "- Importing Spike data for BG046\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG046p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG046c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG047\n",
      "- Importing Spike data for BG047\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG047p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG047c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG048\n",
      "- Importing Spike data for BG048\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG048p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG048c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG049\n",
      "- Importing Spike data for BG049\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG049p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG049c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "Session:  BG050\n",
      "- Importing Spike data for BG050\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG050p Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n",
      "BG050c Processing EDA data\n",
      "- Calculated noisy samples\n",
      "- Applied transfer constant 2.5\n",
      "- Calculated unstandardised tonic + phasic components for entire timeseries\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated SCL/SCR resposnes\n"
     ]
    }
   ],
   "source": [
    "run_eda_analyses = True #define whether to run this section or skip it\n",
    "standardise_eda_range_within_subjs = False #define whether to standardise (z score) EDA values within subjects\n",
    "\n",
    "if run_eda_analyses:\n",
    "    import warnings # Suppress the specific DeprecationWarning\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "    #------- Import excel file containing notes from ecg inspection\n",
    "    analysisFolder = BASE_DIR / \"Analysis\"\n",
    "    eda_inspection_file = os.path.join(analysisFolder, 'Data checks', 'empathy_eda_inspection.xlsx')\n",
    "    df_eda_inspection = pd.read_excel(eda_inspection_file)\n",
    "\n",
    "    for sessionID in session_folders:\n",
    "\n",
    "        # get session folder\n",
    "        sessionFolder = os.path.join(directory, sessionID)\n",
    "        print('Session: ', sessionID) \n",
    "        \n",
    "        # Import Spike2 data file\n",
    "        spike_file_extension = \"empathy.smrx\"\n",
    "        spike_search_pattern = os.path.join(sessionFolder, f'*{spike_file_extension}')\n",
    "        spike_files = glob.glob(spike_search_pattern)\n",
    "        if len(spike_files) > 1:\n",
    "            print(\"-\", \"{} SMRX files found.\".format(len(spike_files)))\n",
    "        elif len(spike_files) < 1:\n",
    "            print(RED + 'WARNING: no empathy spike data files found')\n",
    "\n",
    "        # Process exceptions\n",
    "        if sessionID == 'BG020':\n",
    "            #For some reason, BG020 spike file is not being imported correctly so have duplicated, which works fine\n",
    "            spike_files = BASE_DIR / \"Participant data\" / \"BG020\" / \"BG020_empathy_duplicate.smrx\"\n",
    "\n",
    "        # Loop over files\n",
    "        for i_spike_file in range(len(spike_files)):\n",
    "            \n",
    "            spike_filename = spike_files[i_spike_file]\n",
    "\n",
    "            print(\"-\", \"Importing Spike data for {}\".format(sessionID))\n",
    "\n",
    "            # Read Spike2 file\n",
    "            reader = CedIO(filename=spike_filename)\n",
    "            \n",
    "            block = reader.read_block()\n",
    "            spike2data = block.segments[0] # single segment\n",
    "\n",
    "            # Create a dictionary of analog channels and their indexes\n",
    "            analog_channel_dictionary = {}\n",
    "\n",
    "            for data_channel in range(len(spike2data.analogsignals)):\n",
    "                channel_idx = data_channel\n",
    "                channel_name = spike2data.analogsignals[data_channel].array_annotations['channel_names'][0]\n",
    "                #print(channel_name)\n",
    "                analog_channel_dictionary[channel_name] = channel_idx\n",
    "\n",
    "            # ---------- Get the EDA data for the subjects\n",
    "            p_eda_data = spike2data.analogsignals[analog_channel_dictionary['p_EDA']] #*IMPORTANT\n",
    "            c_eda_data = spike2data.analogsignals[analog_channel_dictionary['c_EDA']] #*IMPORTANT\n",
    "\n",
    "        \n",
    "        # ---------- Extract textmarks from smrx file using SonPy\n",
    "        smrfile = sonpy.lib.SonFile(spike_filename, True)\n",
    "\n",
    "        textmarkChanNo = 30\n",
    "        chanIdxInFile = textmarkChanNo - 1\n",
    "\n",
    "        max_n_tick = smrfile.ChannelMaxTime(chanIdxInFile)\n",
    "\n",
    "        n_chunks = 4\n",
    "        breakpoints = divide_into_n_breakpoints(max_n_tick, n_chunks)\n",
    "        df_textmarks_allchunks = pd.DataFrame()\n",
    "\n",
    "        for i_chunk in range(n_chunks):\n",
    "            first_sample = breakpoints[i_chunk]\n",
    "            last_sample = breakpoints[i_chunk + 1]\n",
    "\n",
    "            # Adjust the last chunk to ensure it includes the final tick\n",
    "            if i_chunk == n_chunks - 1:\n",
    "                last_sample += 1  # Add 1 to include the very last tick\n",
    "            \n",
    "            nSamplesToImport = last_sample - first_sample\n",
    "            textmarks = smrfile.ReadTextMarks(chan=chanIdxInFile, nMax=nSamplesToImport, tFrom=first_sample, tUpto=last_sample)\n",
    "\n",
    "            textmarks_sampleNo = np.full(len(textmarks), np.nan)\n",
    "            textmarks_text = np.array(['---'] * len(textmarks), dtype=str)\n",
    "\n",
    "            for i_mark in range(len(textmarks)):\n",
    "                mark = textmarks[i_mark]\n",
    "                textmarks_sampleNo[i_mark] = round(mark.Tick / 100)\n",
    "                textmarks_text[i_mark] = ''.join([str(mark[0]), str(mark[1]), str(mark[2])])\n",
    "            \n",
    "            df_textmarks = pd.DataFrame({'sampleNo': textmarks_sampleNo, 'text': textmarks_text})\n",
    "            df_textmarks_allchunks = pd.concat([df_textmarks_allchunks, df_textmarks], ignore_index=True)\n",
    "\n",
    "        #IMPORTANT - process textmark exceptions\n",
    "        if sessionID == 'BG002':\n",
    "            #start from row idx 19\n",
    "            df_textmarks_allchunks = df_textmarks_allchunks.iloc[19:]\n",
    "\n",
    "        elif sessionID == 'BG021':\n",
    "            #add rat textmark at last sample of spike file (when it crashed)\n",
    "            new_row_rating = pd.DataFrame({\"sampleNo\": [5509836], \"text\": ['rat']})  # Create a new DataFrame with the new row\n",
    "            df_textmarks_allchunks = pd.concat([df_textmarks_allchunks, new_row_rating], ignore_index=True)  # Append the row  \n",
    "\n",
    "        #IMPORTANT - calculate scaling factors based off input range    \n",
    "        dataRange = 10 #CED 2502 data is acquired between -5 and +5\n",
    "\n",
    "        #calculate participant scaling factor\n",
    "        p_id = sessionID + 'p'\n",
    "        row1_p_combined = df_all_combined.index[df_all_combined[\"subjectID\"].str.lower() == p_id.lower()][0]\n",
    "        p_eda_range = df_all_combined.loc[row1_p_combined, 'eda_range'] #get input range\n",
    "        if isinstance(p_eda_range, pd.Series):\n",
    "            # If 'p_eda_range' is still a series with multiple rows, retrieve the first element\n",
    "            p_eda_range = p_eda_range.iloc[0]  # Get the first value\n",
    "        p_transferConstant = p_eda_range/dataRange #calculate participant transfer constant\n",
    "        print('- p transfer constant = ' + str(p_transferConstant))\n",
    "\n",
    "        #calculate companion scaling factor\n",
    "        c_id = sessionID + 'c'\n",
    "        row1_c_combined = df_all_combined.index[df_all_combined[\"subjectID\"].str.lower() == c_id.lower()][0]\n",
    "        c_eda_range = df_all_combined.loc[row1_c_combined, 'eda_range'] #get input range\n",
    "        if isinstance(c_eda_range, pd.Series):\n",
    "            # If 'c_eda_range' is still a series with multiple rows, retrieve the first element\n",
    "            c_eda_range = c_eda_range.iloc[0]  # Get the first value\n",
    "        c_transferConstant = c_eda_range/dataRange #calculate companion transfer constant\n",
    "        print('- c transfer constant = ' + str(c_transferConstant))\n",
    "    \n",
    "        #----- Downsample data\n",
    "        downsample_factor = 1 #factor to downsample by (N.B. factor not Hz)\n",
    "\n",
    "        #--- Downsample textmarks for calculating EDA responses\n",
    "        df_textmarks_allchunks['sampleNo'] = (df_textmarks_allchunks['sampleNo'] / downsample_factor).round(0).astype(int) #downsample textmarks\n",
    "        \n",
    "        #generate IDs of both subjects in session\n",
    "        subjsInSession = [\"p\", \"c\"]\n",
    "\n",
    "        for subj in subjsInSession:\n",
    "            \n",
    "            subjID = sessionID + subj\n",
    "\n",
    "            if subj == \"p\": #IMPORTANT - defines array for calculating EDA responses\n",
    "                subj_eda_data = p_eda_data\n",
    "                subj_transfer_constant = p_transferConstant\n",
    "            elif subj == \"c\":\n",
    "                subj_eda_data = c_eda_data\n",
    "                subj_transfer_constant = c_transferConstant\n",
    "            else:\n",
    "                print(RED + 'ERROR - issue defining subj_eda_data - subj not recognised')\n",
    "\n",
    "            #Process exceptions\n",
    "            if subjID == 'BG006p':\n",
    "                #BG006 p and c psychophys channels are wrong way round for empathy task. Swap over!\n",
    "                subj_eda_data = c_eda_data\n",
    "            elif subjID == 'BG006c':\n",
    "                 subj_eda_data = p_eda_data\n",
    "\n",
    "            #---- Calculate mean SCL, SCR count, sum SCR amplitudes for each phase\n",
    "            print(subjID + ' Processing EDA data')\n",
    "\n",
    "            #---- create array specifying noisy sections of EDA data\n",
    "            row_subj_recording = (df_eda_inspection['Subject'] == subjID)# Find rows in ecg inspection excel file for this subject\n",
    "            eda_idx_noisy_section = df_eda_inspection.loc[row_subj_recording, 'EDA_bad_segments'] # Get sample numbers of entire sections of peaks to delete\n",
    "\n",
    "            if isinstance(eda_idx_noisy_section, str): # Ensure it's a list or iterable of strings, in case it's a single value\n",
    "                eda_idx_noisy_section = [eda_idx_noisy_section]\n",
    "\n",
    "            eda_idx_noisy_samples = [] # Initialise an empty list to collect all indices to delete\n",
    "\n",
    "            for cell_value in eda_idx_noisy_section: # Process each cell value\n",
    "                if cell_value.lower() == 'none':  # Skip cells with 'none'\n",
    "                    continue\n",
    "                for section in cell_value.split(','):  # Split multiple ranges by commas\n",
    "                    print(BLUE + '- Processing noisy sections' + RESET)\n",
    "                    start, end = map(int, section.split(':'))  # Split each range and convert to integers\n",
    "                    eda_idx_noisy_samples.extend(range(start, end + 1))  # Include the endpoint\n",
    "            \n",
    "            eda_noisy_samples = np.full(len(subj_eda_data), False, dtype=bool) #define logical array\n",
    "            if len(eda_idx_noisy_samples) > 0: #downsample any sample numbers\n",
    "                eda_idx_noisy_samples = [round(int(idx)/downsample_factor) for idx in eda_idx_noisy_samples]\n",
    "                eda_noisy_samples[eda_idx_noisy_samples] = True #set noisy trials to true\n",
    "            \n",
    "            print('- Calculated noisy samples')\n",
    "\n",
    "            #----- Transform EDA signal (convert to microsiemens, filter low freq. drift ) -updated 15.04.25\n",
    "            subj_eda_signal = subj_eda_data.magnitude.squeeze() #Get eda data\n",
    "            subj_eda_signal = subj_eda_signal + 5 #add 5 to make all values positive\n",
    "            subj_eda_signal = subj_eda_signal * subj_transfer_constant #apply transfer constant to convert to microsiemens\n",
    "            print('- Applied transfer constant ' + str(round(subj_transfer_constant, 2)))\n",
    "            \n",
    "            \n",
    "            if standardise_eda_range_within_subjs:\n",
    "                subj_eda_signals_nk, subj_scr_info_nk = nk.eda_process(nk.standardize(subj_eda_signal), sampling_rate = SPIKE_FS) #standardised (z scored within subjects)\n",
    "                print(BLUE + '- Calculated STANDARDISED tonic + phasic components for entire timeseries' + RESET)\n",
    "            else:\n",
    "                subj_eda_signals_nk, subj_scr_info_nk = nk.eda_process(subj_eda_signal, sampling_rate = SPIKE_FS) #unstandardised\n",
    "                print('- Calculated unstandardised tonic + phasic components for entire timeseries')\n",
    "\n",
    "            #-------- Identify textmarks for each trial\n",
    "            #check that number of anticipation and shock textmarks matches number of trials\n",
    "            nTrialsInTextmarks = len(df_textmarks_allchunks[df_textmarks_allchunks['text'].isin(['apr'])]) #get number of trials in textmarks\n",
    "            subj_rows_combined = df_all_combined['subjectID'].str.contains(subjID, na=False, case=False)  #get subject rows in df_all_combined\n",
    "\n",
    "            if nTrialsInTextmarks != sum(subj_rows_combined):\n",
    "                print(RED + 'ERROR: there are ' + str(nTrialsInTextmarks) + \n",
    "                    ' apr textmarks but ' + str(sum(subj_rows_combined)) + \n",
    "                    ' apr textmarks but ' + str(sum(subj_rows_combined)) + \n",
    "                    ' rows of data for this subject in df_all_combined' + RESET)\n",
    "            else:\n",
    "                print('- No. of apr textmarks (' + str(nTrialsInTextmarks) + \n",
    "                    ') matches no. rows (' + str(sum(subj_rows_combined)) + \n",
    "                    ') data for this subject in df_all_combined')\n",
    "\n",
    "\n",
    "            # Iterate through each trial and calculate SCL/SCR measures for each phase\n",
    "            #> Filter rows for the relevant textmarks in df_textmarks_all_chunks\n",
    "            relevant_textmarks = [\"apr\", #anticipation pre-name\n",
    "                                  \"apo\", #anticipation post-name\n",
    "                                  \"sph\", \"spl\", \"sps\", \"sch\", \"scl\", \"scs\", #shock delivery\n",
    "                                  \"res\", #start of response period\n",
    "                                  \"rat\"] #submit rating/experimenter change shock settings\n",
    "            filtered_textmarks = df_textmarks_allchunks[df_textmarks_allchunks[\"text\"].isin(relevant_textmarks)]\n",
    "\n",
    "            # Find all indices of the \"apr\" textmark (one for each trial)\n",
    "            apr_indices = filtered_textmarks[filtered_textmarks[\"text\"] == \"apr\"].index\n",
    "\n",
    "            # Iterate through each trial using \"apr\" textmarks as the starting point\n",
    "            for trial_idx, apr_idx in enumerate(apr_indices):\n",
    "                \n",
    "                #Calculate trial number (start at 1 not 0)\n",
    "                trial_no = trial_idx + 1\n",
    "                \n",
    "                #calculate subject row as logical array\n",
    "                subject_row = (df_all_combined['subjectID'].str.contains(subjID, na=False, case=False)) & (df_all_combined['trialNo'] == trial_no)\n",
    "\n",
    "                # Get the indices of the textmarks for this trial\n",
    "                sample_no_apr = int(filtered_textmarks.loc[apr_idx, \"sampleNo\"])\n",
    "\n",
    "                # Get the next \"apo\" textmark (signalling start of anticipation post name period) after \"apr\"\n",
    "                sample_no_apo = filtered_textmarks.loc[\n",
    "                    (filtered_textmarks.index > apr_idx) & (filtered_textmarks[\"text\"] == \"apo\"), \"sampleNo\"\n",
    "                ].iloc[0]\n",
    "\n",
    "                # Get the first shock (sph, spl, sps, sch, scl, scs) textmark after \"apo\"\n",
    "                sample_no_shock = filtered_textmarks.loc[\n",
    "                    (filtered_textmarks.index > apr_idx)\n",
    "                    & (filtered_textmarks[\"text\"].isin([\"sph\", \"spl\", \"sps\", \"sch\", \"scl\", \"scs\"])),\n",
    "                    \"sampleNo\",\n",
    "                ].iloc[0]\n",
    "\n",
    "                #Get the first response period (res) textmark after \"apo\"\n",
    "                sample_no_res = filtered_textmarks.loc[\n",
    "                    (filtered_textmarks.index > apr_idx) & (filtered_textmarks[\"text\"] == \"res\"), \"sampleNo\"\n",
    "                ].iloc[0]\n",
    "\n",
    "                # Get the next \"rat\" textmark (signalling rating) after anticipation post name\n",
    "                sample_no_rat = filtered_textmarks.loc[\n",
    "                    (filtered_textmarks.index > apr_idx)\n",
    "                    & (filtered_textmarks[\"text\"] == \"rat\"),\n",
    "                    \"sampleNo\",\n",
    "                ].iloc[0]\n",
    "\n",
    "                #Convert indices to integers\n",
    "                sample_no_apr = int(sample_no_apr)\n",
    "                sample_no_apo = int(sample_no_apo)\n",
    "                sample_no_shock = int(sample_no_shock)\n",
    "                sample_no_res = int(sample_no_res)\n",
    "                sample_no_rat = int(sample_no_rat)\n",
    "\n",
    "                #Check anticipation phase textmarks\n",
    "                if (sample_no_shock - sample_no_apo > 10100) | (sample_no_shock - sample_no_apo < 5900):\n",
    "                    #print(RED + '- Trial ' +  str(trial_no) + ', Anticipation Phase is ' + str(sample_no_shock - sample_no_apo) + ' ms' + RESET)\n",
    "                    \n",
    "                    #If antiicpation phase is incorrect length, work out shock time as 500ms before start of response phase\n",
    "                    sample_no_shock = sample_no_res - DURATION_SHOCK\n",
    "                    #print(BLUE + '- Trial ' +  str(trial_no) + ', New Anticipation Phase duration: ' + str(sample_no_shock - sample_no_apo) + ' ms' + RESET)\n",
    "                    \n",
    "                    #if this does not fix it then print error\n",
    "                    if (sample_no_shock - sample_no_apo > 10100) | (sample_no_shock - sample_no_apo < 5900):\n",
    "                        print(RED + '- ERROR: Trial ' +  str(trial_no) + ', Anticipation Phase is ' + str(sample_no_shock - sample_no_apo) + ' ms even after using \"res\" textmark.' + RESET)\n",
    "\n",
    "                #-------- Calculate SCL and SCR measures for each phase\n",
    "\n",
    "                #Waiting\n",
    "                if any(eda_noisy_samples[sample_no_apr : sample_no_apr + DURATION_WAITING]): #if eda data is noisy then set values to NaN\n",
    "                        mean_scl_waiting = np.nan # Average SCL\n",
    "                        n_scr_waiting = np.nan  # SCR count\n",
    "                        scr_amplitude_sum_waiting = np.nan #sum SCR amplitudes\n",
    "                else:\n",
    "                    subj_signals_waiting = subj_eda_signals_nk.iloc[sample_no_apr : sample_no_apr + DURATION_WAITING]\n",
    "                    mean_scl_waiting = subj_signals_waiting[\"EDA_Tonic\"].mean() # Average SCL\n",
    "                    n_scr_waiting = subj_signals_waiting[\"SCR_Peaks\"].sum()  # SCR count\n",
    "                    scr_amplitude_sum_waiting = subj_signals_waiting.loc[subj_signals_waiting[\"SCR_Peaks\"] == 1, \"SCR_Amplitude\"].sum() #sum SCR amplitudes\n",
    "\n",
    "                #Anticipation\n",
    "                if any(eda_noisy_samples[sample_no_apo : sample_no_shock]): #if eda data is noisy then set values to NaN\n",
    "                        mean_scl_anticipation = np.nan # Average SCL\n",
    "                        n_scr_anticipation = np.nan  # SCR count\n",
    "                        scr_amplitude_sum_anticipation = np.nan #sum SCR amplitudes\n",
    "                else:\n",
    "                    subj_signals_anticipation = subj_eda_signals_nk.iloc[sample_no_apo : sample_no_shock]\n",
    "                    mean_scl_anticipation = subj_signals_anticipation[\"EDA_Tonic\"].mean() # Average SCL\n",
    "                    n_scr_anticipation = subj_signals_anticipation[\"SCR_Peaks\"].sum()  # SCR count\n",
    "                    scr_amplitude_sum_anticipation = subj_signals_anticipation.loc[subj_signals_anticipation[\"SCR_Peaks\"] == 1, \"SCR_Amplitude\"].sum() #sum SCR amplitudes\n",
    "\n",
    "                #Response\n",
    "                if any(eda_noisy_samples[sample_no_shock + DURATION_SHOCK : sample_no_shock + DURATION_SHOCK + DURATION_RESPONSE_EXCL_SHOCK]): #if eda data is noisy then set values to NaN\n",
    "                        mean_scl_response = np.nan # Average SCL\n",
    "                        n_scr_response = np.nan  # SCR count\n",
    "                        scr_amplitude_sum_response = np.nan #sum SCR amplitudes\n",
    "                else:\n",
    "                    subj_signals_response = subj_eda_signals_nk.iloc[sample_no_shock + DURATION_SHOCK : sample_no_shock + DURATION_SHOCK + DURATION_RESPONSE_EXCL_SHOCK]\n",
    "                    #do not include shock in mean\n",
    "                    mean_scl_response = subj_signals_response[\"EDA_Tonic\"].mean() # Average SCL\n",
    "                    n_scr_response = subj_signals_response[\"SCR_Peaks\"].sum()  # SCR count\n",
    "                    scr_amplitude_sum_response = subj_signals_response.loc[subj_signals_response[\"SCR_Peaks\"] == 1, \"SCR_Amplitude\"].sum() #sum SCR amplitudes\n",
    "                    \n",
    "                #------------ Calculate SCL timeseries for each phase\n",
    "                target_fs_ts = 5\n",
    "                \n",
    "                #Waiting\n",
    "                if any(eda_noisy_samples[sample_no_apr - DURATION_PRE_PHASE : sample_no_apr + DURATION_WAITING]): #if eda data is noisy then set values to NaN\n",
    "                    downsampled_scl_waiting_ts = np.array([np.nan])\n",
    "                else:\n",
    "                    subj_signals_waiting_ts = subj_eda_signals_nk.iloc[sample_no_apr - DURATION_PRE_PHASE : sample_no_apr + DURATION_WAITING]\n",
    "                    scl_waiting_ts = subj_signals_waiting_ts[\"EDA_Tonic\"].to_numpy(dtype=float) # SCL\n",
    "                    downsampled_scl_waiting_ts = np.round(downsample_with_last(scl_waiting_ts, SPIKE_FS, target_fs = target_fs_ts), 2) #downsample\n",
    "\n",
    "                #Anticipation\n",
    "                if any(eda_noisy_samples[sample_no_apo - DURATION_PRE_PHASE : sample_no_shock]): #if eda data is noisy then set values to NaN\n",
    "                    downsampled_scl_anticipation_ts = np.array([np.nan]) \n",
    "                else:\n",
    "                    subj_signals_anticipation_ts = subj_eda_signals_nk.iloc[sample_no_apo - DURATION_PRE_PHASE : sample_no_shock]\n",
    "                    scl_anticipation_ts = subj_signals_anticipation_ts[\"EDA_Tonic\"].to_numpy(dtype=float) # SCL\n",
    "                    downsampled_scl_anticipation_ts = np.round(downsample_with_last(scl_anticipation_ts, SPIKE_FS, target_fs = target_fs_ts), 2) #downsample\n",
    "\n",
    "                #Response\n",
    "                if any(eda_noisy_samples[sample_no_shock - DURATION_PRE_PHASE : sample_no_shock + DURATION_SHOCK + DURATION_RESPONSE_EXCL_SHOCK]): #if eda data is noisy then set values to NaN\n",
    "                    downsampled_scl_response_ts = np.array([np.nan]) \n",
    "                else:\n",
    "                    #include shock in timeseries\n",
    "                    subj_signals_response_ts = subj_eda_signals_nk.iloc[sample_no_shock - DURATION_PRE_PHASE : sample_no_shock + DURATION_SHOCK + DURATION_RESPONSE_EXCL_SHOCK]\n",
    "                    scl_response_ts = subj_signals_response_ts[\"EDA_Tonic\"].to_numpy(dtype=float) # SCL\n",
    "                    downsampled_scl_response_ts = np.round(downsample_with_last(scl_response_ts, SPIKE_FS, target_fs = target_fs_ts), 2) #downsample\n",
    "\n",
    "                #if subject_row is valid then add SCL and SCR values to dataframe\n",
    "                if sum(subject_row) == 1:\n",
    "                    # Add mean scl to dataframe\n",
    "                    df_all_combined.loc[subject_row, \"mean_scl_waiting\"] = round(mean_scl_waiting, 4)\n",
    "                    df_all_combined.loc[subject_row, \"mean_scl_anticipation\"] = round(mean_scl_anticipation, 4)\n",
    "                    df_all_combined.loc[subject_row, \"mean_scl_response\"] = round(mean_scl_response, 4)\n",
    "\n",
    "                    # Add n SCRs to dataframe\n",
    "                    df_all_combined.loc[subject_row, \"n_scr_waiting\"] = round(n_scr_waiting, 4)\n",
    "                    df_all_combined.loc[subject_row, \"n_scr_anticipation\"] = round(n_scr_anticipation, 4)\n",
    "                    df_all_combined.loc[subject_row, \"n_scr_response\"] = round(n_scr_response, 4)\n",
    "\n",
    "                    # Add sum SCR amplitudes to dataframe\n",
    "                    df_all_combined.loc[subject_row, \"scr_amplitude_sum_waiting\"] = round(scr_amplitude_sum_waiting, 4)\n",
    "                    df_all_combined.loc[subject_row, \"scr_amplitude_sum_anticipation\"] = round(scr_amplitude_sum_anticipation, 4)\n",
    "                    df_all_combined.loc[subject_row, \"scr_amplitude_sum_response\"] = round(scr_amplitude_sum_response, 4)\n",
    "\n",
    "                    # Add SCL timeseries to the dataframe\n",
    "                    #>Assign as a string representation of the list\n",
    "                    df_all_combined.loc[subject_row, 'scl_series_waiting'] = str(downsampled_scl_waiting_ts.tolist())\n",
    "                    df_all_combined.loc[subject_row, 'scl_series_anticipation'] = str(downsampled_scl_anticipation_ts.tolist())\n",
    "                    df_all_combined.loc[subject_row, 'scl_series_response'] = str(downsampled_scl_response_ts.tolist())\n",
    "\n",
    "                    #Process exceptions\n",
    "\n",
    "                    #> BG021 trial 79 crashed during response phase so set responses to nan\n",
    "                    if (sessionID == 'BG021') & (trial_no == 79):\n",
    "                        df_all_combined.loc[subject_row, \"mean_scl_response\"] = np.nan\n",
    "                        df_all_combined.loc[subject_row, \"n_scr_response\"] = np.nan\n",
    "                        df_all_combined.loc[subject_row, \"scr_amplitude_sum_response\"] = np.nan\n",
    "                        df_all_combined.loc[subject_row, 'scl_series_response'] = str( np.array([np.nan]) )\n",
    "                        print(BLUE + 'Set SCL/SCR responses for trial 79 resposne phase to nan' + RESET)\n",
    "\n",
    "                elif sum(subject_row) > 1 :\n",
    "                    print(RED + f\"WARNING: multiple subject_rows {subject_row} is in df_all_combined.\" + RESET)\n",
    "                elif sum(subject_row) == 0 :\n",
    "                    print(RED + f\"WARNING: subject_row {subject_row} is not in df_all_combined.\" + RESET)\n",
    "            \n",
    "            #Process exceptions\n",
    "            #> BG007c EDA data is unusable so do not include any EDA data for any trials\n",
    "            if subjID == 'BG007c':\n",
    "                df_all_combined.loc[subj_rows_combined, [\"mean_scl_waiting\", \"mean_scl_anticipation\", \"mean_scl_response\"]] = np.nan\n",
    "                df_all_combined.loc[subj_rows_combined, [\"n_scr_waiting\", \"n_scr_anticipation\", \"n_scr_response\"]] = np.nan\n",
    "                df_all_combined.loc[subj_rows_combined, [\"scr_amplitude_sum_waiting\", \"scr_amplitude_sum_anticipation\", \"scr_amplitude_sum_response\"]] = np.nan\n",
    "                df_all_combined.loc[subj_rows_combined, ['scl_series_waiting', 'scl_series_anticipation', 'scl_series_response']] = str( np.array([np.nan]) )\n",
    "                print(BLUE + '- BG007c set SCL/SCR responses for all trials nan due to unusable EDA data' + RESET)\n",
    "\n",
    "            #Print message\n",
    "            print('- Successfully calculated SCL/SCR resposnes')                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate B2B responses and timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session:  BG001\n",
      "- Importing Spike data for BG001\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "\u001b[94m- BG007p set b2b responses for all trials nan due to unusable b2b data\u001b[0m\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG002\n",
      "- Importing Spike data for BG002\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3297200  end= 3299237\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3508922  end= 3509872\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (40) matches no. rows (40) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG003\n",
      "- Importing Spike data for BG003\n",
      "\u001b[94m- Added section start=4132308 end= 5460922to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4720855 to 4726285: 9 peaks (duration: 5430 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5102354 to 5109813: 13 peaks (duration: 7459 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5110338 to 5117773: 12 peaks (duration: 7435 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5118318 to 5126048: 12 peaks (duration: 7730 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5126588 to 5133658: 11 peaks (duration: 7070 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5134203 to 5147497: 21 peaks (duration: 13294 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5919575 to 5926520: 11 peaks (duration: 6945 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4088372  end= 4090488\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4798595  end= 4799184\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG004\n",
      "- Importing Spike data for BG004\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3923248 to 3973672: 13 peaks (duration: 50424 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3974332 to 3980502: 7 peaks (duration: 6170 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3984411  end= 3984771\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG005\n",
      "- Importing Spike data for BG005\n",
      "\u001b[94m- Added section start=2622760 end= 2643068to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=275508 end= 2771920to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=3634537 end= 3664354to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=4132308 end= 5460922to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=5716314 end= 5729861to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2493855 to 2496030: 4 peaks (duration: 2175 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2630728 to 2634697: 10 peaks (duration: 3969 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2805710 to 2819915: 20 peaks (duration: 14205 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3247008 to 3249163: 3 peaks (duration: 2155 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3251043 to 3257578: 9 peaks (duration: 6535 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3258888 to 3266203: 10 peaks (duration: 7315 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3267658 to 3281657: 21 peaks (duration: 13999 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5465452 to 5472868: 12 peaks (duration: 7416 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5475303 to 5481637: 9 peaks (duration: 6334 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5483087 to 5490588: 10 peaks (duration: 7501 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5491953 to 5499207: 11 peaks (duration: 7254 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5500487 to 5515108: 20 peaks (duration: 14621 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5471921  end= 5476628\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5624696  end= 5625576\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  6168243  end= 6168807\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG006\n",
      "- Importing Spike data for BG006\n",
      "\u001b[94m- Added section start=4239713 end= 4251541to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2482304 to 2492914: 18 peaks (duration: 10610 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2493739 to 2506409: 22 peaks (duration: 12670 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1644707  end= 1646124\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2996858  end= 2997368\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3959140  end= 3961247\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4456956  end= 4457590\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4459379  end= 4460071\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4468320  end= 4469093\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4471599  end= 4472159\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4486789  end= 4487408\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4661835  end= 4663011\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4877141  end= 4877724\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG007\n",
      "- Importing Spike data for BG007\n",
      "\u001b[94m- Added section start=3428235 end= 4467059to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=5274304 end= 5510144to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4469636 to 4479745: 11 peaks (duration: 10109 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4480475 to 4490501: 11 peaks (duration: 10026 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4491395 to 4501210: 12 peaks (duration: 9815 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4501980 to 4511144: 10 peaks (duration: 9164 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4511779 to 4530064: 21 peaks (duration: 18285 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5508611 to 5518220: 11 peaks (duration: 9609 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5518880 to 5527424: 10 peaks (duration: 8544 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5528319 to 5538355: 10 peaks (duration: 10036 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5539225 to 5549089: 11 peaks (duration: 9864 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5550014 to 5567814: 20 peaks (duration: 17800 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5680183 to 5699713: 21 peaks (duration: 19530 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5700528 to 5711032: 11 peaks (duration: 10504 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5711957 to 5722578: 11 peaks (duration: 10621 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5723472 to 5742586: 21 peaks (duration: 19114 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5907664 to 5927329: 21 peaks (duration: 19665 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4534738  end= 4535549\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4944502  end= 4945167\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5111312  end= 5112154\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5168099  end= 5168403\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5659666  end= 5661169\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG008\n",
      "- Importing Spike data for BG008\n",
      "\u001b[94m- Added section start=3671028 end= 3673958to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=4716618 end= 4723918to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=5234359 end= 5236671to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3073041 to 3075901: 4 peaks (duration: 2860 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3233374 to 3235469: 3 peaks (duration: 2095 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3285333 to 3289233: 6 peaks (duration: 3900 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4435485 to 4440660: 7 peaks (duration: 5175 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4441000 to 4458109: 24 peaks (duration: 17109 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4458744 to 4468775: 15 peaks (duration: 10031 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4469335 to 4477919: 13 peaks (duration: 8584 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4479149 to 4486634: 11 peaks (duration: 7485 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4487210 to 4502869: 21 peaks (duration: 15659 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5100954 to 5108710: 12 peaks (duration: 7756 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5172963 to 5187558: 21 peaks (duration: 14595 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5189643 to 5205953: 24 peaks (duration: 16310 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5207163 to 5214733: 12 peaks (duration: 7570 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5215938 to 5231323: 22 peaks (duration: 15385 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5231893 to 5240402: 10 peaks (duration: 8509 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5240947 to 5249808: 11 peaks (duration: 8861 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5251208 to 5267942: 23 peaks (duration: 16734 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5269227 to 5284777: 21 peaks (duration: 15550 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5539153 to 5545213: 8 peaks (duration: 6060 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2357804  end= 2359701\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2512064  end= 2516679\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2563648  end= 2567288\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2661759  end= 2665596\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2763109  end= 2766173\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2920443  end= 2923969\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3074500  end= 3076833\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3176578  end= 3180313\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3636769  end= 3639895\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3671356  end= 3673458\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3744316  end= 3747557\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3796454  end= 3800235\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3957282  end= 3960127\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4211126  end= 4212240\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4555810  end= 4558831\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4638808  end= 4641837\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4743952  end= 4747638\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4796083  end= 4799822\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4990233  end= 4993575\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5042069  end= 5045235\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG009\n",
      "- Importing Spike data for BG009\n",
      "\u001b[94m- Added section start=5037791 end= 5093479to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5088572 to 5097583: 12 peaks (duration: 9011 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5098213 to 5107462: 11 peaks (duration: 9249 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5108302 to 5117848: 12 peaks (duration: 9546 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5118483 to 5128047: 12 peaks (duration: 9564 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5128817 to 5146262: 23 peaks (duration: 17445 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4021754  end= 4023080\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4643227  end= 4644090\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4696879  end= 4698207\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4943790  end= 4944947\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5472880  end= 5474225\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  6142032  end= 6143031\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  6366144  end= 6369094\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG010\n",
      "- Importing Spike data for BG010\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3352270  end= 3355880\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG011\n",
      "- Importing Spike data for BG011\n",
      "\u001b[94m- Added section start=5134621 end= 5166942to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3996373 to 4013358: 18 peaks (duration: 16985 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4124786 to 4146775: 21 peaks (duration: 21989 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4147785 to 4159766: 12 peaks (duration: 11981 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4160620 to 4171210: 11 peaks (duration: 10590 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4172080 to 4192670: 21 peaks (duration: 20590 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5454497 to 5463936: 9 peaks (duration: 9439 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1815516  end= 1817141\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2354867  end= 2356400\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2451870  end= 2454118\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2499960  end= 2502019\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2547105  end= 2548637\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2821885  end= 2823603\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3611214  end= 3613643\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4648310  end= 4650703\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5049364  end= 5051413\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5474613  end= 5477312\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG012\n",
      "- Importing Spike data for BG012\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4739768 to 4748279: 12 peaks (duration: 8511 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2382305  end= 2384354\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3309609  end= 3311532\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4188893  end= 4191241\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG013\n",
      "- Importing Spike data for BG013\n",
      "\u001b[94m- Added section start=2717129 end= 2732148to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2017730 to 2024129: 8 peaks (duration: 6399 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2925442 to 2931426: 8 peaks (duration: 5984 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3023940 to 3030695: 9 peaks (duration: 6755 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4295036 to 4301362: 8 peaks (duration: 6326 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4328612 to 4331946: 4 peaks (duration: 3334 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4357461 to 4368685: 14 peaks (duration: 11224 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4370515 to 4374116: 5 peaks (duration: 3601 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4430015 to 4439555: 12 peaks (duration: 9540 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4440275 to 4449825: 12 peaks (duration: 9550 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4450525 to 4459914: 11 peaks (duration: 9389 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4460644 to 4478269: 22 peaks (duration: 17625 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5624702 to 5633296: 12 peaks (duration: 8594 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2826885  end= 2828343\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4461444  end= 4463346\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4928089  end= 4929417\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4978474  end= 4979878\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5367246  end= 5368589\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5554802  end= 5556527\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG014\n",
      "- Importing Spike data for BG014\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2685980 to 2690549: 7 peaks (duration: 4569 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2061062  end= 2062293\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2138375  end= 2139532\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3478066  end= 3479936\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3604512  end= 3606319\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3794918  end= 3796975\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4595074  end= 4596648\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4847029  end= 4848430\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5376803  end= 5377917\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5386467  end= 5388386\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG015\n",
      "- Importing Spike data for BG015\n",
      "\u001b[94m- Added section start=2531508 end= 2537134to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=3106600 end= 3110528to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2108383  end= 2109833\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3485564  end= 3487779\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3774537  end= 3776780\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5047806  end= 5048985\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG016\n",
      "- Importing Spike data for BG016\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2340570  end= 2341484\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2688339  end= 2689620\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG017\n",
      "- Importing Spike data for BG017\n",
      "\u001b[94m- Added section start=4203442 end= 4217151to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 6661774 to 6661876: 0 peaks (duration: 102 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG018\n",
      "- Importing Spike data for BG018\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4300626 to 4308970: 14 peaks (duration: 8344 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4309500 to 4316140: 9 peaks (duration: 6640 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4318046 to 4325370: 11 peaks (duration: 7324 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4326720 to 4333835: 10 peaks (duration: 7115 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4335115 to 4349665: 21 peaks (duration: 14550 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG019\n",
      "- Importing Spike data for BG019\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2144233 to 2150668: 8 peaks (duration: 6435 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2151383 to 2159142: 11 peaks (duration: 7759 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2160323 to 2167103: 9 peaks (duration: 6780 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2167748 to 2183207: 23 peaks (duration: 15459 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3633275 to 3638955: 8 peaks (duration: 5680 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2183867  end= 2184608\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4177851  end= 4178635\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG020\n",
      "- Importing Spike data for BG020\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5059488 to 5066648: 9 peaks (duration: 7160 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5127843 to 5133172: 7 peaks (duration: 5329 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2396686  end= 2397425\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG021\n",
      "- Importing Spike data for BG021\n",
      "\u001b[94m- Added section start=2187028 end= 2199124to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=2299776 end= 2311455to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=3730200 end= 3738318to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=4033023 end= 4038637to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5491778 to 5509836: 23 peaks (duration: 18058 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2376706  end= 2377515\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2717944  end= 2720643\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2967240  end= 2970187\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3176980  end= 3178854\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5083179  end= 5085276\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5196166  end= 5197563\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5262986  end= 5264901\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (79) matches no. rows (79) data for this subject in df_all_combined\n",
      "\u001b[94mSet fisys/fidia responses for trial 79 response phase to nan\u001b[0m\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG022\n",
      "- Importing Spike data for BG022\n",
      "\u001b[94m- Added section start=1796283 end= 1804841to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1681530 to 1687001: 8 peaks (duration: 5471 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1822064 to 1829949: 17 peaks (duration: 7885 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1830414 to 1836734: 14 peaks (duration: 6320 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1838499 to 1851343: 20 peaks (duration: 12844 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4718539 to 4726993: 22 peaks (duration: 8454 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5623366 to 5625634: 4 peaks (duration: 2268 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2430709  end= 2432798\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2475595  end= 2477684\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5510493  end= 5511903\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG023\n",
      "- Importing Spike data for BG023\n",
      "\u001b[94m- Added section start=1813698 end= 1820057to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=1924781 end= 1938078to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=2846379 end= 2869584to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=3079776 end= 3083678to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=3137858 end= 3144960to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 933714 to 938258: 5 peaks (duration: 4544 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4184795 to 4202110: 20 peaks (duration: 17315 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  963387  end= 964631\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1090319  end= 1091706\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  141930  end= 1142807\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1813698  end= 1815555\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2861917  end= 2862946\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3850062  end= 3851190\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3885951  end= 3887320\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG024\n",
      "- Importing Spike data for BG024\n",
      "\u001b[94m- Added section start=3853597 end= 4650000to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2317649 to 2329628: 16 peaks (duration: 11979 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4593995 to 4601960: 13 peaks (duration: 7965 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4602470 to 4609315: 10 peaks (duration: 6845 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4610575 to 4617844: 17 peaks (duration: 7269 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4618974 to 4625670: 9 peaks (duration: 6696 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4626990 to 4641734: 24 peaks (duration: 14744 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5305939 to 5320242: 21 peaks (duration: 14303 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG025\n",
      "- Importing Spike data for BG025\n",
      "\u001b[94m- Added section start=3512978 end= 3550962to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=5160088 end= 5164631to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1779572 to 1790456: 19 peaks (duration: 10884 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3415419 to 3423090: 11 peaks (duration: 7671 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4215286 to 4233651: 24 peaks (duration: 18365 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4257981 to 4265875: 11 peaks (duration: 7894 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4266575 to 4275576: 12 peaks (duration: 9001 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4276216 to 4285355: 14 peaks (duration: 9139 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4286005 to 4303145: 24 peaks (duration: 17140 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4304540 to 4312690: 12 peaks (duration: 8150 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4313310 to 4321549: 11 peaks (duration: 8239 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4322260 to 4340054: 23 peaks (duration: 17794 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5828676 to 5833930: 7 peaks (duration: 5254 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2499550  end= 2500379\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3309927  end= 3311313\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3340221  end= 3341055\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3331351  end= 3335297\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3318248  end= 3318683\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3545161  end= 3546089\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3578381  end= 3579889\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3672834  end= 3673681\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3834995  end= 3835817\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3956596  end= 3957299\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4796095  end= 4797846\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4698505  end= 4700876\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5257235  end= 5257845\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5272915  end= 5276648\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5869798  end= 5871634\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  6003577  end= 6004584\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  6027023  end= 6028191\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  6038210  end= 6042081\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  6173568  end= 6176055\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG026\n",
      "- Importing Spike data for BG026\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2393568  end= 2393582\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2591591  end= 2592859\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4796907  end= 4797546\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG027\n",
      "- Importing Spike data for BG027\n",
      "\u001b[94m- Added section start=3201302 end= 3215771to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2991417 to 3002681: 15 peaks (duration: 11264 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4146178 to 4153578: 11 peaks (duration: 7400 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4602421 to 4608941: 10 peaks (duration: 6520 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2479059  end= 2480393\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2491611  end= 2492253\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2742375  end= 2743882\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3219993  end= 3220625\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3683993  end= 3686652\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4528589  end= 4530516\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4817946  end= 4819183\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG028\n",
      "- Importing Spike data for BG028\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG029\n",
      "- Importing Spike data for BG029\n",
      "\u001b[94m- Added section start=3551482 end= 3553887to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3961025 to 3971664: 13 peaks (duration: 10639 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3972439 to 3982519: 12 peaks (duration: 10080 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3983249 to 3993849: 13 peaks (duration: 10600 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3994574 to 4003913: 12 peaks (duration: 9339 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4004608 to 4021913: 22 peaks (duration: 17305 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4736162 to 4754137: 23 peaks (duration: 17975 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1953363  end= 1954177\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2266012  end= 2266831\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2326224  end= 2326999\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2646040  end= 2646456\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2950221  end= 2950973\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3105099  end= 3106483\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3431622  end= 3432417\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3469195  end= 3469734\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3551247  end= 3554152\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4535381  end= 4536432\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG030\n",
      "- Importing Spike data for BG030\n",
      "\u001b[94m- Added section start=3061749 end= 3200000to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1839814  end= 1841658\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3180559  end= 3181223\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3770433  end= 3771485\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4387997  end= 4389586\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4491151  end= 4493836\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG031\n",
      "- Importing Spike data for BG031\n",
      "\u001b[94m- Added section start=2471611 end= 2487132to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5170884 to 5176509: 6 peaks (duration: 5625 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3469829  end= 3471605\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4711095  end= 4712841\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5160749  end= 5162313\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG032\n",
      "- Importing Spike data for BG032\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5777894 to 5794816: 21 peaks (duration: 16922 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3365100  end= 3368258\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3709068  end= 3710432\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3767504  end= 3768958\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4238575  end= 4239522\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4707889  end= 4711529\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5599668  end= 5600733\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG033\n",
      "- Importing Spike data for BG033\n",
      "\u001b[94m- Added section start=4236658 end= 4431975to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2742465 to 2751776: 9 peaks (duration: 9311 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4299627 to 4313096: 17 peaks (duration: 13469 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4313846 to 4326606: 14 peaks (duration: 12760 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4364831 to 4377300: 13 peaks (duration: 12469 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4378270 to 4390031: 12 peaks (duration: 11761 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4760635 to 4771269: 11 peaks (duration: 10634 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5029775 to 5040445: 18 peaks (duration: 10670 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5044814 to 5052942: 8 peaks (duration: 8128 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3235854  end= 3236981\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3326062  end= 3327536\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3500275  end= 3501329\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3659593  end= 3661490\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3512653  end= 3513553\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3821529  end= 3823492\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4313772  end= 4314730\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4404698  end= 4405729\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4891083  end= 4892144\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG034\n",
      "- Importing Spike data for BG034\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4499066 to 4504565: 8 peaks (duration: 5499 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3022272  end= 3023647\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3054297  end= 3057046\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG035\n",
      "- Importing Spike data for BG035\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1533902 to 1549692: 22 peaks (duration: 15790 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4193326 to 4198751: 8 peaks (duration: 5425 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1276270  end= 1277811\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1537755  end= 1539984\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2531609  end= 2533681\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3156553  end= 3157454\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3199135  end= 3200009\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3312233  end= 3314184\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4335173  end= 4336204\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4561333  end= 4563367\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG036\n",
      "- Importing Spike data for BG036\n",
      "\u001b[94m- Added section start=1700000 end= 1777851to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4812497 to 4817282: 8 peaks (duration: 4785 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG037\n",
      "- Importing Spike data for BG037\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2325065 to 2328534: 4 peaks (duration: 3469 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4357576 to 4370310: 17 peaks (duration: 12734 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2325049  end= 2325910\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG038\n",
      "- Importing Spike data for BG038\n",
      "\u001b[94m- Added section start=8784702 end= 879309to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG039\n",
      "- Importing Spike data for BG039\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1608564  end= 1609415\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2056297  end= 2056605\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2095860  end= 2096921\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4300053  end= 4300781\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG040\n",
      "- Importing Spike data for BG040\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1641081 to 1649630: 11 peaks (duration: 8549 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1650345 to 1665796: 22 peaks (duration: 15451 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  870767  end= 871548\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  886415  end= 889179\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1555773  end= 1557340\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2154484  end= 2155892\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4154618  end= 4157736\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4525518  end= 4527067\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG041\n",
      "- Importing Spike data for BG041\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4334425 to 4356388: 23 peaks (duration: 21963 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2217325  end= 2219105\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2695371  end= 2696846\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4074041  end= 4075086\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG042\n",
      "- Importing Spike data for BG042\n",
      "\u001b[94m- Added section start=1335000 end= 1550000to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=1823625 end= 1839173to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=2085726 end= 2102388to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=2756396 end= 2759147to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=3510387 end= 3514333to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=4447656 end= 4477553to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 5116523 to 5119857: 5 peaks (duration: 3334 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1242103  end= 1243078\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1864214  end= 1864933\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1927299  end= 1930604\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2254857  end= 2255396\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2528336  end= 2528988\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2551281  end= 2552041\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2610138  end= 2610868\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2618027  end= 2619769\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2674952  end= 2676234\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2691550  end= 2691994\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2742276  end= 2743039\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2961096  end= 2961867\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3000091  end= 3000925\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3057893  end= 3058621\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3173356  end= 3174173\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3243726  end= 3244378\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3456815  end= 3460106\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3581033  end= 3581453\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3840889  end= 3841626\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3985029  end= 3986393\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4066511  end= 4067225\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4070615  end= 4071515\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4162305  end= 4163226\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4611584  end= 4612319\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4722193  end= 4723365\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4761160  end= 4761880\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4770066  end= 4770908\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4874118  end= 4874905\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  5048216  end= 5048922\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG043\n",
      "- Importing Spike data for BG043\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG044\n",
      "- Importing Spike data for BG044\n",
      "\u001b[94m- Added section start=1910562 end= 1929537to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1047545 to 1054626: 6 peaks (duration: 7081 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1490448 to 1516328: 21 peaks (duration: 25880 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2157358 to 2180583: 20 peaks (duration: 23225 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  367485  end= 375068\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  978654  end= 980033\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1736054  end= 1739734\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2126602  end= 2127331\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3603730  end= 3604285\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG045\n",
      "- Importing Spike data for BG045\n",
      "\u001b[94m- Added section start=1945123 end= 2013796to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=2042172 end= 2061052to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=2448788 end= 2456802to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=3341593 end= 3421828to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3381378 to 3389867: 11 peaks (duration: 8489 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3390547 to 3399488: 13 peaks (duration: 8941 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3400088 to 3408427: 11 peaks (duration: 8339 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3409157 to 3428557: 24 peaks (duration: 19400 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3430042 to 3447987: 24 peaks (duration: 17945 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4916042 to 4934077: 23 peaks (duration: 18035 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1243400  end= 1245575\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1306834  end= 1308822\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1403852  end= 1405057\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1763399  end= 1764449\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2036252  end= 2037290\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2107891  end= 2109371\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2381648  end= 2383155\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2483691  end= 2484411\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2623455  end= 2625072\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2729036  end= 2730111\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2083405  end= 2804182\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3534198  end= 3535454\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3859278  end= 3859984\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4338656  end= 4339746\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4351915  end= 4352274\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG046\n",
      "- Importing Spike data for BG046\n",
      "- Calculated noisy samples\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4134608  end= 4135388\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG047\n",
      "- Importing Spike data for BG047\n",
      "\u001b[94m- Added section start=4449505 end= 4454797to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 1304410 to 1309035: 5 peaks (duration: 4625 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2084493 to 2094893: 10 peaks (duration: 10400 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2095864 to 2107398: 12 peaks (duration: 11534 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2108348 to 2119657: 11 peaks (duration: 11309 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2120608 to 2131803: 11 peaks (duration: 11195 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2132758 to 2143642: 11 peaks (duration: 10884 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2144633 to 2165047: 21 peaks (duration: 20414 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1096755  end= 1097435\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1298624  end= 1299068\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1304390  end= 1305698\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1433966  end= 1434579\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1717347  end= 1718217\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2365904  end= 2366840\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3580228  end= 3582234\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG048\n",
      "- Importing Spike data for BG048\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2028250 to 2034954: 11 peaks (duration: 6704 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2035569 to 2042994: 11 peaks (duration: 7425 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2043600 to 2051130: 11 peaks (duration: 7530 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2051714 to 2058809: 11 peaks (duration: 7095 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2059489 to 2073734: 21 peaks (duration: 14245 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3918041 to 3926826: 13 peaks (duration: 8785 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3927461 to 3936541: 13 peaks (duration: 9080 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3937121 to 3953481: 24 peaks (duration: 16360 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3954125 to 3968601: 20 peaks (duration: 14476 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2166301  end= 2166911\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG049\n",
      "- Importing Spike data for BG049\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3488952 to 3495557: 9 peaks (duration: 6605 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4657724 to 4668660: 16 peaks (duration: 10936 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2142045  end= 2144272\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2195188  end= 2196446\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2287507  end= 2287988\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3949738  end= 3951695\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4373716  end= 4374372\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n",
      "Session:  BG050\n",
      "- Importing Spike data for BG050\n",
      "\u001b[94m- Added section start=1942562 end= 2227401to b2b_idx_noisy_samples\u001b[0m\n",
      "\u001b[94m- Added section start=3279007 end= 3322764to b2b_idx_noisy_samples\u001b[0m\n",
      "- Calculated noisy samples\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2060125 to 2068886: 10 peaks (duration: 8761 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2069776 to 2079980: 11 peaks (duration: 10204 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2080845 to 2090474: 12 peaks (duration: 9629 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2092105 to 2101820: 11 peaks (duration: 9715 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 2102630 to 2119935: 20 peaks (duration: 17305 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3026867 to 3035342: 11 peaks (duration: 8475 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3402158 to 3402726: 1 peaks (duration: 568 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3979398 to 3987818: 10 peaks (duration: 8420 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 3988628 to 3999778: 12 peaks (duration: 11150 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4000628 to 4010628: 11 peaks (duration: 10000 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4011323 to 4021677: 11 peaks (duration: 10354 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4022433 to 4042832: 22 peaks (duration: 20399 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4044452 to 4053012: 9 peaks (duration: 8560 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4053712 to 4062347: 9 peaks (duration: 8635 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4063242 to 4073432: 11 peaks (duration: 10190 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4074587 to 4092811: 22 peaks (duration: 18224 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4093521 to 4101606: 10 peaks (duration: 8085 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4304108 to 4326318: 22 peaks (duration: 22210 samples)\u001b[0m\n",
      "\u001b[91m Unstable physiocal (<25 beats) from sample 4964913 to 4965928: 1 peaks (duration: 1015 samples)\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1287965  end= 1289773\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1469010  end= 1471756\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  1708081  end= 1710635\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  2247429  end= 2250071\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3209618  end= 3210548\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3599207  end= 3599841\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3766480  end= 3767537\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  3774541  end= 3775248\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4169917  end= 4171655\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4414310  end= 4415913\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4457631  end= 4458224\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4697463  end= 4699415\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4819686  end= 4821400\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data between: start=  4827323  end= 4829238\u001b[0m\n",
      "\u001b[94mInterpolated fiSYS and fiDIA data from  1000  ms before to  1000  ms after each physiocal\u001b[0m\n",
      "Starting median smoothing\n",
      "Completed median smoothing\n",
      "- No. of apr textmarks (80) matches no. rows (80) data for this subject in df_all_combined\n",
      "- Successfully calculated fisys/fidia resposnes\n"
     ]
    }
   ],
   "source": [
    "run_b2b_analyses = True #define whether to run this section or skip it\n",
    "plot_b2b_data = False #define whether to plot b2b data\n",
    "median_smooth_b2b_data = True #define whether to median smooth b2b data\n",
    "how_process_physiocal = \"interpolate\" # \"interpolate\" or \"exclude\" - define whether to interpolate or exclude data around physiocal\n",
    "\n",
    "#Function for finding (start, end) indices of contiguous runs of `num` in a 1D signal\n",
    "def find_runs(signal, num):\n",
    "\n",
    "    \"\"\"\n",
    "    Find contiguous runs of a given value in a 1D signal.\n",
    "\n",
    "    Identifies start and end indices of consecutive segments where `signal == num`.\n",
    "    End indices are returned in Python slicing format (end is exclusive).\n",
    "\n",
    "    Args:\n",
    "        signal (array-like): 1D input signal.\n",
    "        num (int or float): Value to search for.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[int, int]]: List of (start_idx, end_idx) pairs for each run.\n",
    "\n",
    "    Example:\n",
    "        signal = [0, 1, 1, 1, 0, 1]\n",
    "        num = 1\n",
    "        returns [(1, 4), (5, 6)]\n",
    "    \"\"\"\n",
    "    \n",
    "    signal = np.asarray(signal)\n",
    "    mask = (signal == num)\n",
    "    diff = np.diff(mask.astype(int))\n",
    "    \n",
    "    starts = np.where(diff == 1)[0] + 1  # Starts of runs\n",
    "    ends = np.where(diff == -1)[0] + 1   # Ends of runs\n",
    "    \n",
    "    # Handle edge cases by inserting `num` at position 0 in starts/ends\n",
    "    if mask[0]:\n",
    "        starts = np.insert(starts, 0, num)  # Insert `num` at index 0 of starts\n",
    "    if mask[-1]:\n",
    "        ends = np.append(ends, len(signal))\n",
    "    \n",
    "    return list(zip(starts, ends))\n",
    "\n",
    "if run_b2b_analyses:\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "    # Visual verification variables\n",
    "    b2b_visual_verif_done = [] #Add IDs of sessions that have had visual verification of b2b data done\n",
    "\n",
    "    #------- Import excel file containing notes from ecg inspection\n",
    "    analysisFolder = BASE_DIR / 'Analysis'\n",
    "    b2b_inspection_file = os.path.join(analysisFolder, 'Data checks', 'empathy_b2b_inspection.xlsx')\n",
    "    df_b2b_inspection = pd.read_excel(b2b_inspection_file)\n",
    "\n",
    "    #Predefine hr timeseries columns as objects to allow insertion of entire timeseries into each cell\n",
    "    df_all_combined['fisys_series_waiting'] = None \n",
    "    df_all_combined = df_all_combined.astype({'fisys_series_waiting': 'object'})\n",
    "    df_all_combined['fidia_series_waiting'] = None\n",
    "    df_all_combined = df_all_combined.astype({'fidia_series_waiting': 'object'})\n",
    "\n",
    "    df_all_combined['fisys_series_anticipation'] = None\n",
    "    df_all_combined = df_all_combined.astype({'fisys_series_anticipation': 'object'})\n",
    "    df_all_combined['fidia_series_anticipation'] = None\n",
    "    df_all_combined = df_all_combined.astype({'fidia_series_anticipation': 'object'})\n",
    "\n",
    "    df_all_combined['fisys_series_response'] = None\n",
    "    df_all_combined = df_all_combined.astype({'fisys_series_response': 'object'})\n",
    "    df_all_combined['fidia_series_response'] = None\n",
    "    df_all_combined = df_all_combined.astype({'fidia_series_response': 'object'})\n",
    "\n",
    "    for sessionID in session_folders:\n",
    "\n",
    "        # get session folder\n",
    "        sessionFolder = os.path.join(directory, sessionID)\n",
    "        print('Session: ', sessionID) \n",
    "        \n",
    "        # Import Spike2 data file\n",
    "        spike_file_extension = \"empathy.smrx\"\n",
    "        spike_search_pattern = os.path.join(sessionFolder, f'*{spike_file_extension}')\n",
    "        spike_files = glob.glob(spike_search_pattern)\n",
    "        if len(spike_files) > 1:\n",
    "            print(\"-\", \"{} SMRX files found.\".format(len(spike_files)))\n",
    "        elif len(spike_files) < 1:\n",
    "            print(RED + 'WARNING: no empathy spike data files found')\n",
    "\n",
    "        # Process exceptions\n",
    "        if sessionID == 'BG020':\n",
    "            #For some reason, BG020 spike file is not being imported correctly so have duplicated, which works fine\n",
    "            spike_files = BASE_DIR / \"Participant data\" / \"BG020\" / \"BG020_empathy_duplicate.smrx\"\n",
    "\n",
    "        # Loop over files\n",
    "        for i_spike_file in range(len(spike_files)):\n",
    "            \n",
    "            spike_filename = spike_files[i_spike_file]\n",
    "\n",
    "            print(\"-\", \"Importing Spike data for {}\".format(sessionID))\n",
    "\n",
    "            # Read Spike2 file\n",
    "            reader = CedIO(filename=spike_filename)\n",
    "            \n",
    "            block = reader.read_block()\n",
    "            spike2data = block.segments[0] # single segment\n",
    "\n",
    "            # Create a dictionary of analog channels and their indexes\n",
    "            analog_channel_dictionary = {}\n",
    "\n",
    "            for data_channel in range(len(spike2data.analogsignals)):\n",
    "                channel_idx = data_channel\n",
    "                channel_name = spike2data.analogsignals[data_channel].array_annotations['channel_names'][0]\n",
    "                #print(channel_name)\n",
    "                analog_channel_dictionary[channel_name] = channel_idx\n",
    "\n",
    "            # ---------- Get the b2b data for the subjects\n",
    "            p_fisys_data = spike2data.analogsignals[analog_channel_dictionary['p_fiSYS']] #*IMPORTANT\n",
    "            p_fidia_data = spike2data.analogsignals[analog_channel_dictionary['p_fiDIA']] #*IMPORTANT\n",
    "            p_physiocal_data = spike2data.analogsignals[analog_channel_dictionary['p_physiocal']] #*IMPORTANT\n",
    "            p_ecg_data = spike2data.analogsignals[analog_channel_dictionary['p_ECG']] #*IMPORTANT\n",
    "            c_ecg_data = spike2data.analogsignals[analog_channel_dictionary['p_ECG']] #*IMPORTANT\n",
    "\n",
    "            #process exceptions\n",
    "            if sessionID == 'BG006':\n",
    "                #BG006 p and c psychophys channels are wrong way round for empathy task. Swap over!\n",
    "                p_ecg_data = spike2data.analogsignals[analog_channel_dictionary['c_ECG']] #*IMPORTANT\n",
    "            elif sessionID == 'BG041':\n",
    "                #BG041 b2b blood pressure recorded from companion not participant\n",
    "                p_ecg_data = spike2data.analogsignals[analog_channel_dictionary['c_ECG']] #*IMPORTANT\n",
    "\n",
    "\n",
    "            #Prepare b2b data\n",
    "            p_fisys_signal = p_fisys_data.magnitude.squeeze()\n",
    "            p_fidia_signal = p_fidia_data.magnitude.squeeze()\n",
    "            p_physiocal_signal = p_physiocal_data.magnitude.squeeze()\n",
    "        \n",
    "        # ---------- Extract textmarks from smrx file using SonPy\n",
    "        smrfile = sonpy.lib.SonFile(spike_filename, True)\n",
    "\n",
    "        textmarkChanNo = 30\n",
    "        chanIdxInFile = textmarkChanNo - 1\n",
    "\n",
    "        max_n_tick = smrfile.ChannelMaxTime(chanIdxInFile)\n",
    "\n",
    "        n_chunks = 4\n",
    "        breakpoints = divide_into_n_breakpoints(max_n_tick, n_chunks)\n",
    "        df_textmarks_allchunks = pd.DataFrame()\n",
    "\n",
    "        for i_chunk in range(n_chunks):\n",
    "            first_sample = breakpoints[i_chunk]\n",
    "            last_sample = breakpoints[i_chunk + 1]\n",
    "\n",
    "            # Adjust the last chunk to ensure it includes the final tick\n",
    "            if i_chunk == n_chunks - 1:\n",
    "                last_sample += 1  # Add 1 to include the very last tick\n",
    "            \n",
    "            nSamplesToImport = last_sample - first_sample\n",
    "            textmarks = smrfile.ReadTextMarks(chan=chanIdxInFile, nMax=nSamplesToImport, tFrom=first_sample, tUpto=last_sample)\n",
    "\n",
    "            textmarks_sampleNo = np.full(len(textmarks), np.nan)\n",
    "            textmarks_text = np.array(['---'] * len(textmarks), dtype=str)\n",
    "\n",
    "            for i_mark in range(len(textmarks)):\n",
    "                mark = textmarks[i_mark]\n",
    "                textmarks_sampleNo[i_mark] = round(mark.Tick / 100)\n",
    "                textmarks_text[i_mark] = ''.join([str(mark[0]), str(mark[1]), str(mark[2])])\n",
    "            \n",
    "            df_textmarks = pd.DataFrame({'sampleNo': textmarks_sampleNo, 'text': textmarks_text})\n",
    "            df_textmarks_allchunks = pd.concat([df_textmarks_allchunks, df_textmarks], ignore_index=True)\n",
    "\n",
    "        #IMPORTANT - process textmark exceptions\n",
    "        if sessionID == 'BG002':\n",
    "            #start from row idx 19\n",
    "            df_textmarks_allchunks = df_textmarks_allchunks.iloc[19:]\n",
    "\n",
    "        elif sessionID == 'BG021':\n",
    "            #add rat textmark at last sample of spike file (when it crashed)\n",
    "            new_row_rating = pd.DataFrame({\"sampleNo\": [5509836], \"text\": ['rat']})  # Create a new DataFrame with the new row\n",
    "            df_textmarks_allchunks = pd.concat([df_textmarks_allchunks, new_row_rating], ignore_index=True)  # Append the row  \n",
    "\n",
    "        #IMPORTANT - subtract 4 seconds from textmarks to account for transmission delay from Finometer\n",
    "        df_textmarks_allchunks[\"sampleNo\"] = df_textmarks_allchunks[\"sampleNo\"] - 4 * SPIKE_FS\n",
    "\n",
    "        #calculate subjectID\n",
    "        subjID = sessionID + 'p'\n",
    "\n",
    "        #Process exceptions\n",
    "        if sessionID == 'BG041':\n",
    "            #in session BG041, b2b data was recorded for companion not participant\n",
    "            subjID = sessionID + 'c'\n",
    "\n",
    "        #---- Define noisy sections of b2b data for exclusion\n",
    "        row_p_inspection = (df_b2b_inspection['Subject'] == subjID)# Find rows in b2b inspection excel file for this subject\n",
    "        row_p_ecg_inspection = (df_ecg_inspection['pID'] == subjID)# Find rows in ecg inspection excel file for this subject\n",
    "        b2b_idx_noisy_section = df_b2b_inspection.loc[row_p_inspection, 'B2B_bad_segments'] # Get sample numbers of entire sections of peaks to delete\n",
    "\n",
    "        if isinstance(b2b_idx_noisy_section, str): # Ensure it's a list or iterable of strings, in case it's a single value\n",
    "            b2b_idx_noisy_section = [b2b_idx_noisy_section]\n",
    "\n",
    "        b2b_idx_noisy_samples = [] # Initialise an empty list to collect all indices to delete\n",
    "\n",
    "        for cell_value in b2b_idx_noisy_section: # Process each cell value\n",
    "            if cell_value.lower() == 'none':  # Skip cells with 'none'\n",
    "                continue\n",
    "            for section in cell_value.split(','):  # Split multiple ranges by commas\n",
    "                start, end = map(int, section.split(':'))  # Split each range and convert to integers\n",
    "                b2b_idx_noisy_samples.extend(range(start, end + 1))  # Include the endpoint\n",
    "                print(BLUE + '- Added section start=' + str(start) + \" end= \" + str(end) + \"to b2b_idx_noisy_samples\" + RESET)\n",
    "                  \n",
    "        b2b_noisy_samples = np.full(len(p_fisys_data), False, dtype=bool) #define logical array\n",
    "        b2b_noisy_samples[b2b_idx_noisy_samples] = True #set noisy trials to true \n",
    "        \n",
    "        print('- Calculated noisy samples')\n",
    "\n",
    "        #---- Calculate when physiocal unstable\n",
    "        #Define when there are <30 beats between physiocal calibrations\n",
    "        p_physiocal_signal = np.round(p_physiocal_signal, 0).astype(int) #round p_physiocal_signal to 0 dp so it is either 0 or 1\n",
    "        \n",
    "        #Get ECG data\n",
    "        p_ecg_data_np = np.squeeze(p_ecg_data)\n",
    "        fps_p_ecg = int(p_ecg_data.sampling_rate.magnitude)\n",
    "\n",
    "        #*IMPORTANT - Process exceptions\n",
    "        if sessionID == \"BG004\":\n",
    "            p_ecg_data_np.magnitude[3933300:3975500] = 0\n",
    "            p_ecg_signal[3933300:3975500] = 0\n",
    "\n",
    "        if sessionID == \"BG026\":\n",
    "            p_ecg_data_np.magnitude[0:343500] = 0\n",
    "            p_ecg_signal[0:343500] = 0\n",
    "\n",
    "        #Detect R peaks\n",
    "        _, p_ecg_peaks_forb2b = ecg_peaks(signal=p_ecg_data_np, method='sleepecg', sfreq=fps_p_ecg) # participant data - use sleepecg algorithm from Systole\n",
    "\n",
    "        #Add missed R peaks\n",
    "        idx_p_peaks_to_add = df_ecg_inspection.loc[row_p_ecg_inspection, 'add_peaks']\n",
    "        idx_p_peaks_to_add = idx_p_peaks_to_add.to_numpy() #convert from series to numpy\n",
    "        idx_p_peaks_to_add = np.fromstring(','.join(idx_p_peaks_to_add), sep=',', dtype=float)#convert to float\n",
    "        idx_p_peaks_to_add = idx_p_peaks_to_add.astype(int)\n",
    "        if len(idx_p_peaks_to_add) > 0:\n",
    "            p_ecg_peaks_forb2b[idx_p_peaks_to_add] = True #change to 1 in p_ecg_peaks\n",
    "\n",
    "        #Find runs of zeros in physiocal\n",
    "        zero_runs = find_runs(p_physiocal_signal, 0)\n",
    "        # Identify bad sections with <25 ECG peaks between physiocal calibrations\n",
    "        sections_few_physiocals = []\n",
    "        physiocal_beats_threshold = 25 #IMPORTANT - physiocal beats threshold. \n",
    "        #Set to 25 not 30 to allow for differences between cleaned ECG and finometer n\n",
    "        for start, end in zero_runs:\n",
    "            n_peaks = np.sum(p_ecg_peaks_forb2b[start:end])\n",
    "            if n_peaks < physiocal_beats_threshold:\n",
    "                sections_few_physiocals.append({\n",
    "                    'start_idx': start,\n",
    "                    'end_idx': end,\n",
    "                    'n_peaks': n_peaks,\n",
    "                    'duration_samples': end - start\n",
    "                })\n",
    "\n",
    "        #get sample number of first \"apr\" textmark\n",
    "        apr_indices = df_textmarks_allchunks[df_textmarks_allchunks[\"text\"] == \"apr\"].index\n",
    "        first_apr_sample_no = int(df_textmarks_allchunks.loc[apr_indices[0], \"sampleNo\"])\n",
    "\n",
    "        # Print sections with too beats between physiocals\n",
    "        for section in sections_few_physiocals:\n",
    "            if section['start_idx'] > first_apr_sample_no:\n",
    "                print(RED + f\" Unstable physiocal (<25 beats) from sample {section['start_idx']} to {section['end_idx']}: \"\n",
    "                    f\"{section['n_peaks']} peaks (duration: {section['duration_samples']} samples)\" + RESET)\n",
    "                \n",
    "                #Record section as unusable in b2b_noisy_samples\n",
    "                b2b_noisy_samples[section['start_idx'] : section['end_idx']] = True #set samples with too few beats between physiocals to True\n",
    "\n",
    "        #---- Interpolate short noisy sections\n",
    "        # Interpolate short noisy sections\n",
    "        p_idx_sections_to_interpolate = df_b2b_inspection.loc[row_p_inspection, 'B2B_interpolate'] # Get sample numbers of short sections to interpolate\n",
    "\n",
    "        if isinstance(p_idx_sections_to_interpolate, str): # Ensure it's a list or iterable of strings, in case it's a single value\n",
    "            p_idx_sections_to_interpolate = [p_idx_sections_to_interpolate]\n",
    "\n",
    "        p_indices_to_delete = [] # Initialise an empty list to collect all indices to delete\n",
    "\n",
    "        for cell_value in p_idx_sections_to_interpolate: # Process each cell value\n",
    "            if cell_value.lower() == 'none':  # Skip cells with 'none'\n",
    "                continue\n",
    "            for section in cell_value.split(','):  # Split multiple ranges by commas\n",
    "                start, end = map(int, section.split(':'))  # Split each range and convert to integers\n",
    "                \n",
    "                # Check indices are within bounds\n",
    "                if end >= len(p_fisys_signal) or start < 0:\n",
    "                    print(RED + \"Invalid indices: start= \", str(start),\" end=\", str(end) + RESET)\n",
    "                    continue\n",
    "                \n",
    "                # Indices to interpolate (exclude endpoints)\n",
    "                interp_indices = np.arange(start + 1, end)\n",
    "                \n",
    "                # Known points (start and end of the segment)\n",
    "                x_known = np.array([start, end])\n",
    "\n",
    "                y_fisys_known = p_fisys_signal[x_known]\n",
    "                y_fidia_known = p_fidia_signal[x_known]\n",
    "                \n",
    "                # Create cubic spline interpolator\n",
    "                cs_fisys = CubicSpline(x_known, y_fisys_known)\n",
    "                cs_fidia = CubicSpline(x_known, y_fidia_known)\n",
    "                \n",
    "                # Interpolate and replace fiSYS and fiDIA values\n",
    "                p_fisys_signal[interp_indices] = cs_fisys(interp_indices)\n",
    "                p_fidia_signal[interp_indices] = cs_fidia(interp_indices)\n",
    "\n",
    "                # Print update\n",
    "                print(BLUE + \"Interpolated fiSYS and fiDIA data between: start= \", str(start),\" end=\", str(end) + RESET)\n",
    "        \n",
    "        #---------\n",
    "        #Interpolate OR Exclude data during physiocal\n",
    "        #Find runs of ones in physiocal\n",
    "        one_runs = find_runs(p_physiocal_signal, 1)\n",
    "        \n",
    "        #For each physiocal calibration, interpolate -X s before to +X s after physiocal\n",
    "        for start, end in one_runs:\n",
    "            \n",
    "            if how_process_physiocal == \"interpolate\":\n",
    "                #If interpolating blood pressure values during physiocal....\n",
    "                duration_extend_interpolation = 1 * SPIKE_FS #IMPORTANT - specify how long to interpolate either side of physiocal in s\n",
    "                start_interpolate = start - duration_extend_interpolation #calculate interpolation start and end points\n",
    "                end_interpolate = end + duration_extend_interpolation\n",
    "\n",
    "                #make sure interpolation does not extend beyond start or end of recording\n",
    "                start_interpolate = max(0, start_interpolate) #take maximum out of 0 and start_interpolate\n",
    "                end_interpolate = min(len(p_physiocal_signal)-1, end_interpolate) #take minimum out of data length and end_interpolate\n",
    "\n",
    "                # Indices to interpolate (exclude endpoints)\n",
    "                interp_indices = np.arange(start_interpolate + 1, end_interpolate)\n",
    "                \n",
    "                # Known points (start and end of the segment)\n",
    "                x_known = np.array([start_interpolate, end_interpolate])\n",
    "                y_known_fisys = p_fisys_signal[x_known]\n",
    "                y_known_fidia = p_fidia_signal[x_known]\n",
    "                \n",
    "                # Create cubic spline interpolator\n",
    "                cs_fisys = CubicSpline(x_known, y_known_fisys)\n",
    "                cs_fidia = CubicSpline(x_known, y_known_fidia)\n",
    "                \n",
    "                # Interpolate and replace fiSYS and fiDIA values\n",
    "                p_fisys_signal[interp_indices] = cs_fisys(interp_indices)\n",
    "                p_fidia_signal[interp_indices] = cs_fidia(interp_indices)\n",
    "            \n",
    "            elif how_process_physiocal == \"exclude\":\n",
    "                #If excluding blood pressure values during physiocal....\n",
    "                 b2b_noisy_samples[start:end] = True #set samples with too few beats between physiocals to True\n",
    "            else:\n",
    "                print(RED + \"ERROR: how_process_physiocal should be either 'interpolate' or 'exclude'\")\n",
    "\n",
    "        if how_process_physiocal == \"interpolate\":\n",
    "            # Print update\n",
    "            print(BLUE + \"Interpolated fiSYS and fiDIA data from \", str(duration_extend_interpolation),\" ms before to \", str(duration_extend_interpolation), \" ms after each physiocal\" + RESET)\n",
    "        elif how_process_physiocal == \"exclude\":\n",
    "            # Print update\n",
    "            print(BLUE + \"Excluded fiSYS and fiDIA data from \", str(duration_extend_interpolation),\" ms before to \", str(duration_extend_interpolation), \" ms after each physiocal\" + RESET)\n",
    "\n",
    "\n",
    "        #Downsample to X Hz for plotting and median smoothing\n",
    "        BP_downsample_target_fs = 10 #set target frequency for downsampling\n",
    "        BP_factor_downsampled = SPIKE_FS/BP_downsample_target_fs #work out the factor by which data has been downsampled \n",
    "        p_fisys_signal_ds = nk.signal_resample(p_fisys_signal, sampling_rate = SPIKE_FS, desired_sampling_rate = BP_downsample_target_fs) #downsample fisys\n",
    "        p_fidia_signal_ds = nk.signal_resample(p_fidia_signal, sampling_rate = SPIKE_FS, desired_sampling_rate = BP_downsample_target_fs) #downsample fidia\n",
    "        p_fisys_times_ds = nk.signal_resample(p_fisys_data.times, sampling_rate = SPIKE_FS, desired_sampling_rate = BP_downsample_target_fs) #downsample times\n",
    "        p_physiocal_signal_ds = nk.signal_resample(p_physiocal_signal, sampling_rate = SPIKE_FS, desired_sampling_rate = BP_downsample_target_fs) #downsample physiocal\n",
    "        b2b_noisy_samples_ds = nk.signal_resample(b2b_noisy_samples, sampling_rate = SPIKE_FS, desired_sampling_rate = BP_downsample_target_fs) #downsample b2b_noisy_samples_ds\n",
    "        \n",
    "        # If median-smoothing data, then apply median smoothing\n",
    "        if median_smooth_b2b_data:\n",
    "            print('Starting median smoothing')\n",
    "            window_size = 49  # window size for filtering in samples (must be odd)\n",
    "            p_fisys_signal_ds_medsmooth = medfilt(p_fisys_signal_ds, kernel_size=window_size)\n",
    "            p_fidia_signal_ds_medsmooth = medfilt(p_fidia_signal_ds, kernel_size=window_size)\n",
    "            print('Completed median smoothing')\n",
    "\n",
    "            # Define final preprocessed data as downsampled then median smoothed data\n",
    "            p_fisys_preproc = p_fisys_signal_ds_medsmooth\n",
    "            p_fidia_preproc = p_fidia_signal_ds_medsmooth\n",
    "        else:\n",
    "            # Define final preprocessed data as downsampled data\n",
    "            p_fisys_preproc = p_fisys_signal_ds\n",
    "            p_fidia_preproc = p_fidia_signal_ds\n",
    "\n",
    "        #----------------Plot figure\n",
    "        if plot_b2b_data:\n",
    "            \n",
    "\n",
    "            #----- Plot FiSYS data\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(x=p_fisys_times_ds, y=p_physiocal_signal_ds, mode='lines', name='Physiocal', line=dict(color='orange'))) #plot Physiocal trace\n",
    "            fig.add_trace(go.Scatter(x=p_fisys_times_ds, y=p_fisys_signal_ds, mode='lines', name='Raw', line=dict(color='blue'))) #plot fisys trace\n",
    "            \n",
    "            # If median-smoothing data, then plot smoothed data\n",
    "            if median_smooth_b2b_data:\n",
    "                fig.add_trace(go.Scatter(x=p_fisys_times_ds, y=p_fisys_signal_ds_medsmooth, mode='lines', name='Median filt.', line=dict(color='green'))) #plot median smoothed fisys trace\n",
    "\n",
    "            ## Add textmarks\n",
    "            textmarks_of_interest = ['apr', 'rat', 'sph', 'spl', 'sch', 'scl', 'sps', 'scs'] #apr = anticipation before name displayed; rat = subjects prompted to submit rating\n",
    "            df_textmarks_subset = df_textmarks_allchunks[df_textmarks_allchunks['text'].isin(textmarks_of_interest)]\n",
    "            for _, row in df_textmarks_subset.iterrows():\n",
    "                tm_sample_no = row['sampleNo']\n",
    "                tm_text = row['text']\n",
    "                \n",
    "                # Add vertical line\n",
    "                fig.add_vline(x=tm_sample_no/(SPIKE_FS), line_width=2, line_dash=\"dash\", line_color=\"grey\") #divide sample number by SPIKE_FS to convert to s\n",
    "                \n",
    "                # Add text annotation\n",
    "                fig.add_annotation(\n",
    "                    x=tm_sample_no/(SPIKE_FS),\n",
    "                    y=0.5,  # Adjust this value to position the text vertically\n",
    "                    text=tm_text,\n",
    "                    showarrow=False,\n",
    "                    textangle=-90,\n",
    "                    yanchor=\"bottom\",\n",
    "                    font=dict(color=\"black\", size=20)\n",
    "                )\n",
    "\n",
    "            fig.update_layout(\n",
    "                title = f\"{subjID}: FiSYS data\"\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "\n",
    "            #----- Plot FiDIA data\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(x=p_fisys_times_ds, y=p_physiocal_signal_ds, mode='lines', name='Physiocal', line=dict(color='orange'))) #plot Physiocal trace\n",
    "            fig.add_trace(go.Scatter(x=p_fisys_times_ds, y=p_fidia_signal_ds, mode='lines', name='Raw', line=dict(color='blue'))) #plot fidia trace\n",
    "            \n",
    "            # If median-smoothing data, then plot smoothed data\n",
    "            if median_smooth_b2b_data:\n",
    "                fig.add_trace(go.Scatter(x=p_fisys_times_ds, y=p_fidia_signal_ds_medsmooth, mode='lines', name='Median filt.', line=dict(color='green'))) #plot median smoothed fisys trace\n",
    "\n",
    "            ## Add textmarks\n",
    "            textmarks_of_interest = ['apr', 'rat', 'sph', 'spl', 'sch', 'scl', 'sps', 'scs'] #apr = anticipation before name displayed; rat = subjects prompted to submit rating\n",
    "            df_textmarks_subset = df_textmarks_allchunks[df_textmarks_allchunks['text'].isin(textmarks_of_interest)]\n",
    "            for _, row in df_textmarks_subset.iterrows():\n",
    "                tm_sample_no = row['sampleNo']\n",
    "                tm_text = row['text']\n",
    "                \n",
    "                # Add vertical line\n",
    "                fig.add_vline(x=tm_sample_no/(SPIKE_FS), line_width=2, line_dash=\"dash\", line_color=\"grey\") #divide sample number by SPIKE_FS to convert to s\n",
    "                \n",
    "                # Add text annotation\n",
    "                fig.add_annotation(\n",
    "                    x=tm_sample_no/(SPIKE_FS),\n",
    "                    y=0.5,  # Adjust this value to position the text vertically\n",
    "                    text=tm_text,\n",
    "                    showarrow=False,\n",
    "                    textangle=-90,\n",
    "                    yanchor=\"bottom\",\n",
    "                    font=dict(color=\"black\", size=20)\n",
    "                )\n",
    "\n",
    "            fig.update_layout(\n",
    "                title = f\"{subjID}: FiDIA data\"\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "\n",
    "        #Multiply b2b blood pressure values by 100 to make them correct scale\n",
    "        p_fisys_signal_ds = p_fisys_signal_ds * 100\n",
    "        p_fidia_signal_ds = p_fidia_signal_ds * 100\n",
    "        \n",
    "        #-------- Identify textmarks for each trial\n",
    " \n",
    "        #check that number of anticipation and shock textmarks matches number of trials\n",
    "        nTrialsInTextmarks = len(df_textmarks_allchunks[df_textmarks_allchunks['text'].isin(['apr'])]) #get number of trials in textmarks\n",
    "        subj_rows_combined = df_all_combined['subjectID'].str.contains(subjID, na=False, case=False)  #get subject rows in df_all_combined\n",
    "\n",
    "        if nTrialsInTextmarks != sum(subj_rows_combined):\n",
    "            print(RED + 'ERROR: there are ' + str(nTrialsInTextmarks) + \n",
    "                ' apr textmarks but ' + str(sum(subj_rows_combined)) + \n",
    "                ' apr textmarks but ' + str(sum(subj_rows_combined)) + \n",
    "                ' rows of data for this subject in df_all_combined' + RESET)\n",
    "        else:\n",
    "            print('- No. of apr textmarks (' + str(nTrialsInTextmarks) + \n",
    "                ') matches no. rows (' + str(sum(subj_rows_combined)) + \n",
    "                ') data for this subject in df_all_combined')\n",
    "\n",
    "\n",
    "        # Iterate through each trial and calculate SCL/SCR measures for each phase\n",
    "        #> Filter rows for the relevant textmarks in df_textmarks_all_chunks\n",
    "        relevant_textmarks = [\"apr\", #anticipation pre-name\n",
    "                                \"apo\", #anticipation post-name\n",
    "                                \"sph\", \"spl\", \"sps\", \"sch\", \"scl\", \"scs\", #shock delivery\n",
    "                                \"res\", #start of response period\n",
    "                                \"rat\"] #submit rating/experimenter change shock settings\n",
    "        filtered_textmarks = df_textmarks_allchunks[df_textmarks_allchunks[\"text\"].isin(relevant_textmarks)]\n",
    "\n",
    "        # Find all indices of the \"apr\" textmark (one for each trial)\n",
    "        apr_indices = filtered_textmarks[filtered_textmarks[\"text\"] == \"apr\"].index\n",
    "\n",
    "\n",
    "        # Iterate through each trial using \"apr\" textmarks as the starting point\n",
    "        for trial_idx, apr_idx in enumerate(apr_indices):\n",
    "            \n",
    "            #Calculate trial number (start at 1 not 0)\n",
    "            trial_no = trial_idx + 1\n",
    "            \n",
    "            #calculate subject row as logical array\n",
    "            subject_row = (df_all_combined['subjectID'].str.contains(subjID, na=False, case=False)) & (df_all_combined['trialNo'] == trial_no)\n",
    "\n",
    "            # Get the indices of the textmarks for this trial\n",
    "            sample_no_apr = int(filtered_textmarks.loc[apr_idx, \"sampleNo\"])\n",
    "\n",
    "            # Get the next \"apo\" textmark (signalling start of anticipation post name period) after \"apr\"\n",
    "            sample_no_apo = filtered_textmarks.loc[\n",
    "                (filtered_textmarks.index > apr_idx) & (filtered_textmarks[\"text\"] == \"apo\"), \"sampleNo\"\n",
    "            ].iloc[0]\n",
    "\n",
    "            # Get the first shock (sph, spl, sps, sch, scl, scs) textmark after \"apo\"\n",
    "            sample_no_shock = filtered_textmarks.loc[\n",
    "                (filtered_textmarks.index > apr_idx)\n",
    "                & (filtered_textmarks[\"text\"].isin([\"sph\", \"spl\", \"sps\", \"sch\", \"scl\", \"scs\"])),\n",
    "                \"sampleNo\",\n",
    "            ].iloc[0]\n",
    "\n",
    "            #Get the first response period (res) textmark after \"apo\"\n",
    "            sample_no_res = filtered_textmarks.loc[\n",
    "                (filtered_textmarks.index > apr_idx) & (filtered_textmarks[\"text\"] == \"res\"), \"sampleNo\"\n",
    "            ].iloc[0]\n",
    "\n",
    "            # Get the next \"rat\" textmark (signalling rating) after anticipation post name\n",
    "            sample_no_rat = filtered_textmarks.loc[\n",
    "                (filtered_textmarks.index > apr_idx)\n",
    "                & (filtered_textmarks[\"text\"] == \"rat\"),\n",
    "                \"sampleNo\",\n",
    "            ].iloc[0]\n",
    "\n",
    "            #Convert indices to integers\n",
    "            sample_no_apr_ds = int(round(int(sample_no_apr)/BP_factor_downsampled, 0))\n",
    "            sample_no_apo_ds = int(round(int(sample_no_apo)/BP_factor_downsampled, 0))\n",
    "            sample_no_shock_ds = int(round(int(sample_no_shock)/BP_factor_downsampled, 0))\n",
    "            sample_no_res_ds = int(round(int(sample_no_res)/BP_factor_downsampled, 0))\n",
    "            sample_no_rat_ds = int(round(int(sample_no_rat)/BP_factor_downsampled, 0))\n",
    "            durationShock_ds = int(round(DURATION_SHOCK/BP_factor_downsampled, 0))\n",
    "            durationWaiting_ds = int(round(DURATION_WAITING/BP_factor_downsampled, 0))\n",
    "            durationResponseExclShock_ds = int(round(DURATION_RESPONSE_EXCL_SHOCK/BP_factor_downsampled, 0))\n",
    "            durationPrePhase_ds = int(round(DURATION_PRE_PHASE/BP_factor_downsampled, 0))\n",
    "\n",
    "            #Check anticipation phase textmarks\n",
    "            if (sample_no_shock_ds - sample_no_apo_ds > round(10100/BP_factor_downsampled, 0) ) | (sample_no_shock_ds - sample_no_apo_ds < round(5900/BP_factor_downsampled, 0) ):\n",
    "                #print(RED + '- Trial ' +  str(trial_no) + ', Anticipation Phase is ' + str(sample_no_shock - sample_no_apo) + ' ms' + RESET)\n",
    "                \n",
    "                #If antiicpation phase is incorrect length, work out shock time as 500ms before start of response phase\n",
    "                sample_no_shock_ds = sample_no_res_ds - durationShock_ds\n",
    "                #print(BLUE + '- Trial ' +  str(trial_no) + ', New Anticipation Phase duration: ' + str(sample_no_shock - sample_no_apo) + ' ms' + RESET)\n",
    "                \n",
    "                #if this does not fix it then print error\n",
    "                if (sample_no_shock_ds - sample_no_apo_ds > round(10100/BP_factor_downsampled,0) ) | (sample_no_shock_ds - sample_no_apo_ds < round(5900/BP_factor_downsampled,0) ):\n",
    "                    print(RED + '- ERROR: Trial ' +  str(trial_no) + ', Anticipation Phase is ' + str(sample_no_shock_ds - sample_no_apo_ds) + ' ms even after using \"res\" textmark.' + RESET)\n",
    "\n",
    "            #-------- Calculate B2B measures for each phase\n",
    "\n",
    "            #Waiting\n",
    "            if any(b2b_noisy_samples_ds[sample_no_apr_ds : sample_no_apr_ds + durationWaiting_ds]): #if eda data is noisy then set values to NaN\n",
    "                    mean_fisys_waiting = np.nan # mean fisys\n",
    "                    mean_fidia_waiting = np.nan  # mean fidia\n",
    "            else:\n",
    "                p_fisys_waiting = p_fisys_signal_ds[sample_no_apr_ds : sample_no_apr_ds + durationWaiting_ds] \n",
    "                p_fidia_waiting = p_fidia_signal_ds[sample_no_apr_ds : sample_no_apr_ds + durationWaiting_ds] \n",
    "                mean_fisys_waiting = p_fisys_waiting.mean() # Mean fiSYS\n",
    "                mean_fidia_waiting = p_fidia_waiting.mean() # Mean fiDIA\n",
    "\n",
    "\n",
    "            #Anticipation\n",
    "            if any(b2b_noisy_samples_ds[sample_no_apo_ds : sample_no_shock_ds]): #if eda data is noisy then set values to NaN\n",
    "                    mean_fisys_anticipation = np.nan # mean fisys\n",
    "                    mean_fidia_anticipation = np.nan  # mean fidia\n",
    "            else:\n",
    "                p_fisys_anticipation = p_fisys_signal_ds[sample_no_apo_ds : sample_no_shock_ds] \n",
    "                p_fidia_anticipation = p_fidia_signal_ds[sample_no_apo_ds : sample_no_shock_ds] \n",
    "                mean_fisys_anticipation = p_fisys_anticipation.mean() # Mean fiSYS\n",
    "                mean_fidia_anticipation = p_fidia_anticipation.mean()  # Mean fiDIA\n",
    "\n",
    "            #Response\n",
    "            if any(b2b_noisy_samples_ds[sample_no_shock_ds + durationShock_ds : sample_no_shock_ds + durationShock_ds + durationResponseExclShock_ds]): #if eda data is noisy then set values to NaN\n",
    "                    mean_fisys_response = np.nan # mean fisys\n",
    "                    mean_fidia_response = np.nan  # mean fidia\n",
    "            else:\n",
    "                p_fisys_response = p_fisys_signal_ds[sample_no_shock_ds + durationShock_ds : sample_no_shock_ds + durationShock_ds + durationResponseExclShock_ds] \n",
    "                p_fidia_response = p_fidia_signal_ds[sample_no_shock_ds + durationShock_ds : sample_no_shock_ds + durationShock_ds + durationResponseExclShock_ds] \n",
    "                #do not include shock in mean\n",
    "                mean_fisys_response = p_fisys_response.mean() # Mean fiSYS\n",
    "                mean_fidia_response = p_fidia_response.mean()  # Mean fiDIA\n",
    "                \n",
    "            #------------ Calculate b2b timeseries for each phase\n",
    "            \n",
    "            #Waiting\n",
    "            if any(b2b_noisy_samples_ds[sample_no_apr_ds - durationPrePhase_ds : sample_no_apr_ds + durationWaiting_ds]): #if eda data is noisy then set values to NaN\n",
    "                downsampled_fisys_waiting_ts = np.array([np.nan])\n",
    "                downsampled_fidia_waiting_ts = np.array([np.nan])\n",
    "            else:\n",
    "                p_fisys_waiting_ts = p_fisys_signal_ds[sample_no_apr_ds - durationPrePhase_ds : sample_no_apr_ds + durationWaiting_ds]\n",
    "                p_fidia_waiting_ts = p_fidia_signal_ds[sample_no_apr_ds - durationPrePhase_ds : sample_no_apr_ds + durationWaiting_ds]\n",
    "                downsampled_fisys_waiting_ts = np.round(downsample_with_last(p_fisys_waiting_ts, BP_downsample_target_fs, target_fs = target_fs_ts), 2) #downsample\n",
    "                downsampled_fidia_waiting_ts = np.round(downsample_with_last(p_fidia_waiting_ts, BP_downsample_target_fs, target_fs = target_fs_ts), 2) #downsample\n",
    "\n",
    "            #Anticipation\n",
    "            if any(b2b_noisy_samples_ds[sample_no_apo_ds - durationPrePhase_ds : sample_no_shock_ds]): #if eda data is noisy then set values to NaN\n",
    "                downsampled_fisys_anticipation_ts = np.array([np.nan])\n",
    "                downsampled_fidia_anticipation_ts = np.array([np.nan])\n",
    "            else:\n",
    "                p_fisys_anticipation_ts = p_fisys_signal_ds[sample_no_apo_ds - durationPrePhase_ds : sample_no_shock_ds]\n",
    "                p_fidia_anticipation_ts = p_fidia_signal_ds[sample_no_apo_ds - durationPrePhase_ds : sample_no_shock_ds]\n",
    "                downsampled_fisys_anticipation_ts = np.round(downsample_with_last(p_fisys_anticipation_ts, BP_downsample_target_fs, target_fs = target_fs_ts), 2) #downsample\n",
    "                downsampled_fidia_anticipation_ts = np.round(downsample_with_last(p_fidia_anticipation_ts, BP_downsample_target_fs, target_fs = target_fs_ts), 2) #downsample\n",
    "\n",
    "            #Response\n",
    "            if any(b2b_noisy_samples_ds[sample_no_shock_ds - durationPrePhase_ds : sample_no_shock_ds + durationShock_ds + durationResponseExclShock_ds]): #if eda data is noisy then set values to NaN\n",
    "                downsampled_fisys_response_ts = np.array([np.nan])\n",
    "                downsampled_fidia_response_ts = np.array([np.nan])\n",
    "            else:\n",
    "                #include shock in timeseries\n",
    "                p_fisys_response_ts = p_fisys_signal_ds[sample_no_shock_ds - durationPrePhase_ds : sample_no_shock_ds + durationShock_ds + durationResponseExclShock_ds]\n",
    "                p_fidia_response_ts = p_fidia_signal_ds[sample_no_shock_ds - durationPrePhase_ds : sample_no_shock_ds + durationShock_ds + durationResponseExclShock_ds]\n",
    "                downsampled_fisys_response_ts = np.round(downsample_with_last(p_fisys_response_ts, BP_downsample_target_fs, target_fs = target_fs_ts), 2) #downsample\n",
    "                downsampled_fidia_response_ts = np.round(downsample_with_last(p_fidia_response_ts, BP_downsample_target_fs, target_fs = target_fs_ts), 2) #downsample\n",
    "\n",
    "            #if subject_row is valid then add fisys and fidia values to dataframe\n",
    "            if sum(subject_row) == 1:\n",
    "                # Add mean fisys to dataframe\n",
    "                df_all_combined.loc[subject_row, \"mean_fisys_waiting\"] = np.round(mean_fisys_waiting, 4) \n",
    "                df_all_combined.loc[subject_row, \"mean_fisys_anticipation\"] = np.round(mean_fisys_anticipation, 4)\n",
    "                df_all_combined.loc[subject_row, \"mean_fisys_response\"] = np.round(mean_fisys_response, 4)\n",
    "\n",
    "                # Add mean fidia to dataframe\n",
    "                df_all_combined.loc[subject_row, \"mean_fidia_waiting\"] = np.round(mean_fidia_waiting, 4)\n",
    "                df_all_combined.loc[subject_row, \"mean_fidia_anticipation\"] = np.round(mean_fidia_anticipation, 4)\n",
    "                df_all_combined.loc[subject_row, \"mean_fidia_response\"] = np.round(mean_fidia_response, 4)\n",
    "\n",
    "                # Add fisys timeseries to the dataframe\n",
    "                #>Assign as a string representation of the list\n",
    "                df_all_combined.loc[subject_row, 'fisys_series_waiting'] = str(downsampled_fisys_waiting_ts.tolist())\n",
    "                df_all_combined.loc[subject_row, 'fisys_series_anticipation'] = str(downsampled_fisys_anticipation_ts.tolist())\n",
    "                df_all_combined.loc[subject_row, 'fisys_series_response'] = str(downsampled_fisys_response_ts.tolist())\n",
    "\n",
    "                # Add fisys timeseries to the dataframe\n",
    "                #>Assign as a string representation of the list\n",
    "                df_all_combined.loc[subject_row, 'fidia_series_waiting'] = str(downsampled_fidia_waiting_ts.tolist())\n",
    "                df_all_combined.loc[subject_row, 'fidia_series_anticipation'] = str(downsampled_fidia_anticipation_ts.tolist())\n",
    "                df_all_combined.loc[subject_row, 'fidia_series_response'] = str(downsampled_fidia_response_ts.tolist())\n",
    "\n",
    "                #Process exceptions\n",
    "\n",
    "                #> BG021 trial 79 crashed during response phase so set responses to nan\n",
    "                if (sessionID == 'BG021') & (trial_no == 79):\n",
    "                    df_all_combined.loc[subject_row, \"mean_fisys_response\"] = np.nan\n",
    "                    df_all_combined.loc[subject_row, \"mean_fidia_response\"] = np.nan\n",
    "                    df_all_combined.loc[subject_row, 'fisys_series_response'] = str( np.array([np.nan]) )\n",
    "                    df_all_combined.loc[subject_row, 'fidia_series_response'] = str( np.array([np.nan]) )\n",
    "                    print(BLUE + 'Set fisys/fidia responses for trial 79 response phase to nan' + RESET)\n",
    "\n",
    "            elif sum(subject_row) > 1 :\n",
    "                print(RED + f\"WARNING: multiple subject_rows {subject_row} is in df_all_combined.\" + RESET)\n",
    "            elif sum(subject_row) == 0 :\n",
    "                print(RED + f\"WARNING: subject_row {subject_row} is not in df_all_combined.\" + RESET)\n",
    "        \n",
    "        #Process exceptions\n",
    "        #> BG001p b2b data is unusable so do not include any b2b data for any trials\n",
    "        if subjID == 'BG001p':\n",
    "            df_all_combined.loc[subj_rows_combined, [\"mean_fisys_waiting\", \"mean_fisys_anticipation\", \"mean_fisys_response\"]] = np.nan\n",
    "            df_all_combined.loc[subj_rows_combined, [\"mean_fidia_waiting\", \"mean_fidia_anticipation\", \"mean_fidia_response\"]] = np.nan\n",
    "            df_all_combined.loc[subj_rows_combined, ['fisys_series_waiting', 'fisys_series_anticipation', 'fisys_series_response',\n",
    "                                                     'fidia_series_waiting', 'fidia_series_anticipation', 'fidia_series_response']] = str( np.array([np.nan]) )\n",
    "            print(BLUE + '- BG007p set b2b responses for all trials nan due to unusable b2b data' + RESET)\n",
    "\n",
    "        #Print message\n",
    "        print('- Successfully calculated fisys/fidia resposnes')                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate resting state HRV and EDA measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session:  BG001\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG001\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG001p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG001c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG002\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG002\\\\BG002_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG002\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG002p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 446146:447941 \u001b[0m\n",
      "- No peaks deleted\n",
      "\u001b[94m- Added missing peaks  [447311]\u001b[0m\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG002c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG003\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG003\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG003p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG003c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG004\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG004\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG004p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG004c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG005\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG005\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG005p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG005c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG006\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG006\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG006p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG006c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG007\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bgreenwood\\AppData\\Local\\miniconda3\\envs\\empathy_env\\lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:529: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG007p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG007c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "\u001b[94m- BG007c rs EDA data excluded due to faulty recording\u001b[0m\n",
      "\u001b[94m- BG007c rs EDA data excluded due to faulty recording\u001b[0m\n",
      "\u001b[94m- BG007c rs EDA data excluded due to faulty recording\u001b[0m\n",
      "\u001b[94m- BG007c rs EDA data excluded due to faulty recording\u001b[0m\n",
      "\u001b[94m- BG007c rs EDA data excluded due to faulty recording\u001b[0m\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG008\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG008\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG008p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG008c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG009\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG009\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG009p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 1082618:1084675 \u001b[0m\n",
      "- No peaks deleted\n",
      "\u001b[94m- Added missing peaks  [1083787 1084428]\u001b[0m\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG009c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG010\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG010\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG010p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG010c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [406279 406641 410954 412922 413176]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [406711]\u001b[0m\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG011\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG011\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG011p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG011c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG012\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG012\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG012p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG012c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG013\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG013\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG013p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG013c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 477394:480142 \u001b[0m\n",
      "- No peaks deleted\n",
      "\u001b[94m- Added missing peaks  [447678 478352 478992]\u001b[0m\n",
      "idx_subj_ectopics_to_interpolate [479497]\n",
      "rs_idx_peaks_to_delete [112417]\n",
      "\u001b[94mInterpolated IBI from 1273 to 1020.4275902070902 \u001b[0m\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG014\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG014\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG014p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG014c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG015\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG015\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG015p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG015c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG016\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG016\\\\BG016_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG016\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG016p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG016c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG017\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG017\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG017p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bgreenwood\\AppData\\Local\\miniconda3\\envs\\empathy_env\\lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:529: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG017c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG018\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG018\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG018p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG018c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG019\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG019\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG019p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG019c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG020\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG020\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG020p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG020c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG021\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG021\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG021p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG021c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG022\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG022\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG022p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 339656:340769, 430755:431522 \u001b[0m\n",
      "\u001b[91mERROR: No peaks to delete at sample nos.  [341361 354328 392179 413375 413601] \u001b[0m\n",
      "\u001b[94mHowever peaks found on previous samples [341360 354327 392178 413374 413600]\u001b[0m\n",
      "\u001b[94m- Successfully deleted peaks from previous samples at [341360 354327 392178 413374 413600]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [339925 340508 413559 431226]\u001b[0m\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG022c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG023\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG023\\\\BG023_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG023\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG023p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG023c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG024\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG024\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG024p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG024c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG025\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG025\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG025p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG025c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG026\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG026\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG026p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG026c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 595955:596843 \u001b[0m\n",
      "- No peaks deleted\n",
      "\u001b[94m- Added missing peaks  [596479]\u001b[0m\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG027\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG027\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG027p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG027c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG028\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG028\\\\BG028_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG028\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG028p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG028c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG029\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG029\\\\BG029_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG029\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG029p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG029c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG030\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG030\\\\BG030_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG030\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG030p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG030c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG031\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG031\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG031p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG031c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG032\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG032\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG032p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG032c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG033\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG033\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG033p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bgreenwood\\AppData\\Local\\miniconda3\\envs\\empathy_env\\lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:529: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG033c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [146846 147341 149165 186400]\u001b[0m\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG034\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG034\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG034p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG034c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG035\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG035\\\\BG035_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG035\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG035p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG035c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG036\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG036\\\\BG036_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG036\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG036p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate [193017 201462 273806 285953 294174]\n",
      "rs_idx_peaks_to_delete [ 16014  24459  96803 108950 117171]\n",
      "\u001b[94mInterpolated IBI from 1392 to 669.760512624686 \u001b[0m\n",
      "\u001b[94mInterpolated IBI from 1480 to 707.2902393441785 \u001b[0m\n",
      "\u001b[94mInterpolated IBI from 1510 to 678.6802720659066 \u001b[0m\n",
      "\u001b[94mInterpolated IBI from 1415 to 672.2944580133967 \u001b[0m\n",
      "\u001b[94mInterpolated IBI from 1555 to 735.3966579561401 \u001b[0m\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG036c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG037\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG037\\\\BG037_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG037\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG037p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG037c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG038\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG038\\\\BG038_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG038\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG038p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG038c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG039\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG039\\\\BG039_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG039\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG039p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG039c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG040\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG040\\\\BG040_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG040\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG040p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG040c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG041\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG041\\\\BG041_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG041\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG041p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bgreenwood\\AppData\\Local\\miniconda3\\envs\\empathy_env\\lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:529: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\bgreenwood\\AppData\\Local\\miniconda3\\envs\\empathy_env\\lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:529: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG041c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG042\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG042\\\\BG042_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG042\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG042p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG042c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG043\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG043\\\\BG043_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG043\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG043p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG043c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG044\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG044\\\\BG044_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG044\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG044p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bgreenwood\\AppData\\Local\\miniconda3\\envs\\empathy_env\\lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:529: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG044c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG045\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG045\\\\BG045_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG045\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG045p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG045c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG046\n",
      "[]\n",
      "- using empathy.smrx spike file\n",
      "- Importing Spike data for BG046\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG046p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG046c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG047\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG047\\\\BG047_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG047\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG047p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bgreenwood\\AppData\\Local\\miniconda3\\envs\\empathy_env\\lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:529: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG047c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG048\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG048\\\\BG048_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG048\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG048p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG048c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG049\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG049\\\\BG049_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG049\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG049p peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG049c peak deletion/addition\n",
      "- No peaks deleted\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "Session:  BG050\n",
      "['F:/ADHD emotion/Empathy study/Participant data\\\\BG050\\\\BG050_rs.smrx']\n",
      "- rs.smrx file found\n",
      "- Importing Spike data for BG050\n",
      "- p transfer constant = 2.5\n",
      "- c transfer constant = 2.5\n",
      "BG050p peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks from entire ranges: 769460:770652 \u001b[0m\n",
      "- No peaks deleted\n",
      "\u001b[94m- Added missing peaks  [770122]\u001b[0m\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n",
      "BG050c peak deletion/addition\n",
      "\u001b[94m- Successfully deleted peaks at [706835 707046 718282 718501]\u001b[0m\n",
      "\u001b[94m- Added missing peaks  [706995 718451]\u001b[0m\n",
      "idx_subj_ectopics_to_interpolate []\n",
      "rs_idx_peaks_to_delete []\n",
      "- Successfully calculated HRV parameters after ectopic interpolation\n",
      "- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix\n",
      "- Applied transfer constant 2.5\n",
      "- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix\n"
     ]
    }
   ],
   "source": [
    "run_rs_analyses = True #define whether to run this section of skip it\n",
    "# Visual verification variables\n",
    "hr_visual_verification = False\n",
    "eda_visual_verification = False\n",
    "\n",
    "\n",
    "if run_rs_analyses:\n",
    "    # Suppress the specific DeprecationWarning\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "    num_plotted = 0\n",
    "    max_plotted = 100\n",
    "    ecg_visual_verif_done = ['BG001', 'BG002', 'BG003', 'BG004', 'BG005',\n",
    "                             'BG006', 'BG007', 'BG008', 'BG009', 'BG010',\n",
    "                             'BG011', 'BG012', 'BG013', 'BG014', 'BG015',\n",
    "                             'BG016', 'BG017', 'BG018', 'BG019', 'BG020',\n",
    "                             'BG021', 'BG022', 'BG023', 'BG024', 'BG025',\n",
    "                             'BG026', 'BG027', 'BG028', 'BG029', 'BG030',\n",
    "                             'BG031', 'BG032', 'BG033', 'BG034', 'BG035',\n",
    "                             'BG036', 'BG037', 'BG038', 'BG039', 'BG040',\n",
    "                             'BG041', 'BG042', 'BG043', 'BG044', 'BG045',\n",
    "                             'BG046', 'BG047', 'BG048', 'BG049', 'BG050']\n",
    "\n",
    "    #------- Peak detection algorithm exceptions\n",
    "    peak_detection_algs = {sessionID: 'sleepecg' for sessionID in session_folders}\n",
    "    #peak_detection_algs['BG026'] = 'hamilton' #option to swap peak detection algorithm to 'hamilton' for selected participants if 'sleepecg' algorithm\n",
    "\n",
    "    #------- Import excel file containing notes from ecg inspection\n",
    "    analysisFolder = BASE_DIR / 'Analysis'\n",
    "    rs_ecg_inspection_file = os.path.join(analysisFolder, 'Data checks', 'rs_ecg_inspection.xlsx')\n",
    "    df_rs_ecg_inspection = pd.read_excel(rs_ecg_inspection_file)\n",
    "\n",
    "    # ------- Dictionary for resting-state textmark exceptions\n",
    "    rs_exceptions = {\n",
    "        'BG002': {'start': 418900, 'end': 418900 + 120000},\n",
    "        'BG016': {'start': 'rss_last', 'end': 152249},\n",
    "        'BG023': {'start': 213083, 'end': 333091},\n",
    "        'BG028': {'start': 200131, 'end': 320135},\n",
    "        'BG029': {'start': 417083, 'end': 537067},\n",
    "        'BG030': {'start': 519183, 'end': 639222},\n",
    "        'BG035': {'start': 3262,   'end': 123294},\n",
    "        'BG036': {'start': 177003, 'end': 297034},\n",
    "        'BG037': {'start': 34779,  'end': 154791},\n",
    "        'BG038': {'start': 41609,  'end': 161617},\n",
    "        'BG039': {'start': 38219,  'end': 158253},\n",
    "        'BG040': {'start': 31694,  'end': 151746},\n",
    "        'BG041': {'start': 26406,  'end': 146410},\n",
    "        'BG042': {'start': 214415, 'end': 334427},\n",
    "        'BG043': {'start': 114942, 'end': 234954},\n",
    "        'BG044': {'start': 37418,  'end': 157438},\n",
    "        'BG045': {'start': 94772,  'end': 214796},\n",
    "        'BG047': {'start': 30088,  'end': 150118},\n",
    "        'BG048': {'start': 10695,  'end': 130675},\n",
    "        'BG049': {'start': 37199,  'end': 157227},\n",
    "        'BG050': {'start': 682066, 'end': 802089},\n",
    "    }\n",
    "\n",
    "    for sessionID in session_folders:\n",
    "\n",
    "        # get session folder\n",
    "        sessionFolder = os.path.join(directory, sessionID)\n",
    "        print('Session: ', sessionID) \n",
    "        \n",
    "        #debugging:\n",
    "        #print(sessionFolder)\n",
    "\n",
    "        # Look for spike data file ending rs.smrx\n",
    "        rs_spike_file_extension = \"rs.smrx\"\n",
    "        rs_spike_search_pattern = os.path.join(sessionFolder, f'*{rs_spike_file_extension}')\n",
    "        rs_spike_files = glob.glob(rs_spike_search_pattern)\n",
    "        print(rs_spike_files)\n",
    "\n",
    "        #if found then use it, otherwise use empathy.smrx file\n",
    "        if len(rs_spike_files) >= 1:\n",
    "            spike_filename = rs_spike_files[0]\n",
    "            print(\"-\", \"rs.smrx file found\")\n",
    "        else:\n",
    "            empathy_spike_file_extension = \"empathy.smrx\"\n",
    "            rs_spike_search_pattern = os.path.join(sessionFolder, f'*{empathy_spike_file_extension}')\n",
    "            rs_spike_files = glob.glob(rs_spike_search_pattern)\n",
    "            spike_filename = rs_spike_files[0]\n",
    "            print(\"-\", \"using empathy.smrx spike file\")\n",
    "\n",
    "        #Process exceptions\n",
    "        if sessionID == 'BG042':\n",
    "            spike_filename = BASE_DIR / \"Participant data\" / \"BG042\" / \"BG042_rs_duplicate.smrx\"\n",
    "\n",
    "        if sessionID == 'BG043':\n",
    "            spike_filename = BASE_DIR / \"Participant data\" / \"BG043\" / \"BG043_rs_duplicate.smrx\"\n",
    "\n",
    "        # Loop over files\n",
    "        print(\"-\", \"Importing Spike data for {}\".format(sessionID))\n",
    "\n",
    "        # Read Spike2 file\n",
    "        reader = CedIO(filename=spike_filename)\n",
    "        \n",
    "        block = reader.read_block()\n",
    "        spike2data = block.segments[0] # single segment\n",
    "\n",
    "        # Create a dictionary of analog channels and their indexes\n",
    "        analog_channel_dictionary = {}\n",
    "\n",
    "        for data_channel in range(len(spike2data.analogsignals)):\n",
    "            channel_idx = data_channel\n",
    "            channel_name = spike2data.analogsignals[data_channel].array_annotations['channel_names'][0]\n",
    "            analog_channel_dictionary[channel_name] = channel_idx\n",
    "\n",
    "        #print('Analog channels identified:', analog_channel_dictionary)\n",
    "    \n",
    "        # ---------- Extract textmarks from smrx file using SonPy\n",
    "        smrfile = sonpy.lib.SonFile(spike_filename, True)\n",
    "\n",
    "        textmarkChanNo = 30\n",
    "        chanIdxInFile = textmarkChanNo - 1\n",
    "\n",
    "        max_n_tick = smrfile.ChannelMaxTime(chanIdxInFile)\n",
    "        #n_samples = int(np.floor(max_n_tick))\n",
    "        #print('Max n tick =', max_n_tick)\n",
    "\n",
    "        n_chunks = 4\n",
    "        breakpoints = divide_into_n_breakpoints(max_n_tick, n_chunks)\n",
    "        df_textmarks_allchunks = pd.DataFrame()\n",
    "\n",
    "        for i_chunk in range(n_chunks):\n",
    "            first_sample = breakpoints[i_chunk]\n",
    "            last_sample = breakpoints[i_chunk + 1]\n",
    "\n",
    "            # Adjust the last chunk to ensure it includes the final tick\n",
    "            if i_chunk == n_chunks - 1:\n",
    "                last_sample += 1  # Add 1 to include the very last tick\n",
    "            \n",
    "            nSamplesToImport = last_sample - first_sample\n",
    "            textmarks = smrfile.ReadTextMarks(chan=chanIdxInFile, nMax=nSamplesToImport, tFrom=first_sample, tUpto=last_sample)\n",
    "            \n",
    "            textmarks_sampleNo = np.full(len(textmarks), np.nan)\n",
    "            textmarks_text = np.array(['---'] * len(textmarks), dtype=str)\n",
    "\n",
    "            for i_mark in range(len(textmarks)):\n",
    "                mark = textmarks[i_mark]\n",
    "                textmarks_sampleNo[i_mark] = round(mark.Tick / 100)\n",
    "                textmarks_text[i_mark] = ''.join([str(mark[0]), str(mark[1]), str(mark[2])])\n",
    "            \n",
    "            df_textmarks = pd.DataFrame({'sampleNo': textmarks_sampleNo, 'text': textmarks_text})\n",
    "            df_textmarks_allchunks = pd.concat([df_textmarks_allchunks, df_textmarks], ignore_index=True)\n",
    "\n",
    "            #Find textmarks for start and end of resting state recording\n",
    "            if sessionID in rs_exceptions:\n",
    "                exception = rs_exceptions[sessionID]\n",
    "                if exception['start'] == 'rss_last':\n",
    "                    tm_rs_start = df_textmarks_allchunks.loc[\n",
    "                        df_textmarks_allchunks['text'] == 'rss', 'sampleNo'].iloc[-1] # take final rss textmark\n",
    "                else:\n",
    "                    tm_rs_start = exception['start']\n",
    "                tm_rs_end = exception['end']\n",
    "            else:\n",
    "                #For all other sessions:\n",
    "                tm_rs_start = df_textmarks_allchunks.loc[\n",
    "                    df_textmarks_allchunks['text'] == 'rss', 'sampleNo'].iloc[-1] # take final rss textmark\n",
    "                tm_rs_end = df_textmarks_allchunks.loc[\n",
    "                    df_textmarks_allchunks['text'] == 'rse', 'sampleNo'].iloc[-1] # take final rse textmark\n",
    "            \n",
    "            #Check textmarks found\n",
    "            if pd.isna(tm_rs_start):\n",
    "                print(RED + 'WARNING: no rss textmark found')\n",
    "            elif pd.isna(tm_rs_end):\n",
    "                print(RED + 'WARNING: no rse textmark found')\n",
    "\n",
    "            #Check resting state is correct length\n",
    "            rs_tm_duration = tm_rs_end - tm_rs_start\n",
    "            rs_desired_duration = 120 * SPIKE_FS\n",
    "            if abs(rs_desired_duration - rs_tm_duration) > 5*SPIKE_FS : #+- 5s is acceptable\n",
    "                print(RED + 'ERROR: resting state duration = ' + str( round( ( (tm_rs_end - tm_rs_start)/SPIKE_FS), 2) ) + ' s' + RESET )\n",
    "        \n",
    "        \n",
    "        #-----------Get EDA scaling factors\n",
    "        #calculate participant scaling factor\n",
    "        p_id = sessionID + 'p'\n",
    "        row1_p_combined = df_all_combined.index[df_all_combined[\"subjectID\"].str.lower() == p_id.lower()][0]\n",
    "        p_eda_range = df_all_combined.loc[row1_p_combined, 'eda_range'] #get input range\n",
    "        if isinstance(p_eda_range, pd.Series):\n",
    "            # If 'p_eda_range' is still a series with multiple rows, retrieve the first element\n",
    "            p_eda_range = p_eda_range.iloc[0]  # Get the first value\n",
    "        p_transferConstant = p_eda_range/dataRange #calculate participant transfer constant\n",
    "        print('- p transfer constant = ' + str(p_transferConstant))\n",
    "\n",
    "        #calculate companion scaling factor\n",
    "        c_id = sessionID + 'c'\n",
    "        row1_c_combined = df_all_combined.index[df_all_combined[\"subjectID\"].str.lower() == c_id.lower()][0]\n",
    "        c_eda_range = df_all_combined.loc[row1_c_combined, 'eda_range'] #get input range\n",
    "        if isinstance(c_eda_range, pd.Series):\n",
    "            # If 'c_eda_range' is still a series with multiple rows, retrieve the first element\n",
    "            c_eda_range = c_eda_range.iloc[0]  # Get the first value\n",
    "        c_transferConstant = c_eda_range/dataRange #calculate companion transfer constant\n",
    "        print('- c transfer constant = ' + str(c_transferConstant))\n",
    " \n",
    "        #generate IDs of both subjects in session\n",
    "        subjsInSession = [\"p\", \"c\"]\n",
    "\n",
    "        for subj in subjsInSession:\n",
    "            \n",
    "            #------------- Import ECG+EDA data\n",
    "            subjID = sessionID + subj\n",
    "\n",
    "            if subj == \"p\": #IMPORTANT - defines array for calculating HR responses\n",
    "                #*IMPORTANT - import ECG and EDA data\n",
    "                subj_ecg_data = spike2data.analogsignals[analog_channel_dictionary['p_ECG']] \n",
    "                subj_eda_data = spike2data.analogsignals[analog_channel_dictionary['p_EDA']] #*IMPORTANT\n",
    "                subj_transfer_constant = p_transferConstant\n",
    "                \n",
    "            elif subj == \"c\":\n",
    "                subj_ecg_data = spike2data.analogsignals[analog_channel_dictionary['c_ECG']] #*IMPORTANT\n",
    "                subj_eda_data = spike2data.analogsignals[analog_channel_dictionary['c_EDA']] #*IMPORTANT\n",
    "                subj_transfer_constant = c_transferConstant\n",
    "            else:\n",
    "                print(RED + 'ERROR - issue defining subj_ecg_data - subj not recognised')\n",
    "\n",
    "            #Process exceptions\n",
    "            if subjID == 'BG006p':\n",
    "                #BG006 p and c psychophys channels are wrong way round for resting state. Swap over!\n",
    "                subj_ecg_data = spike2data.analogsignals[analog_channel_dictionary['c_ECG']] \n",
    "                subj_eda_data = spike2data.analogsignals[analog_channel_dictionary['c_EDA']] #*IMPORTANT\n",
    "                subj_transfer_constant = c_transferConstant\n",
    "            elif subjID == 'BG006c':\n",
    "                subj_ecg_data = spike2data.analogsignals[analog_channel_dictionary['p_ECG']] \n",
    "                subj_eda_data = spike2data.analogsignals[analog_channel_dictionary['p_EDA']] #*IMPORTANT\n",
    "                subj_transfer_constant = p_transferConstant\n",
    "\n",
    "            #--------Process ecg data--------------------------------------------------------------\n",
    "            subj_ecg_data_np = np.squeeze(subj_ecg_data)\n",
    "            subj_ecg_signal = subj_ecg_data.magnitude.squeeze()\n",
    "            fps_subj_ecg = int(subj_ecg_data.sampling_rate.magnitude)\n",
    "\n",
    "            #Process exceptions\n",
    "            if subjID == \"BG026p\":\n",
    "                subj_ecg_data_np.magnitude[0:343500] = 0\n",
    "                subj_ecg_signal[0:343500] = 0\n",
    "\n",
    "            #----- Identify and correct R peaks\n",
    "            # Detect R peaks *IMPORTANT* lines\n",
    "            _, subj_ecg_peaks = ecg_peaks(signal=subj_ecg_data_np, method=peak_detection_algs[sessionID], sfreq=fps_subj_ecg) # participant data - use sleepecg algorithm from Systole\n",
    "\n",
    "            #df_textmarks_allchunks.to_csv('df_textmarks_allchunks.csv')\n",
    "\n",
    "            if hr_visual_verification:\n",
    "                plot_ECG_peaks(sessionID, subj, subj_ecg_data, subj_ecg_peaks, subj_ecg_signal, df_textmarks_allchunks, resting_or_main = 'resting')\n",
    "    \n",
    "            #------ Delete entire sections of mislabelled peaks-------\n",
    "            print(sessionID + subj + ' peak deletion/addition')\n",
    "\n",
    "            # Find rows in ecg inspection excel file for this recording from this participant\n",
    "            row_subj_recording = (df_rs_ecg_inspection['pID'] == sessionID + subj)\n",
    "\n",
    "            # Delete entire sections of peaks (for replacing slightly but acceptably noisy sections with specific peaks listed in add_peaks)\n",
    "            subj_idx_sections_to_delete_peaks = df_rs_ecg_inspection.loc[row_subj_recording, 'sections_all_peaks_zero'] # Get sample numbers of entire sections of peaks to delete\n",
    "\n",
    "            if isinstance(subj_idx_sections_to_delete_peaks, str): # Ensure it's a list or iterable of strings, in case it's a single value\n",
    "                subj_idx_sections_to_delete_peaks = [subj_idx_sections_to_delete_peaks]\n",
    "\n",
    "            subj_indices_to_delete = [] # Initialise an empty list to collect all indices to delete\n",
    "\n",
    "            for cell_value in subj_idx_sections_to_delete_peaks: # Process each cell value\n",
    "                if cell_value.lower() == 'none':  # Skip cells with 'none'\n",
    "                    continue\n",
    "                for section in cell_value.split(','):  # Split multiple ranges by commas\n",
    "                    start, end = map(int, section.split(':'))  # Split each range and convert to integers\n",
    "                    subj_indices_to_delete.extend(range(start, end + 1))  # Include the endpoint\n",
    "\n",
    "            if subj_indices_to_delete: # Convert to a numpy array for assignment if there are indices to delete\n",
    "                subj_indices_to_delete = np.array(subj_indices_to_delete, dtype=int)\n",
    "                # Delete all peaks in the specified sections\n",
    "                subj_ecg_peaks[subj_indices_to_delete] = 0  # Set the values to zero\n",
    "                print(\n",
    "                    BLUE + '-',\n",
    "                    'Successfully deleted peaks from entire ranges:',\n",
    "                    ', '.join(subj_idx_sections_to_delete_peaks),\n",
    "                    RESET\n",
    "                )\n",
    "\n",
    "            #------Delete specific mislabelled peaks-------\n",
    "            # Get sample numbers of peaks to delete\n",
    "            idx_subj_peaks_to_delete = df_rs_ecg_inspection.loc[row_subj_recording, 'delete_peaks']\n",
    "            idx_subj_peaks_to_delete = idx_subj_peaks_to_delete.to_numpy() #convert from series to numpy\n",
    "            idx_subj_peaks_to_delete = np.fromstring(','.join(idx_subj_peaks_to_delete), sep=',', dtype=float)#convert to float\n",
    "            idx_subj_peaks_to_delete = idx_subj_peaks_to_delete.astype(int)\n",
    "\n",
    "            if len(idx_subj_peaks_to_delete) == 0:\n",
    "                print('-', 'No peaks deleted')\n",
    "\n",
    "            else:\n",
    "                #check that there is a peak at this sample number (i.e. that the value in subj_ecg_peaks at that sample is 1)\n",
    "                subj_peak_vals = subj_ecg_peaks[idx_subj_peaks_to_delete]\n",
    "                subj_mislabelled_peaks = idx_subj_peaks_to_delete[subj_peak_vals == 0] #get sample numbers where there is no peak to delete\n",
    "                subj_truelabelled_peaks = idx_subj_peaks_to_delete[subj_peak_vals == 1] #get sample numbers where there is indeed a peak to delete\n",
    "\n",
    "                #Delete peaks\n",
    "                if len(subj_truelabelled_peaks) > 0:\n",
    "                    #subj_ecg_peaks[subj_truelabelled_peaks] = False #change from one to zero in subj_ecg_peaks_all_chunks to delete peak\n",
    "                    print(BLUE + '-', 'Successfully deleted peaks at',  str(subj_truelabelled_peaks) + RESET)\n",
    "\n",
    "                # Print warning if there is no peak at the sample no. then print warning\n",
    "                if len(subj_mislabelled_peaks) > 0:\n",
    "                    print(RED + 'ERROR: No peaks to delete at sample nos. ', subj_mislabelled_peaks, RESET) #print message\n",
    "\n",
    "                    #For some participants the sample numbers are shifted by 1 so check if there are peaks on the previous samples\n",
    "                    subj_peak_vals_min1 = subj_ecg_peaks[idx_subj_peaks_to_delete - 1]\n",
    "                    subj_mislabelled_peaks_min1 = idx_subj_peaks_to_delete[subj_peak_vals_min1 == 0]\n",
    "                    #or next samples\n",
    "                    subj_peak_vals_plus1 = subj_ecg_peaks[idx_subj_peaks_to_delete + 1]\n",
    "                    subj_mislabelled_peaks_plus1 = idx_subj_peaks_to_delete[subj_peak_vals_plus1 == 0]\n",
    "\n",
    "                    if any(subj_peak_vals_min1) == 1:\n",
    "                        #If all peaks are present 1 sample previously then process\n",
    "                        print(BLUE + 'However peaks found on previous samples ' + str(idx_subj_peaks_to_delete[subj_peak_vals_min1 == 1]-1) + RESET) \n",
    "\n",
    "                        if len(subj_mislabelled_peaks_min1 > 0 ):\n",
    "                            print(RED + 'However no peaks found at ' + str(subj_mislabelled_peaks_min1 -1) + RESET)                        \n",
    "\n",
    "                    if any(subj_peak_vals_plus1) == 1:\n",
    "                        #If any peaks are present 1 sample subsequently then process\n",
    "                        print(BLUE + 'However peaks found on next samples ' + str(idx_subj_peaks_to_delete[subj_peak_vals_plus1 == 1]) + RESET) \n",
    "\n",
    "                        if len(subj_mislabelled_peaks_plus1 > 0 ):\n",
    "                            print(RED + 'However no peaks found at ' + str(subj_mislabelled_peaks_plus1) + RESET)\n",
    "\n",
    " \n",
    "            #Delete peaks on sample, previous sample, and next sample to be sure\n",
    "            subj_ecg_peaks[idx_subj_peaks_to_delete] = False\n",
    "            subj_ecg_peaks[idx_subj_peaks_to_delete - 1] = False\n",
    "            subj_ecg_peaks[idx_subj_peaks_to_delete + 1] = False\n",
    "\n",
    "            #----------- Add missed peaks---------------------\n",
    "            # Get sample numbers of peaks to add\n",
    "            idx_subj_peaks_to_add = df_rs_ecg_inspection.loc[row_subj_recording, 'add_peaks']\n",
    "            idx_subj_peaks_to_add = idx_subj_peaks_to_add.to_numpy() #convert from series to numpy\n",
    "            idx_subj_peaks_to_add = np.fromstring(','.join(idx_subj_peaks_to_add), sep=',', dtype=float)#convert to float\n",
    "            idx_subj_peaks_to_add = idx_subj_peaks_to_add.astype(int)\n",
    "\n",
    "            #add peak to subj_ecg_peaks\n",
    "            if len(idx_subj_peaks_to_add) > 0:\n",
    "                subj_ecg_peaks[idx_subj_peaks_to_add] = 1 #change to 1 in subj_ecg_peaks\n",
    "                print(BLUE + '-', 'Added missing peaks ', str(idx_subj_peaks_to_add) + RESET)\n",
    "\n",
    "            #----------calculate instantaneous HR---------------------\n",
    "            #Calculate instantaneous HR from subj_ecg_peaks\n",
    "            subj_inst_hr = calculate_instantaneous_hr(subj_ecg_peaks, SPIKE_FS)            \n",
    "\n",
    "            #-----------------Calculate HRV parameters\n",
    "\n",
    "            # --- Interpolate RR intervals containing ectopic beats before HRV analysis ---\n",
    "            idx_subj_ectopics_to_interpolate = df_rs_ecg_inspection.loc[row_subj_recording, 'peak_to_interpolate']\n",
    "            idx_subj_ectopics_to_interpolate = idx_subj_ectopics_to_interpolate.to_numpy() #convert from series to numpy\n",
    "            idx_subj_ectopics_to_interpolate = np.fromstring(','.join(idx_subj_ectopics_to_interpolate), sep=',', dtype=float)#convert to float\n",
    "            idx_subj_ectopics_to_interpolate = idx_subj_ectopics_to_interpolate.astype(int)\n",
    "\n",
    "            # Get logical array of ecg peaks between resting state textmarks\n",
    "            rs_subj_ecg_peaks = subj_ecg_peaks[int(tm_rs_start) : int(tm_rs_end)]\n",
    "            \n",
    "            # Subtract tm_rs_start from values in idx_subj_ectopics_to_interpolate to get the indices relative to the start of the resting state\n",
    "            rs_idx_peaks_to_delete = idx_subj_ectopics_to_interpolate - int(tm_rs_start)  # Indices of ectopic beats to interpolate\n",
    "            rs_idx_peaks_to_delete = rs_idx_peaks_to_delete.astype(int)  # Convert to integer type\n",
    "\n",
    "            #debugging\n",
    "            print('idx_subj_ectopics_to_interpolate', idx_subj_ectopics_to_interpolate)\n",
    "            print('rs_idx_peaks_to_delete', rs_idx_peaks_to_delete)\n",
    "\n",
    "            def interpolate_ectopic_rr_cubic_resting(ecg_peaks, idx_peaks_to_interpolate):\n",
    "                \n",
    "                \"\"\"\n",
    "                Interpolate R-R intervals containing ectopic beats using cubic spline interpolation.\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                ecg_peaks : array-like (bool)\n",
    "                    Logical array of detected R-peaks in the ECG signal (True for peak, False otherwise).\n",
    "                idx_peaks_to_interpolate : array-like (int)\n",
    "                    Indices of ectopic R-peaks that should be interpolated.\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                rs_rr_intervals_samples : np.ndarray\n",
    "                    Array of R-peak sample indices for the resting-state period, with ectopic RR intervals interpolated.\n",
    "\n",
    "                Notes\n",
    "                -----\n",
    "                - Uses cubic spline interpolation on valid R-R intervals, excluding ectopic beats.\n",
    "                - Prints debug messages showing original and interpolated IBI values.\n",
    "                - Requires at least 4 valid intervals for spline fitting; otherwise prints an error.\n",
    "                \"\"\"\n",
    "\n",
    "                #Get the R-peak locations in the resting state\n",
    "                rs_rpeaks_samples = np.where(ecg_peaks)[0]  # Indices of detected R-peaks\n",
    "\n",
    "                # Get R-R intervals\n",
    "                rs_rr_intervals = np.diff(rs_rpeaks_samples)  # Calculate R-R intervals\n",
    "\n",
    "                for ectopic in idx_peaks_to_interpolate:\n",
    "                    # Get the index of the ectopic beat immediate before the ectopic\n",
    "                    all_peaks_before_ectopic_idx = np.where(rs_rpeaks_samples < ectopic)[0]\n",
    "                    if len(all_peaks_before_ectopic_idx) > 0:\n",
    "                        peak_before_ectopic_idx = all_peaks_before_ectopic_idx[-1]  # This is the index you want\n",
    "                    else:\n",
    "                        print(RED + 'ERROR: No peaks before ectopic beat' + RESET)\n",
    "                        continue\n",
    "                    \n",
    "                    #define index of IBI containing ectopic beat (peak_before_ectopic_idx + 1)\n",
    "                    ectopic_ibi_index = peak_before_ectopic_idx\n",
    "\n",
    "                    # Create a cubic spline to interpolate the ibi in rs_rr_intervals at ectopic_ibi_index\n",
    "                    \n",
    "                    #Record original ibi value\n",
    "                    original_rr_interval = rs_rr_intervals[ectopic_ibi_index]\n",
    "\n",
    "                    # 3. Identify which RR intervals need interpolation\n",
    "                    ectopic_rr_indices = []\n",
    "                    for ep in idx_peaks_to_interpolate:\n",
    "                        # Find flanking peaks in original array\n",
    "                        before = np.where(rs_rpeaks_samples < ep)[0][-1]\n",
    "                        after = before + 1\n",
    "                        # Map to clean array indices\n",
    "                        ectopic_rr_indices.extend([before-1, before])\n",
    "                    \n",
    "                    # 4. Use only valid intervals for spline fitting\n",
    "                    valid_idx = np.setdiff1d(np.arange(len(rs_rr_intervals)), ectopic_rr_indices)\n",
    "                    \n",
    "                    if len(valid_idx) >= 4:\n",
    "                        cs = CubicSpline(valid_idx, rs_rr_intervals[valid_idx])\n",
    "                        interp_rr = cs(np.arange(len(rs_rr_intervals)))\n",
    "                    else:\n",
    "                        print(RED + 'ERROR: Not enough valid intervals for cubic spline interpolation' + RESET)\n",
    "                        continue\n",
    "\n",
    "                    #get interpolated value\n",
    "                    interpolated_value = interp_rr[ectopic_ibi_index]\n",
    "\n",
    "                    #print\n",
    "                    print(BLUE + 'Interpolated IBI from', original_rr_interval, 'to', interpolated_value, RESET)\n",
    "                    \n",
    "                #convert rs_rr_intervals to sample numbers\n",
    "                rs_rr_intervals_samples = np.concatenate(([0], np.cumsum(interp_rr)))  # Add the first peak at index 0\n",
    "\n",
    "                return rs_rr_intervals_samples\n",
    "\n",
    "            #If there are ectopic beats, then interpolate the RR intervals containing the ectopics\n",
    "            if len(idx_subj_ectopics_to_interpolate) > 0:\n",
    "                rpeaks_clean = interpolate_ectopic_rr_cubic_resting(\n",
    "                rs_subj_ecg_peaks, rs_idx_peaks_to_delete)\n",
    "            else:\n",
    "                #Otherwise just use the original R-peak indices\n",
    "                rpeaks_clean = rs_subj_ecg_peaks\n",
    "\n",
    "            # 4. Calculate HRV metrics using neurokit2\n",
    "            # neurokit2 expects R-peak indices (in samples) and the sampling rate\n",
    "            subj_hrv_params = nk.hrv(rpeaks_clean, sampling_rate=SPIKE_FS, show=False)\n",
    "\n",
    "            print('- Successfully calculated HRV parameters after ectopic interpolation')\n",
    "\n",
    "            #Find subject rows in df_all_combined\n",
    "            subj_rows_combined = df_all_combined['subjectID'].str.contains((subjID), na=False, case=False) #get logical array of rows containing participant ID\n",
    "\n",
    "            # Calculate mean HR\n",
    "            subj_hrv_params[\"HR_Mean\"] = (60*SPIKE_FS)/subj_hrv_params[\"HRV_MeanNN\"]\n",
    "            \n",
    "            # Select relevant HRV parameters\n",
    "            hrv_metrics_to_keep = [\n",
    "                \"HR_Mean\",     #Mean HR\n",
    "                \"HRV_MeanNN\",  # Mean RR (NN) interval\n",
    "                \"HRV_SDNN\",    # Standard deviation of NN intervals\n",
    "                \"HRV_RMSSD\",   # Root mean square of successive differences\n",
    "                \"HRV_pNN50\",   # Percentage of NN intervals >50ms\n",
    "                \"HRV_HF\",      # High-frequency power (vagal tone)\n",
    "                \"HRV_LF\",      # Low-frequency power\n",
    "                \"HRV_LFHF\"     # LF/HF ratio\n",
    "            ]\n",
    "\n",
    "            # Create a dictionary to map original names to new names with 'rs_' prefix\n",
    "            hrv_metrics_renamed = {metric: f\"rs_{metric}\" for metric in hrv_metrics_to_keep}\n",
    "\n",
    "            # Ensure all required metrics exist in the HRV dataframe\n",
    "            subj_hrv_params_filtered = subj_hrv_params[hrv_metrics_to_keep]\n",
    "\n",
    "            # Assign HRV values to the corresponding subject rows in df_all_combined\n",
    "            for original_metric, new_metric in hrv_metrics_renamed.items():\n",
    "                df_all_combined.loc[subj_rows_combined, new_metric] = round(subj_hrv_params_filtered[original_metric].values[0], 5) #round to 5dp\n",
    "\n",
    "                # exclude data from BG047c due to ventricular arrhythmia\n",
    "                if subjID == 'BG047c': \n",
    "                    df_all_combined.loc[subj_rows_combined, new_metric] = pd.NA\n",
    "\n",
    "            print('- Successfully added HRV parameters to df_all_combined with \"rs_\" prefix')\n",
    "\n",
    "            #------------Process EDA data-------------------------------------------\n",
    "            #----- Transform EDA signal (convert to microsiemens, filter low freq. drift ) \n",
    "            #Get eda data\n",
    "            subj_eda_signal = subj_eda_data.magnitude.squeeze()\n",
    "            subj_eda_signal = subj_eda_signal + 5 #add 5 to make all values positive -updated 15.04.25\n",
    "            subj_eda_signal = subj_eda_signal * subj_transfer_constant #apply transfer constant to convert to microsiemens\n",
    "            print('- Applied transfer constant ' + str(round(subj_transfer_constant, 2)))\n",
    "\n",
    "            #get EDA signal between rs textmarks\n",
    "            subj_eda_signal_rs = subj_eda_signal[int(tm_rs_start) : int(tm_rs_end)]\n",
    "\n",
    "            #-------Plot EDA signal\n",
    "            if eda_visual_verification:\n",
    "                # Create time axis (in seconds)\n",
    "                time = np.arange(len(subj_eda_signal_rs)) / 1000  # Convert to seconds\n",
    "\n",
    "                # Plot with explicit time axis\n",
    "                fig = go.Figure()\n",
    "                fig.add_trace(go.Scatter(x=time, y=subj_eda_signal_rs, mode='lines', name='Raw'))\n",
    "                fig.show()\n",
    "\n",
    "            #----Calculate SCL/EDA parameters\n",
    "            subj_eda_signals_rs_nk, _ = nk.eda_process(subj_eda_signal_rs, sampling_rate=SPIKE_FS)\n",
    "            subj_eda_interval_rs_nk = nk.eda_intervalrelated(subj_eda_signals_rs_nk, sampling_rate=SPIKE_FS)\n",
    "\n",
    "            #Calculate sum SCR amplitudes\n",
    "            subj_eda_interval_rs_nk['SCR_Peaks_Amplitude_Sum'] = subj_eda_interval_rs_nk['SCR_Peaks_Amplitude_Mean'] * subj_eda_interval_rs_nk['SCR_Peaks_N']\n",
    "            \n",
    "            #Calculate mean SCL\n",
    "            subj_eda_interval_rs_nk['SCL_Mean'] = subj_eda_signals_rs_nk['EDA_Tonic'].mean()\n",
    "\n",
    "            eda_metrics_to_keep = [\n",
    "                \"SCR_Peaks_N\", # Number of SCR peaks\n",
    "                \"SCR_Peaks_Amplitude_Mean\", #Mean SCR peak amplitude\n",
    "                \"SCR_Peaks_Amplitude_Sum\", #Sum SCR peak amplitudes\n",
    "                \"SCL_Mean\", #Mean SCL\n",
    "                \"EDA_Sympathetic\" # Derived from Posada-Quintero et al. (2016), who argue that dynamics of the sympathetic component of EDA signal is represented in the frequency band of 0.045-0.25Hz.\n",
    "            ]\n",
    "\n",
    "            # Create a dictionary to map original names to new names with 'rs_' prefix\n",
    "            eda_metrics_renamed = {metric: f\"rs_{metric}\" for metric in eda_metrics_to_keep}\n",
    "\n",
    "            # Ensure all required metrics exist in the HRV dataframe\n",
    "            subj_eda_params_filtered = subj_eda_interval_rs_nk[eda_metrics_to_keep]\n",
    "\n",
    "            # Assign EDA values to the corresponding subject rows in df_all_combined\n",
    "            for original_metric, new_metric in eda_metrics_renamed.items():\n",
    "                df_all_combined.loc[subj_rows_combined, new_metric] = round(subj_eda_params_filtered[original_metric].values[0], 5) #round to 5dp\n",
    "\n",
    "                # exclude data from BG007c due to faulty EDA signal\n",
    "                if subjID == 'BG007c': \n",
    "                    df_all_combined.loc[subj_rows_combined, new_metric] = pd.NA\n",
    "                    print(BLUE + '- BG007c rs EDA data excluded due to faulty recording' + RESET)\n",
    "\n",
    "            print('- Successfully added EDA parameters to df_all_combined with \"rs_\" prefix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_destination = BASE_DIR / \"Participant data\" / \"Preprocessed task data\" / \"Empathy task\" / 'df_all_combined.csv'\n",
    "\n",
    "df_all_combined.to_csv(save_destination, index=False)  # Optional: index=False to avoid writing row indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function convert days to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def convert_days_to_datetime(days_since_2000):\n",
    "    \"\"\"\n",
    "    Convert a number of days since 1 Jan 2000 into a formatted datetime string.\n",
    "\n",
    "    This function converts a floating-point number representing days since 0 Jan 2000\n",
    "    (with fractional part representing time of day) into a human-readable datetime string\n",
    "    in the format \"DD/MM/YY HHMMSS.mmm\". The function adjusts the input so that\n",
    "    day 1 corresponds to 1 Jan 2000.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    days_since_2000 : float\n",
    "        Number of days since 0 Jan 2000. The integer part represents whole days,\n",
    "        and the fractional part represents the fraction of a 24-hour day.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Datetime formatted as \"DD/MM/YY HHMMSS.mmm\" (e.g., \"15/08/24 113015.123\").\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Subtracts 1 from the input to align with 1 Jan 2000 as day 1.\n",
    "    - Handles fractional days by converting to hours, minutes, seconds, and milliseconds.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> days_since_2000 = 9021.48654358796\n",
    "    >>> convert_days_to_datetime(days_since_2000)\n",
    "    '28/08/24 115646.430'\n",
    "    \"\"\"\n",
    "    \n",
    "    # subtract 1 day to convert from 'days since 0 Jan 2000' to 'days since 1 Jan 2000'\n",
    "    days_since_2000 = days_since_2000 - 1\n",
    "\n",
    "    # Split the input into whole days and fractional part\n",
    "    whole_days = int(days_since_2000)\n",
    "    fractional_day = days_since_2000 - whole_days\n",
    "\n",
    "    # Create a datetime object for Jan 1, 2000\n",
    "    base_date = datetime(2000, 1, 1)\n",
    "\n",
    "    # Add the number of days\n",
    "    date = base_date + timedelta(days=whole_days)\n",
    "\n",
    "    # Calculate hours, minutes, seconds, and milliseconds from the fractional day\n",
    "    total_seconds = fractional_day * 24 * 3600\n",
    "    hours = int(total_seconds / 3600)\n",
    "    minutes = int((total_seconds % 3600) / 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "    milliseconds = int((total_seconds % 1) * 1000)\n",
    "\n",
    "    # Combine date and time\n",
    "    result_datetime = date.replace(hour=hours, minute=minutes, second=seconds, microsecond=milliseconds*1000)\n",
    "\n",
    "    # Format the result\n",
    "    formatted_time = result_datetime.strftime(\"%d/%m/%y %H%M%S.%f\")[:-3]\n",
    "\n",
    "    return formatted_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "empathy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
